# Flow Matching for Scalable Simulation-Based Inference

**Authors:** %
 Maximilian Dax\thanks{Equal contribution} \\
 Max Planck Institute for Intelligent Systems\\
 Tübingen, Germany \\
\texttt{maximilian.dax@tuebingen.mpg.de} \\
 \And
 Jonas Wildberger\footnotemark[1] \\
 Max Planck Institute for Intelligent Systems\\
 Tübingen, Germany \\
\texttt{wildberger.jonas@tuebingen.mpg.de} \\
 \And
 Simon Buchholz\footnotemark[1] \\
 Max Planck Institute for Intelligent Systems\\
 Tübingen, Germany\\
\texttt{sbuchholz@tue.mpg.de} \\
 \And 
 Stephen R.~Green \\
 *{1.2cm}University of Nottingham*{1.2cm} \\ Nottingham, United Kingdom \\ 
 \And
 Jakob H.~Macke\\
 Max Planck Institute for Intelligent Systems \&\\
 Machine Learning in Science, \\University of Tübingen\\
 Tübingen, Germany \\ 
 \And
 Bernhard Schölkopf\\
 Max Planck Institute for Intelligent Systems\\
 Tübingen, Germany \\

# Abstract

Neural posterior estimation methods based on discrete normalizing flows have become established tools for simulation-based inference (SBI), but scaling them to high-dimensional problems can be challenging. Building on recent advances in generative modeling, we here present flow matching posterior estimation (FMPE), a technique for SBI using continuous normalizing flows. Like diffusion models, and in contrast to discrete flows, flow matching allows for unconstrained architectures, providing enhanced flexibility for complex data modalities. Flow matching, therefore, enables exact density evaluation, fast training, and seamless scalability to large architectures---making it ideal for SBI. We show that FMPE achieves competitive performance on an established SBI benchmark, and then demonstrate its improved scalability on a challenging scientific problem: for gravitational-wave inference, FMPE outperforms methods based on comparable discrete flows, reducing training time by 30\% with substantially improved accuracy. Our work underscores the potential of FMPE to enhance performance in challenging inference scenarios, thereby paving the way for more advanced applications to scientific problems.

# Introduction
\label{sec:introduction}

The ability to readily represent Bayesian posteriors of arbitrary complexity using neural networks would herald a revolution in scientific data analysis. Such networks could be trained using simulated data and used for amortized inference across observations---bringing tractable inference and speed to a myriad of scientific models. Thanks to innovative architectures such as normalizing flows~\cite{rezende2015variational,papamakarios2019normalizing}, approaches to neural simulation-based inference (SBI)~\cite{Cranmer:2019eaq} have seen remarkable progress in recent years. Here, we show that modern approaches to deep generative modeling (particularly flow matching) deliver substantial improvements in simplicity, flexibility and scaling when adapted to SBI. 

The Bayesian approach to data analysis is to compare observations to models via the posterior distribution $p(\theta|x)$. This gives our degree of belief that model parameters $\theta$ gave rise to an observation $x$, and is proportional to the model likelihood $p(x|\theta)$ times the prior $p(\theta)$. One is typically interested in representing the posterior in terms of a collection of samples, however obtaining these through standard likelihood-based algorithms can be challenging for intractable or expensive likelihoods. In such cases, SBI offers an alternative based instead on *data simulations* $x \sim p(x|\theta)$. Combined with deep generative modeling, SBI becomes a powerful paradigm for scientific inference~\cite{Cranmer:2019eaq}. Neural posterior estimation (NPE)~\citep{papamakarios2016fast,lueckmann2017flexible,greenberg2019automatic}, for instance, trains a conditional density estimator $q(\theta|x)$ to approximate the posterior, allowing for rapid sampling and density estimation for any $x$ consistent with the training distribution.

The NPE density estimator $q(\theta|x)$ is commonly taken to be a (discrete) normalizing flow~\citep{rezende2015variational,papamakarios2019normalizing}, an approach that has brought state-of-the-art performance in challenging problems such as gravitational-wave inference~\cite{Dax:2021tsq}. Naturally, performance hinges on the expressiveness of $q(\theta|x)$. Normalizing flows transform noise to samples through a discrete sequence of basic transforms. These have been carefully engineered to be invertible with simple Jacobian determinant, enabling efficient maximum likelihood training, while at the same time producing expressive $q(\theta|x)$. Although many such discrete flows are universal density approximators~\cite{papamakarios2019normalizing}, in practice, they can be challenging to scale to very large networks, which are needed for big-data experiments.

**Figure:** Comparison of network architectures (left) and flow trajectories (right). Discrete flows (NPE, top) require a specialized architecture for the density estimator. Continuous flows (FMPE, bottom) are based on a vector field parametrized with an unconstrained architecture. FMPE uses this additional flexibility to put an enhanced emphasis on the conditioning data $x$, which in the SBI context is typically high dimensional and in a complex domain. Further, the optimal transport path produces simple flow trajectories from the base distribution (inset) to the target. \label{fig:intro-flows}

Recent studies~\cite{sharrock2022sequential,geffner2022score} propose neural posterior score estimation (NPSE), a rather different approach that models the posterior distribution with score-matching (or diffusion) networks. These techniques were originally developed for generative modeling~\cite{sohl2015deep,song2019generative,ho2020denoising}, achieving state-of-the-art results in many domains, including image generation~\cite{NEURIPS2021_49ad23d1,ho2022cascaded}. Like discrete normalizing flows, diffusion models transform noise into samples, but with trajectories parametrized by a *continuous* ``time'' parameter $t$. The trajectories solve a stochastic differential equation~\cite{song2020score} (SDE) defined in terms of a vector field $v_t$, which is trained to match the score of the intermediate distributions $p_t$. NPSE has several advantages compared to NPE, including the ability to combine multiple observations at inference time~\cite{geffner2022score} and, importantly, the freedom to use unconstrained network architectures. 

We here propose to use flow matching, another recent technique for generative modeling, for Bayesian inference, an approach we refer to as flow-matching posterior estimation (FMPE). Flow matching is also based on a vector field $v_t$ and thereby also admits flexible network architectures (Fig.~\ref{fig:intro-flows}). For flow matching, however, $v_t$ directly defines the velocity field of sample trajectories, which solve ordinary differential equations (ODEs) and are deterministic. As a consequence, flow matching allows for additional freedom in designing non-diffusion paths such as optimal transport, and provides direct access to the density~\cite{flow_matching}. These differences are summarized in Tab.~\ref{tab:comparison}.

**Table:** Comparison of posterior-estimation methods. \label{tab:comparison}

| lccc@{}} | NPE | NPSE | \multicolumn{1}{c}{**FMPE (Ours)**} |
| --- | --- | --- | --- |
| Tractable posterior density | Yes | No | Yes |
| Unconstrained network architecture | No | Yes | Yes |
| Network passes for sampling | Single | Many | Many |

Our contributions are as follows: 

- We adapt flow-matching to Bayesian inference, proposing FMPE. In general, the modeling requirements of SBI are different from generative modeling. For the latter, sample quality is critical, i.e., that samples lie in the support of a complex distribution (e.g., images). In contrast, for SBI, $p(\theta|x)$ is typically less complex for fixed $x$, but $x$ itself can be complex and high-dimensional. We therefore consider pyramid-like architectures from $x$ to $v_t$, with gated linear units to incorporate $(\theta, t)$ dependence, rather than the typical U-Net used for images (Fig.~\ref{fig:intro-flows}). We also propose an alternative $t$-weighting in the loss, which improves performance in many tasks.
- Under certain regularity assumptions, we prove an upper bound on the KL divergence between the model and target posterior. This implies that estimated posteriors are mass-covering, i.e., that their support includes all $\theta$ consistent with observed $x$, which is highly desirable for scientific applications \cite{hermans2021averting}.
- We perform a number of experiments to investigate the performance of FMPE.\footnote{Code available \href{https://github.com/dingo-gw/flow-matching-posterior-estimation}{here}.} Our two-pronged approach, which involves a set of benchmark tests and a real-world problem, is designed to probe complementary aspects of the method, covering breadth and depth of applications. First, on an established suite of SBI benchmarks, we show that FMPE performs comparably---or better---than NPE across most tasks, and in particular exhibits mass-covering posteriors in all cases (Sec.~\ref{sec:sbibm}). We then push the performance limits of FMPE on a challenging real-world problem by turning to gravitational-wave inference (Sec.~\ref{sec:gw}). We show that FMPE substantially outperforms an NPE baseline in terms of training time, posterior accuracy, and scaling to larger networks.

# Preliminaries
\label{sec:preliminaries}

#### Normalizing flows.

A normalizing flow~\cite{rezende2015variational,papamakarios2019normalizing} defines a probability distribution $q(\theta|x)$ over parameters $\theta \in \mathbb{R}^n$ in terms of an invertible mapping $\psi_x: \mathbb{R}^n \to \mathbb{R}^n$ from a simple base distribution $q_0(\theta)$, 

$$
\label{eq:normalizing-flow}
q(\theta | x) = (\psi_x)_\ast q_0(\theta) = q_0( \psi_x^{-1}(\theta)) \det \left| \frac{\partial \psi_x^{-1}(\theta)}{\partial \theta} \right|\,,
$$

where $(\cdot)_\ast$ denotes the pushforward operator, and for generality we have conditioned on additional context $x \in \mathbb{R}^m$. Unless otherwise specified, a normalizing flow refers to a *discrete* flow, where $\psi_x$ is given by a composition of simpler mappings with triangular Jacobians, interspersed with shuffling of the $\theta$. This construction results in expressive $q(\theta|x)$ and also efficient density evaluation \cite{papamakarios2019normalizing}. 

#### Continuous normalizing flows.

A continuous flow~\cite{neural_ode} also maps from base to target distribution, but is parametrized by a continuous ``time'' $t \in [0,1]$, where $q_0(\theta|x) = q_0(\theta)$ and $q_1(\theta|x) = q(\theta|x)$. For each $t$, the flow is defined by a vector field $v_{t,x}$ on the sample space.\footnote{In the SBI literature, this is also commonly referred to as ``parameter space''.} This corresponds to the velocity of the sample trajectories, 

$$
\label{eq:ODE}
\frac{d}{dt} \psi_{t, x} (\theta) = v_{t, x}(\psi_{t, x}(\theta)),\qquad \psi_{0, x} (\theta) = \theta.
$$

We obtain the trajectories $\theta_t \equiv \psi_{t,x}(\theta)$ by integrating this ODE. The final density is given by 

$$
\label{eq:cnf-density}
q(\theta|x) = (\psi_{1, x})_\ast q_0(\theta) = q_0(\theta) \exp\left( - \int_0^1 \Div v_{t,x} (\theta_t) \,\mathrm{d} t \right),
$$

which is obtained by solving the transport equation $\partial_t q_t + \Div(q_t v_{t,x}) = 0$.

The advantage of the continuous flow is that $v_{t,x}(\theta)$ can be simply specified by a neural network taking $\mathbb{R}^{n+m+1} \to \mathbb{R}^n$, in which case~\eqref{eq:ODE} is referred to as a *neural ODE*~\cite{neural_ode}. Since the density is tractable via \eqref{eq:cnf-density}, it is in principle possible to train the flow by maximizing the (log-)likelihood. However, this is often not feasible in practice, since both sampling and density estimation require many network passes to numerically solve the ODE \eqref{eq:ODE}. 

#### Flow matching.

An alternative training objective for continuous normalizing flows is provided by flow matching~\cite{flow_matching}. This directly regresses $v_{t,x}$ on a vector field $u_{t,x}$ that generates a target probability path $p_{t,x}$. It has the advantage that training does not require integration of ODEs, however it is not immediately clear how to choose $(u_{t,x},p_{t,x})$. The key insight of~\citep{flow_matching} is that, if the path is chosen on a *sample-conditional* basis,\footnote{We refer to conditioning on $\theta_1$ as *sample*-conditioning to distinguish from conditioning on $x$.} then the training objective becomes extremely simple. Indeed, given a sample-conditional probability path $p_t(\theta|\theta_1)$ and a corresponding vector field $u_t(\theta|\theta_1)$, we specify the sample-conditional flow matching loss as 

$$
\label{eq:flow-matching}
\mathcal{L}_\text{SCFM} = \Ex_{t\sim \mathcal{U}[0,1],\,x\sim p(x),\,\theta_1\sim p(\theta|x),\,\theta_t\sim p_t(\theta_t | \theta_1)} \left\| v_{t,x}(\theta_t) - u_t(\theta_t| \theta_1)\right\|^2.
$$

Remarkably, minimization of this loss is equivalent to regressing $v_{t,x}(\theta)$ on the *marginal* vector field $u_{t,x}(\theta)$ that generates $p_t(\theta|x)$~\citep{flow_matching}. Note that in this expression, the $x$-dependence of $v_{t,x}(\theta)$ is picked up via the expectation value, with the sample-conditional vector field independent of $x$.

There exists considerable freedom in choosing a sample-conditional path. Ref.~\citep{flow_matching} introduces the family of Gaussian paths 

$$
\label{eq:sample-conditional-path}
p_t(\theta|\theta_1) = \N(\theta|\mu_t(\theta_1),\sigma_t(\theta_1)^2I_n),
$$

where the time-dependent means $\mu_t(\theta_1)$ and standard deviations $\sigma_t(\theta_1)$ can be freely specified (subject to boundary conditions\footnote{The sample-conditional probability path should be chosen to be concentrated around $\theta_1$ at $t=1$ (within a small region of size $\sigma_{\text{min}}$) and to be the base distribution at $t=0$.}). For our experiments, we focus on the optimal transport paths defined by $\mu_t(\theta_1)=t\theta_1$ and $\sigma_t(\theta_1)=1-(1-\sigma_{\min})t$ (also introduced in~\citep{flow_matching}). The sample-conditional vector field then has the simple form 

$$
\label{eq:sample-conditional-vector-field}
u_t(\theta|\theta_1) = \frac{\theta_1 - (1 - \sigma_\text{min})\theta}{1 - (1 - \sigma_\text{min})t}.
$$

#### Neural posterior estimation (NPE).

NPE is an SBI method that directly fits a density estimator $q(\theta|x)$ (usually a normalizing flow) to the posterior $p(\theta|x)$~\cite{papamakarios2016fast,lueckmann2017flexible,greenberg2019automatic}. NPE trains with the maximum likelihood objective $\mathcal{L}_\text{NPE} = -\Ex_{p(\theta)p(x|\theta)}\log q(\theta|x)$, using Bayes' theorem to simplify the expectation value with $\Ex_{p(x)p(\theta|x)} \to \Ex_{p(\theta)p(x|\theta)}$. During training, $\mathcal{L}_\text{NPE}$ is estimated based on an empirical distribution consisting of samples $(\theta,x)\sim p(\theta)p(x|\theta)$. Once trained, NPE can perform inference for every new observation using $q(\theta|x)$, thereby *amortizing* the computational cost of simulation and training across all observations. NPE further provides exact density evaluations of $q(\theta|x)$. Both of these properties are crucial for the physics application in section~\ref{sec:gw}, so we aim to retain these properties with FMPE.

## Related work

Flow matching~\cite{flow_matching} has been developed as a technique for generative modeling, and similar techniques are discussed in~\cite{albergo2022building,liu2022flow,neklyudov2022action} and extended in \cite{davtyan2022randomized,tong2023conditional}. Flow matching encompasses the deterministic ODE version of diffusion models~\cite{sohl2015deep,song2019generative,ho2020denoising} as a special instance. Although to our knowledge flow matching has not previously been applied to Bayesian inference, score-matching diffusion models have been proposed for SBI in~\cite{sharrock2022sequential,geffner2022score} with impressive results. These studies, however, use stochastic formulations via SDEs~\cite{song2020score} or Langevin steps and are therefore not directly applicable when evaluations of the posterior density are desired (see Tab.~\ref{tab:comparison}). It should be noted that score modeling can also be used to parameterize continuous normalizing flows via an ODE. Extension of~\cite{sharrock2022sequential,geffner2022score} to the deterministic formulation could thereby be seen as a special case of flow matching. Many of our analyses and the practical guidance provided in Section~\ref{sec:fmpe} therefore also apply to score matching.

We here focus on comparisons of FMPE against NPE~\cite{papamakarios2016fast,lueckmann2017flexible,greenberg2019automatic}, as it best matches the requirements of the application in section~\ref{sec:gw}. Other SBI methods include approximate Bayesian computation~\cite{sisson2018overview,beaumont2002approximate,beaumont2009adaptive,blum2010non,prangle2013semiautomatic}, neural likelihood estimation~\cite{wood2010statistical,drovandi2018approximating,papamakarios2019sequential,lueckmann2019likelihood} and neural ratio estimation~\cite{izbicki2014high,pham2014a,cranmer2015approximating,hermans2019likelihood,durkan2020contrastive,thomas2020likelihoodfree,miller2022contrastive}. Many of these approaches have sequential versions, where the estimator networks are specifically tuned to a specific observation $x_\text{o}$. FMPE has a tractable density, so it is straightforward to apply the sequential NPE~\cite{papamakarios2016fast,lueckmann2017flexible,greenberg2019automatic} approaches to FMPE. In this case, inference is no longer amortized, so we leave this extension to future work. 

# Flow matching posterior estimation
\label{sec:fmpe}

To apply flow matching to SBI we use Bayes' theorem to make the usual replacement $\Ex_{p(x)p(\theta|x)} \to \Ex_{p(\theta)p(x|\theta)}$ in the loss function \eqref{eq:flow-matching}, eliminating the intractable expectation values. This gives the FMPE loss 

$$
\label{eq:fmpe-loss}
\mathcal{L}_\text{FMPE} = \Ex_{t\sim p(t),\,\theta_1\sim p(\theta), x\sim p(x|\theta_1), \,\theta_t\sim p_{t}(\theta_t | \theta_1)} \left\| v_{t,x}(\theta_t) - u_{t}(\theta_t| \theta_1)\right\|^2,
$$

which we minimize using empirical risk minimization over samples $(\theta,x)\sim p(\theta)p(x|\theta)$. In other words, training data is generated by sampling $\theta$ from the prior, and then simulating data $x$ corresponding to $\theta$. This is in close analogy to NPE training, but replaces the log likelihood maximization with the sample-conditional flow matching objective. Note that in this expression we also sample $t \sim p(t)$, $t\in [0,1]$ (see Sec.~\ref{subsec:t-weighting}), which generalizes the uniform distribution in~\eqref{eq:flow-matching}. This provides additional freedom to improve learning in our experiments. 

## Probability mass coverage
\label{subsec:mass-coverage}

As we show in our examples, trained FMPE models $q(\theta|x)$ can achieve excellent results in approximating the true posterior $p(\theta|x)$. However, it is not generally possible to achieve *exact* agreement due to limitations in training budget and network capacity. It is therefore important to understand how inaccuracies manifest. Whereas sample quality is the main criterion for generative modeling, for scientific applications one is often interested in the overall shape of the distribution. In particular, an important question is whether $q(\theta|x)$ is *mass-covering*, i.e., whether it contains the full support of $p(\theta|x)$. This minimizes the risk to falsely rule out possible explanations of the data. It also allows us to use importance sampling if the likelihood $p(x|\theta)$ of the forward model can be evaluated, which can be used for precise estimation of the posterior~\cite{muller2019neural,Dax:2022pxd}.

\begin{wrapfigure}{r}{0.40\textwidth} \centering \includegraphics[width=0.98\textwidth]{Figures/mass-covering-gaussian.pdf} \caption{\label{fig:gaussian-fm-mass-coverage} A Gaussian (blue) fitted to a bimodal distribution (gray) with various objectives. } \end{wrapfigure} Consider first the mass-covering property for NPE. NPE directly minimizes the forward KL divergence $\KL(p(\theta|x)||q(\theta|x))$, and thereby provides probability-mass covering results. Therefore, even if NPE is not accurately trained, the estimate $q(\theta|x)$ should cover the entire support of the posterior $p(\theta|x)$ and the failure to do so can be observed in the validation loss. As an illustration in an unconditional setting, we observe that a unimodal Gaussian $q$ fitted to a bimodal target distribution $p$ captures both modes when using the forward KL divergence $\KL(p||q)$, but only a single mode when using the backwards direction $\KL(q||p)$ (Fig.~\ref{fig:gaussian-fm-mass-coverage}).

For FMPE, we can fit a Gaussian flow-matching model $q(\theta)=\N(\hat\mu,\hat\sigma^2)$ to the same bimodal target, in this case, parametrizing the vector field as 

$$
\label{eq:ut-for-gaussian-flow}
v_t(\theta) = \frac{(\sigma_t^2 + (t\hat\sigma)^2 -\sigma_t)\theta_t + t\hat\mu\cdot\sigma_t}{t\cdot(\sigma_t^2 + (t\hat\sigma)^2)}
$$

(see Appendix~\ref{sec:appendix-gaussian-flow}), we also obtain a mass-covering distribution when fitting the learnable parameters $(\hat\mu,\hat\sigma)$ via \eqref{eq:flow-matching}. This provides some indication that the flow matching objective induces mass-covering behavior, and leads us to investigate the more general question of whether the mean squared error between vector fields $u_t$ and $v_t$ bounds the forward KL divergence. Indeed, the former agrees up to constant with the sample-conditional loss \eqref{eq:flow-matching} (see Sec.~\ref{sec:preliminaries}).

We denote the flows of $u_t$, $v_t$, by $\phi_t$, $\psi_t$, respectively, and we set $q_t=(\psi_t)_\ast q_0$, $p_t=(\phi_t)_\ast q_0$. The precise question then is whether we can bound $\KL(p_1||q_1)$ by $\MSE_{p}(u, v)^\alpha$ for some positive power $\alpha$. It was already observed in \cite{albergo2023stochastic} that this is not true in general, and we provide a simple example to that effect in Lemma~\ref{le:holes} in Appendix~\ref{sec:appendix-mass-covering-fm}. Indeed, it was found in \cite{albergo2023stochastic} that to bound the forward KL divergence we also need to control the Fisher divergence, $\int p_t(\d \theta) (\nabla \ln p_t(\theta)-\nabla q_t(\theta))^2$. Here we show instead that a bound can be obtained under sufficiently strong regularity assumptions on $p_0$, $u_t$, and $v_t$. The following statement is slightly informal, and we refer to the supplement for the complete version. \begin{theorem}\label{theorem:mass-coverage} Let $p_0=q_0$ and assume $u_t$ and $v_t$ are two vector fields whose flows satisfy $p_1=(\phi_1)_\ast p_0$ and $q_1=(\psi_1)_\ast q_0$. Assume that $p_0$ is square integrable and satisfies $|\nabla \ln p_0(\theta)|\leq c(1+|\theta|)$ and $u_t$ and $v_t$ have bounded second derivatives. Then there is a constant $C>0$ such that (for $\MSE_p(u,v)<1)$) \begin{align} \KL(p_1||q_1)\leq C \MSE_{p}(u,v)^{\frac12}. \end{align} \end{theorem} *The proof of this result can be found in appendix~\ref{sec:appendix-mass-covering-fm*.} While the regularity assumptions are not guaranteed to hold in practice when $v_t$ is parametrized by a neural net, the theorem nevertheless gives some indication that the flow-matching objective encourages mass coverage. In Section~\ref{sec:sbibm} and~\ref{sec:gw}, this is complemented with extensive empirical evidence that flow matching indeed provides mass-covering estimates.

We remark that it was shown in \cite{song2021maximum} that the KL divergence of SDE solutions can be bounded by the MSE of the estimated score function. Thus, the smoothing effect of the noise ensures mass coverage, an aspect that was further studied using the Fokker-Planck equation in \cite{albergo2023stochastic}. For flow matching, imposing the regularity assumption plays a similar role.

## Network architecture
\label{subsec:fmpe-arch}

Generative diffusion or flow matching models typically operate on complicated and high dimensional data in the $\theta$ space (e.g., images with millions of pixels). One typically uses U-Net~\cite{ronneberger2015u} like architectures, as they provide a natural mapping from $\theta$ to a vector field $v(\theta)$ of the same dimension. The dependence on $t$ and an (optional) conditioning vector $x$ is then added on top of this architecture.

For SBI, the data $x$ is often associated with a complicated domain, such as image or time series data, whereas parameters $\theta$ are typically low dimensional. In this context, it is therefore useful to build the architecture starting as a mapping from $x$ to $v(x)$ and then add conditioning on $\theta$ and $t$. In practice, one can therefore use any established feature extraction architecture for data in the domain of $x$, and adjust the dimension of the feature vector to $n = \dim(\theta)$. In our experiments, we found that the $(t,\theta)$-conditioning is best achieved using gated linear units~\cite{dauphin2017language} to the hidden layers of the network (see also Fig.~\ref{fig:intro-flows}); these are also commonly used for conditioning discrete flows on $x$.

## Re-scaling the time prior
\label{subsec:t-weighting}

The time prior $\mathcal{U}[0,1]$ in~\eqref{eq:flow-matching} distributes the training capacity uniformly across $t$. We observed that this is not always optimal in practice, as the complexity of the vector field may depend on $t$. For FMPE we therefore sample $t$ in \eqref{eq:fmpe-loss} from a power-law distribution $p_\alpha(t) \propto t^{1 / (1 + \alpha)},~t\in[0,1]$, introducing an additional hyperparameter $\alpha$. This includes the uniform distribution for $\alpha=0$, but for $\alpha>0$, assigns greater importance to the vector field for larger values of $t$. We empirically found this to improve learning for distributions with sharp bounds (e.g., Two Moons in Section~\ref{sec:sbibm}).

# SBI benchmark
\label{sec:sbibm}

We now evaluate FMPE on ten tasks included in the benchmark presented in~\cite{lueckmann2021benchmarking}, ranging from simple Gaussian toy models to more challenging SBI problems from epidemiology and ecology, with varying dimensions for parameters ($\dim(\theta)\in[2,10]$) and observations ($\dim(x)\in[2,100]$). For each task, we train three separate FMPE models with simulation budgets $N\in\{10^3, 10^4, 10^5\}$. We use a simple network architecture consisting of fully connected residual blocks~\cite{he2015deep} to parameterize the conditional vector field. For the two tasks with $\dim (x) = 100$ (B-GLM-Raw, SLCP-D), we condition on $(t,\theta)$ via gated linear units as described in Section~\ref{subsec:fmpe-arch} (Fig.~\ref{fig:benchmark-theta-embedding} in Appendix~\ref{sec:appendix-sbibm} shows the corresponding performance gain). For the remaining tasks with $\dim(x)\leq 10$ we concatenate $(t,\theta,x)$ instead. We reserve 5\% of the simulations for validation. See Appendix~\ref{sec:appendix-sbibm} for details.

For each task and simulation budget, we evaluate the model with the lowest validation loss by comparing $q(\theta|x)$ to the reference posteriors $p(\theta|x)$ provided in~\cite{lueckmann2021benchmarking} for ten different observations $x$ in terms of the C2ST score~\cite{friedman2003multivariate,lopez2016revisiting}. This performance metric is computed by training a classifier to discriminate inferred samples $\theta\sim q(\theta|x)$ from reference samples $\theta\sim p(\theta|x)$. The C2ST score is then the test accuracy of this classifier, ranging from 0.5 (best) to 1.0. We observe that FMPE exhibits comparable performance to an NPE baseline model for most tasks and outperforms on several (Fig.~\ref{fig:benchmark-c2st}). In terms of the MMD metric (Fig.~\ref{fig:benchmark-mmd} in the Appendix), FMPE clearly outperforms NPE (but MMD can be sensitive to its hyperparameters~\cite{lueckmann2021benchmarking}). As NPE is one of the highest ranking methods for many tasks in the benchmark, these results show that FMPE indeed performs competitively with other existing SBI methods. We report an additional baseline for score matching in Fig.~\ref{fig:benchmark-SMPE} in the Appendix.

As NPE and FMPE both directly target the posterior with a density estimator (in contrast to most other SBI methods), observed differences can be primarily attributed to their different approaches for density estimation. Interestingly, a great performance improvement of FMPE over NPE is observed for SLCP with a large simulation budget ($N=10^5$). The SLCP task is specifically designed to have a simple likelihood but a complex posterior, and the FMPE performance underscores the enhanced flexibility of the FMPE density estimator.

\begin{wrapfigure}[16]{r}{0.455\textwidth} \centering \includegraphics[width=0.98\textwidth]{Figures/log_probs_single.pdf} \caption{\label{fig:sbibm-logprobs} Histogram of FMPE densities $\log q(\theta|x)$ for reference samples $\theta\sim p(\theta|x)$ (Two Moons task, $N=10^3$). The estimate $q(\theta|x)$ clearly covers $p(\theta|x)$ entirely. } \end{wrapfigure}

Finally, we empirically investigate the mass coverage suggested by our theoretical analysis in Section~\ref{subsec:mass-coverage}. We display the density $\log q(\theta|x)$ of the reference samples $\theta\sim p(\theta|x)$ under our FMPE model $q$ as a histogram (Fig.~\ref{fig:sbibm-logprobs}). All samples $\theta\sim p(\theta|x)$ fall into the support from $q(\theta|x)$. This becomes apparent when comparing to the density $\log q(\theta|x)$ for samples $\theta\sim q(\theta|x)$ from $q$ itself. This FMPE result is therefore mass covering. Note that this does not necessarily imply conservative posteriors (which is also not generally true for the forward KL divergence~\cite{hermans2021averting,delaunoy2022reliable,delaunoy2023balancing}), and some parts of $p(\theta|x)$ may still be undersampled. Probability mass coverage, however, implies that no part is entirely missed (compare Fig.~\ref{fig:gaussian-fm-mass-coverage}), even for multimodal distributions such as Two Moons. Fig.~\ref{fig:sbibm-logprobs-all} in the Appendix confirms the mass coverage for the other benchmark tasks.

# Gravitational-wave inference
\label{sec:gw}

## Background

Gravitational waves (GWs) are ripples of spacetime predicted by Einstein and produced by cataclysmic cosmic events such as the mergers of binary black holes (BBHs). GWs propagate across the universe to Earth, where the LIGO-Virgo-KAGRA observatories measure faint time-series signals embedded in noise. To-date, roughly 90 detections of merging black holes and neutron stars have been made~\cite{LIGOScientific:2021djp}, all of which have been characterized using Bayesian inference to compare against theoretical models.\footnote{BBH parameters $\theta\in\mathbb{R}^{15}$ include black-hole masses, spins, and the spacetime location and orientation of the system (see Tab.~\ref{tab:GW-parameters-with-priors} in the Appendix). We represent $x$ in frequency domain; for two LIGO detectors and complex $f\in[20,512]~\text{Hz},~\Delta f = 0.125~\text{Hz}$, we have $x\in\mathbb{R}^{15744}$.} These have yielded insights into the origin and evolution of black holes~\citep{LIGOScientific:2020kqk}, fundamental properties of matter and gravity~\citep{LIGOScientific:2018cki,LIGOScientific:2020tif}, and even the expansion rate of the universe~\citep{LIGOScientific:2017adf}. 

**Figure:** Comparison of FMPE with NPE, a standard SBI method, across 10 benchmark tasks \cite{lueckmann2021benchmarking}. \label{fig:benchmark-c2st}

Under reasonable assumptions on detector noise, the GW likelihood is tractable,\footnote{Noise is assumed to be stationary and Gaussian, so for frequency-domain data, the GW likelihood $p(x|\theta) = \mathcal{N}(h(\theta) | S_{\mathrm{n}})(x)$. Here $h(\theta)$ is a theoretical signal model based on Einstein's theory of general relativity, and $S_{\mathrm{n}}$ is the power spectral density of the detector noise.} and inference is typically performed using tools~\cite{Veitch:2014wba,Ashton:2018jfp,Romero-Shaw:2020owr,Speagle_2020} based on Markov chain Monte Carlo~\cite{metropolis1953equation,Hastings:1970} or nested sampling~\cite{Skilling:2006} algorithms. This can take from hours to months, depending on the nature of the event and the complexity of the signal model, with a typical analysis requiring up to $\sim 10^8$ likelihood evaluations. The ever-increasing rate of detections means that these analysis times risk becoming a bottleneck. SBI offers a promising solution for this challenge that has thus been actively studied in the literature~\cite{Cuoco:2020ogp,Gabbard:2019rde,Green:2020hst,Delaunoy:2020zcu,Green:2020dnx,Dax:2021tsq,Dax:2021myb,Chatterjee:2022ggk,Dax:2022pxd}. A fully amortized NPE-based method called DINGO recently achieved accuracies comparable to stochastic samplers with inference times of less than a minute per event~\cite{Dax:2021tsq}. To achieve accurate results, however, DINGO uses group-equivariant NPE~\cite{Dax:2021tsq,Dax:2021myb} (GNPE), an NPE extension that integrates known conditional symmetries. GNPE, therefore, does not provide a tractable density, which is problematic when verifying and correcting inference results using importance sampling~\cite{Dax:2022pxd}. 

## Experiments

We here apply FMPE to GW inference. As a baseline, we train an NPE network with the settings described in~\cite{Dax:2021tsq} with a few minor changes (see Appendix~\ref{sec:appendix-gw}).\footnote{Our implementation builds on the public DINGO code from \url{https://github.com/dingo-gw/dingo}.} This uses an embedding network~\cite{radev2020bayesflow} to compress $x$ to a 128-dimensional feature vector, which is then used to condition a neural spline flow~\cite{durkan2019neural}. The embedding network consists of a learnable linear layer initialized with principal components of GW simulations followed by a series of dense residual blocks~\cite{he2015deep}. This architecture is a powerful feature extractor for GW measurements~\cite{Dax:2021tsq}. As pointed out in Section~\ref{subsec:fmpe-arch}, it is straightforward to reuse such architectures for FMPE, with the following three modifications: (1) we provide the conditioning on $(t,\theta)$ to the network via gated linear units in each hidden layer; (2) we change the dimension of the final feature vector to the dimension of $\theta$ so that the network parameterizes the conditional vector field $(t,x,\theta)\rightarrow v_{t,x}(\theta)$; (3) we increase the number and width of the hidden layers to use the capacity freed up by removing the discrete normalizing flow.

We train the NPE and FMPE networks with $5\cdot 10^6$ simulations for 400 epochs using a batch size of 4096 on an A100 GPU. The FMPE network ($1.9\cdot 10^8$ learnable parameters, training takes $\approx 2~\text{days}$) is larger than the NPE network ($1.3\cdot 10^8$ learnable parameters, training takes $\approx 3~\text{days}$), but trains substantially faster. We evaluate both networks on GW150914~\cite{Abbott:2016blz}, the first detected GW. We generate a reference posterior using the method described in~\cite{Dax:2022pxd}. Fig.~\ref{fig:GW150914} compares the inferred posterior distributions qualitatively and quantitatively in terms of the Jensen-Shannon divergence (JSD) to the reference.\footnote{We omit the three parameters $\phi_\text{c},\phi_{JL},\theta_{JN}$ in the evaluation as we use phase marginalization in importance sampling and the reference therefore uses a different basis for these parameters~\cite{Dax:2022pxd}. For GNPE we report the results from~\cite{Dax:2021tsq}, which are generated with slightly different data conditioning. Therefore, we do not display the GNPE results in the corner plot, and the JSDs serve only as a rough comparison. The JSD for the $t_\text{c}$ parameter is not reported in~\cite{Dax:2021tsq} due to a $t_\text{c}$ marginalized reference.} 

FMPE substantially outperforms NPE in terms of accuracy, with a mean JSD of $0.5~\text{mnat}$ (NPE: $3.6~\text{mnat}$), and max JSD $<2.0~\text{mnat}$, an indistinguishability criterion for GW posteriors~\cite{Romero-Shaw:2020owr}. Remarkably, FMPE accuracy is even comparable to GNPE, which leverages physical symmetries to simplify data. Finally, we find that the Bayesian evidences inferred with NPE ($\log p(x) = -7667.958\pm 0.006$) and FMPE ($\log p(x) = -7667.969\pm 0.005$) are consistent within their statistical uncertainties. A correct evidence is only obtained in importance sampling when the inferred posterior $q(\theta|x)$ covers the entire posterior $p(\theta|x)$~\cite{Dax:2022pxd}, so this is another indication that FMPE indeed induces mass-covering posteriors.

**Figure:** Results for GW150914~\cite{Abbott:2016blz}. Left: Corner plot showing 1D marginals on the diagonal and 2D 50\% credible regions. We display four GW parameters (distance $d_\text{L}$, time of arrival $t_\text{c}$, and sky coordinates $\alpha,\delta$); these represent the least accurate NPE parameters. Right: Deviation between inferred posteriors and the reference, quantified by the Jensen-Shannon divergence (JSD). The FMPE posterior matches the reference more accurately than NPE, and performs similarly to symmetry-enhanced GNPE. (We do not display GNPE results on the left due to different data conditioning settings in available networks.) \label{fig:GW150914}

## Discussion

Our results for GW150914 show that FMPE substantially outperforms NPE on this challenging problem. We believe that this is related to the network structure as follows. The NPE network allocates roughly two thirds of its parameters to the discrete normalizing flow and only one third to the embedding network (i.e., the feature extractor for $x$). Since FMPE parameterizes just a vector field (rather than a collection of splines in the normalizing flow) it can devote its network capacity to the interpretation of the high-dimensional $x \in \mathbb{R}^{15744}$. Hence, it scales better to larger networks and achieves higher accuracy. Remarkably, the performance iscomparable to GNPE, which involves a much simpler learning task with likelihood symmetries integrated by construction. This enhanced performance, comes in part at the cost of increased inference times, typically requiring hundreds of network forward passes. See Appendix~\ref{sec:appendix-gw} for further details. 

In future work we plan to carry out a more complete analysis of GW inference using FMPE. Indeed, GW150914 is a loud event with good data quality, where NPE already performs quite well. DINGO with GNPE has been validated in a variety of settings~\cite{Dax:2021tsq,Dax:2021myb,Dax:2022pxd,Wildberger:2022agw} including events with a larger performance gap between NPE and GNPE~\cite{Dax:2021myb}. Since FMPE (like NPE) does not integrate physical symmetries, it would likely need further enhancements to fully compete with GNPE. This may require a symmetry-aware architecture~\cite{cohen2016group}, or simply further scaling to larger networks. A straightforward application of the GNPE mechanism to FMPE---GFMPE---is also possible, but less practical due to the higher inference costs of FMPE. Nevertheless, our results demonstrate that FMPE is a promising direction for future research in this field. 

# Conclusions

We introduced flow matching posterior estimation, a new simulation-based inference technique based on continuous normalizing flows. In contrast to existing neural posterior estimation methods, it does not rely on restricted density estimation architectures such as discrete normalizing flows, and instead parametrizes a distribution in terms of a conditional vector field. Besides enabling flexible path specifications, while maintaining direct access to the posterior density, we empirically found that regressing on a vector field rather than an entire distribution improves the scalability of FMPE compared to existing approaches. Indeed, fewer parameters are needed to learn this vector field, allowing for larger networks, ultimately enabling to solve more complex problems. Furthermore, our architecture for FMPE (a straightforward ResNet with GLU conditioning) facilitates parallelization and allows for cheap forward/backward passes.

We evaluated FMPE on a set of 10 benchmark tasks and found competitive or better performance compared to other simulation-based inference methods. On the challenging task of gravitational-wave inference, FMPE substantially outperformed comparable discrete flows, producing samples on par with a method that explicitly leverages symmetries to simplify training. Additionally, flow matching latent spaces are more naturally structured than those of discrete flows, particularly when using paths such as optimal transport. Looking forward, it would be interesting to exploit such structure in designing learning algorithms. This performance and flexibilty underscores the capability of continuous normalizing flows to efficiently solve inverse problems. 

# Acknowledgements

We thank the `Dingo` team for helpful discussions and comments. We would like to particularly acknowledge the contributions of Alessandra Buonanno, Jonathan Gair, Nihar Gupte and Michael Pürrer. This material is based upon work supported by NSF's LIGO Laboratory which is a major facility fully funded by the National Science Foundation. This research has made use of data or software obtained from the Gravitational Wave Open Science Center (gw-openscience.org), a service of LIGO Laboratory, the LIGO Scientific Collaboration, the Virgo Collaboration, and KAGRA. LIGO Laboratory and Advanced LIGO are funded by the United States National Science Foundation (NSF) as well as the Science and Technology Facilities Council (STFC) of the United Kingdom, the Max-Planck-Society (MPS), and the State of Niedersachsen/Germany for support of the construction of Advanced LIGO and construction and operation of the GEO600 detector. Additional support for Advanced LIGO was provided by the Australian Research Council. Virgo is funded, through the European Gravitational Observatory (EGO), by the French Centre National de Recherche Scientifique (CNRS), the Italian Istituto Nazionale di Fisica Nucleare (INFN) and the Dutch Nikhef, with contributions by institutions from Belgium, Germany, Greece, Hungary, Ireland, Japan, Monaco, Poland, Portugal, Spain. The construction and operation of KAGRA are funded by Ministry of Education, Culture, Sports, Science and Technology (MEXT), and Japan Society for the Promotion of Science (JSPS), National Research Foundation (NRF) and Ministry of Science and ICT (MSIT) in Korea, Academia Sinica (AS) and the Ministry of Science and Technology (MoST) in Taiwan. M.D. thanks the Hector Fellow Academy for support. J.H.M. and B.S. are members of the MLCoE, EXC number 2064/1 – Project number 390727645 and the Tübingen AI Center funded by the German Ministry for Science and Education (FKZ 01IS18039A).

\input{main.bbl}

\appendix

# Gaussian flow
\label{sec:appendix-gaussian-flow}

We here derive the form of a vector field $v_t(\theta)$ that restricts the resulting continuous flow to a one dimensional Gaussian with mean $\hat\mu$ variance $\hat\sigma^2$. With the optimal transport path $\mu_t(\theta) = t\theta_1$, $\sigma_t(\theta) = 1 - (1 - \sigma_\text{min}) t \equiv\sigma_t$ from~\cite{flow_matching}, the sample-conditional probability path~\eqref{eq:sample-conditional-path} reads \begin{align}\label{eq:gauss-ot-ptsc} p_t(\theta|\theta_1) = \N[t\theta_1,\sigma_t^2](\theta). \end{align} We set our target distribution \begin{align}\label{eq:gauss-ot-q1} q_1(\theta_1) = \N[\hat\mu,\hat\sigma^2](\theta_1). \end{align} To derive the marginal probability path and the marginal vector field we need two identities for the convolution $\ast$ of Gaussian densities. Recall that the convolution of two function is defined by $f\ast g(x)=\int f(x-y)g(y)\,\d y$. We define the function \begin{align}\label{eq:def_g} g_{\mu,\sigma^2}(\theta) & = \theta\cdot\N\left[\mu,\sigma^2\right](\theta). \end{align} Then the following holds \begin{align}\label{eq:conv1} \N[\mu_1,\sigma_1^2]\ast \N[\mu_2,\sigma_2^2]&= \N[\mu_1+\mu_2,\sigma_1^2+\sigma_2^2]\\ \label{eq:conv2} g_{0,\sigma_1^2}\ast \N[\mu_2,\sigma_2^2] &= \frac{\sigma_1^2}{\sigma_1^2+\sigma_2^2}\left(g_{\mu_2,\sigma_1^2+\sigma_2^2}-\mu_2\N[\mu_2,\sigma_1^2+\sigma_2^2]\right) \end{align} 

### Marginal probability paths

Marginalizing over $\theta_1$ in~\eqref{eq:gauss-ot-ptsc} with~\eqref{eq:gauss-ot-q1}, we find \begin{align}\label{eq:gauss-ot-pt} \begin{split} p_t(\theta) & = \int p_t(\theta|\theta_1) q(\theta_1)\, d\theta_1\\ & = \int \N\left[t\theta_1, \sigma_t^2\right](\theta)\,\N\left[\hat\mu,\hat\sigma^2\right](\theta_1) d\theta_1\\ & = \int \N\left[0, \sigma_t^2\right](\theta - t\theta_1)\, \N(t\hat\mu,(t\hat\sigma)^2)(t\theta_1)\cdot t\, d\theta_1\\ & = \int \N\left[0, \sigma_t^2\right](\theta - \theta^t_1)\, \N\left[t\hat\mu,(t\hat\sigma)^2\right](\theta^t_1)\, d\theta^t_1\\ & = \N\left[t\hat\mu, \sigma_t^2 + (t\hat\sigma)^2\right](\theta) \end{split} \end{align} where we defined $\theta_1^t = t\theta_1$ and used \eqref{eq:conv1}. 

### Marginal vector field

We now calculate the marginalized vector field $u_t(\theta)$ based on equation~(8) in~\cite{flow_matching}. Using the sample-conditional vector field~\eqref{eq:sample-conditional-vector-field} and the distributions~\eqref{eq:gauss-ot-ptsc} and~\eqref{eq:gauss-ot-q1} we find \begin{align} \begin{split} u_t(\theta) & = \int u_t(\theta|\theta_1)\frac{p_t(\theta|\theta_1)q(\theta_1)}{p_t(\theta)}\, d\theta_1\\ & = \frac{1}{p_t(\theta)}\int \frac{(\theta_1 - (1 - \sigma_\text{min})\theta)}{\sigma_t}\cdot \N\left[t\theta_1,\sigma_t^2\right](\theta)\cdot \N\left[\hat\mu,\hat\sigma^2\right](\theta_1)\, d\theta_1\\ & = \frac{1}{p_t(\theta)}\int \frac{\left(\theta_1 - (1 - \sigma_\text{min})\theta\right)}{\sigma_t}\cdot \N\left[0,\sigma_t^2\right](\theta - t\theta_1)\cdot \N\left[t\hat\mu,(t\hat\sigma)^2\right](t\theta_1)\cdot t\, d\theta_1\\ & = \frac{1}{p_t(\theta)}\int \frac{\left(\theta_1' - (1 - \sigma_\text{min})t\cdot\theta\right)} {\sigma_t\cdot t}\cdot \N\left[0,\sigma_t^2\right](\theta - \theta_1')\cdot \N\left[t\hat\mu,(t\hat\sigma)^2\right](\theta_1')\cdot d\theta_1'\\ & = \frac{1}{p_t(\theta)}\int \frac{\left(-\theta_1'' + (1 - (1 - \sigma_\text{min})t)\cdot\theta\right)} {\sigma_t\cdot t}\cdot \N\left[0,\sigma_t^2\right](\theta_1'')\cdot \N\left[t\hat\mu,(t\hat\sigma)^2\right](\theta - \theta_1'')\cdot d\theta_1''\\ & = \frac{1}{p_t(\theta)}\int \frac{\left(-\theta_1'' + \sigma_t\cdot\theta\right)} {\sigma_t\cdot t}\cdot \N\left[0,\sigma_t^2\right](\theta_1'')\cdot \N\left[t\hat\mu,(t\hat\sigma)^2\right](\theta - \theta_1'')\cdot d\theta_1'' \end{split} \end{align} where we used the change of variables $ \theta_1' = t\theta_1$ and $ \theta_1'' = \theta-\theta_1'$. Now we evaluate this expression using \eqref{eq:def_g}, then the identities \eqref{eq:conv1} and \eqref{eq:conv2} and the marginal probability \eqref{eq:gauss-ot-pt} \begin{align}\label{eq:gaussian-vf-ot} \begin{split} u_t(\theta) & = \frac{-1}{p_t(\theta)\cdot \sigma_t\cdot t} \left(g_{0,\sigma_t^2}\ast \N\left[t\hat\mu,(t\hat\sigma)^2\right]\right)(\theta) + \frac{\theta}{p_t(\theta)\cdot t} \left(\N\left[0,\sigma_t^2\right]\ast \N\left[t\hat\mu,(t\hat\sigma)^2\right]\right)(\theta)\\ & = \frac{-1}{p_t(\theta)\cdot \sigma_t\cdot t} \frac{(\theta - t\hat\mu)\cdot\sigma_t^2}{\sigma_t^2 + (t\hat\sigma)^2}\cdot \N\left[t\hat\mu, (\sigma_t^2 + (t\hat\sigma)^2)\right](\theta)+ \frac{\theta}{p_t(\theta)\cdot t} \N\left[t\hat\mu, (\sigma_t^2 + (t\hat\sigma)^2)\right](\theta)\\ & = \frac{(\sigma_t^2 + (t\hat\sigma)^2)\theta-(\theta - t\hat\mu)\cdot\sigma_t} {p_t(\theta)\cdot t\cdot(\sigma_t^2 + (t\hat\sigma)^2)} \cdot p_t(\theta)\\ & = \frac{(\sigma_t^2 + (t\hat\sigma)^2 -\sigma_t)\theta + t\hat\mu\cdot\sigma_t} {t\cdot(\sigma_t^2 + (t\hat\sigma)^2)}. \end{split} \end{align} By choosing a vector field $v_t$ of the form~\eqref{eq:gaussian-vf-ot} with learnable parameters $\hat\mu,\hat\sigma^2$, we can thus define a continuous flow that is restricted to a one dimensional Gaussian.

# Mass covering properties of flows
\label{sec:appendix-mass-covering-fm}

In this supplement, we investigate the mass covering properties of continuous normalizing flows trained using mean squared error and in particular prove Theorem~\ref{theorem:mass-coverage}. We first recall the notation from the main part. We always assume that the data is distributed according to $p_1(\theta)$. In addition, there is a known and simple base distribution $p_0$ and we assume that there is a vector field $u_t:[0,1]\times \R^d\to \R^d$ that connects $p_0$ and $p_1$ in the following sense. We denote by $\phi_t$ the flow generated by $u_t$, i.e., $\phi_t$ satisfies \begin{align} \partial_t \phi_t(\theta)=u_t(\phi_t(\theta)). \end{align} Then we assume that $(\phi_1)_\ast p_0=p_1$ and we also define the interpolations $p_t=(\phi_t)_\ast p_0$.

We do not have access to the ground truth distributions $p_t$ and the vector field $u_t$ but we try to learn a vector field $v_t$ approximating $u_t$. We denote its flow by $\psi_t$ and we define $q_t = (\psi_t)_\ast q_0$ and $q_0=p_0$. We are interested in the mass covering properties of the learned approximation $q_1$ of $p_1$. In particular, we want to relate the KL-divergence $\KL(p_1||q_1)$ to the mean squared error, \begin{align} \MSE_p(u,v)=\int_0^1 \d t\, \int p_t(\d\theta)(u_t(\theta)-v_t(\theta))^2, \end{align} of the generating vector fields. The first observation is that without any regularity assumptions on $v_t$ it is impossible to obtain any bound on the KL-divergence in terms of the mean squared error.

\begin{lemma}\label{le:holes} For every $\eps>0$ there are vector field $u_t$ and $v_t$ and a base distribution $p_0=q_0$ such that \begin{align} \MSE_p(u,v)<\eps \text{ and } \KL(p_1||q_1)=\infty. \end{align} In addition we can construct $u_t$ and $v_t$ such that the support of $p_1$ is larger than the support of $q_1$. \end{lemma} \begin{proof} We consider the uniform distribution $p_0=q_0\sim\mc{U}([-1,1])$ and the vector fields \begin{align} u_t(\theta) = 0 \end{align} and \begin{align} v_t(\theta) = \begin{cases} \eps \quad \text{for $0\leq \theta< \eps$,} \\ 0 \quad \text{otherwise}. \end{cases} \end{align} As before, let $\phi_t$ denote the flow of the vector field $u_t$ and similarly $\psi_t$ denote the flow of $v_t$. Clearly $\phi_t (\theta)=\theta$. On the other hand \begin{align} \psi_t(\theta) = \begin{cases} \min(\theta + \eps t , \eps) \quad \text{if $0\leq \theta<\eps$,} \\ \theta \quad \text{otherwise}. \end{cases} \end{align} In particular \begin{align} \psi_1(\theta) = \begin{cases} \eps \quad \text{if $0\leq \theta<\eps$,} \\ \theta \quad \text{otherwise}. \end{cases} \end{align} This implies that $p_1=(\phi_1)_\ast p_0 \sim \mc{U}([-1,1])$. On the other hand $q_1 =(\psi_1)_\ast q_0$ has support in $[-1,0]\cup [\eps, 1]$. In particular, the distribution of $q_1$ is not mass covering with respect to $p_1$ and $\KL (p_1||q_1)=\infty$. Finally, we observe that the MSE can be arbitrarily small \begin{align} \MSE_p(u,v)= \int_0^1 \d t \int p_t(\d \theta) |u_t(\theta)-v_t(\theta)|^2 = \int_0^1 \int_0^\eps \frac12 \eps^2 =\frac{\eps^3}{2}. \end{align} Here we used that the density of $p_t(\d \theta)$ is $1/2$ for $-1\leq \theta\leq 1$. \end{proof} We see that an arbitrary small MSE-loss cannot ensure that the probability distribution is mass covering and the KL-divergence is finite. On a high level this can be explained by the fact that for vector fields $v_t$ that are not Lipschitz continuous the flow is not necessarily continuous, and we can generate holes in the distribution. Note that we chose $p_0$ to be a uniform distribution for simplicity, but the result extends to any smooth distribution, in particular the result does not rely on the discontinuity of $p_0$.

Next, we investigate the mass covering property for Lipschitz continuous flows. When the flows $u_t$ and $v_t$ are Lipschitz continuous (in $\theta$) this ensures that the flows $\psi_1$ and $\phi_1$ are continuous in $x$ and it is not possible to create holes in the distribution as shown above for non-continuous vector fields. We show a weaker bound in this setting. \begin{lemma}\label{le:lipschitz} For every $0\leq \delta \leq 1$ there is a base distribution $p_0=q_0$ and the are Lipschitz-continuous vector fields $u_t$ and $v_t$ such that $\MSE_p(u,v)=\delta$ and \begin{align} \KL(p_1||q_1)\geq \frac12 \MSE_p(u,v)^{1/3}. \end{align} \end{lemma} \begin{proof} We consider $p_0$, $q_0$ and $u_t$ as in Lemma~\ref{le:holes}, and we define \begin{align} v_t(\theta) = \begin{cases} 2\theta \quad \text{for $0\leq \theta< \eps$,} \\ 2\eps - \theta \quad \text{for $\eps\leq \theta< 2\eps$,} \\ 0 \quad \text{otherwise}. \end{cases} \end{align} Then we can calculate for $0\leq \theta \leq e^{-2}\eps$ that \begin{align} \psi_t(\theta) = \theta e^{2t}. \end{align} Similarly we obtain for $\eps\leq \theta\leq 2\eps$ (solving the ODE $f'=2f$) \begin{align} \psi_t(\theta) = 2\eps - (2\eps - \theta) e^{-2t}. \end{align} We find \begin{align}\label{eq:Psi_evals} \psi_1(0)=0,\; \psi_1(e^{-2}\eps)=\eps, \; \psi_1(\eps)=2-\eps e^{-2}; \; \psi_2(2\eps)=2\eps. \end{align} Next we find for the densities of $q_1$ that \begin{align} q_1(\psi_1(\theta)) = q_0(\theta) |\psi_1'(\theta)|^{-1} =\frac12 \begin{cases} e^{-2} \quad \text{for $0\leq \theta \leq e^{-2}\eps$,} \\ e^{2} \quad \text{for $\eps\leq \theta \leq 2\eps$.} \end{cases} \end{align} Together with \eqref{eq:Psi_evals} this implies that the density of $q_1$ is given by \begin{align} q_1(\theta) =\frac12 \begin{cases} e^{-2} \quad \text{for $0\leq \theta \leq \eps$,} \\ e^{2} \quad \text{for $2\eps-\eps e^{-2}\leq \theta \leq 2\eps$.} \end{cases} \end{align} Note that $p_1(\theta)=1/2$ for $-1\leq \theta\leq 1$ and therefore \begin{align} \int_0^\eps \ln \frac{p_1(\theta)}{q_1(\theta)} p_1(\d \theta) = \int_0^\eps \ln(e^2)\frac12 \d \theta = \eps, \end{align} and \begin{align} \int_{2\eps-\eps e^{-2}}^{2\eps} \ln \frac{p_1(\theta)}{q_1(\theta)} p_1(\d \theta) = \int_{2\eps-\eps e^{-2}}^{2\eps} \ln(e^{-2})\frac12 \d \theta =- \eps e^{-2} . \end{align} Moreover we note \begin{align} \int_\eps^{2\eps-\eps e^{-2}} q_1(\d \eps) = \int_{e^{-2}\eps}^{\eps} q_0(\d\eps)=\frac12 \eps(1-e^{-2}) =\int_\eps^{2\eps-\eps e^{-2}} p_1(\d \eps), \end{align} which implies (by positivity of the KL-divergence) that \begin{align} \int_\eps^{2\eps-\eps e^{-2}} \ln\left(\frac{p_1(\theta)}{q_1(\theta)}\right) p_1(\d \theta)\geq 0. \end{align} We infer using also $p_1(\theta)=q_1(\theta)=1/2$ for $\theta\in [-1, 0]\cap[2\eps,1]$ that \begin{align} \KL (p_1||q_1) =\int \ln\left(\frac{p_1(\theta)}{q_1(\theta)}\right) p_1(\d \theta)\geq \eps (1-e^{-2}). \end{align} On the other hand we can bound \begin{align} \int_0^1 \d t \int p_t(\d\theta) |v_t(\theta)-u_t(\theta)|^2 = \frac12 \int_0^1 \d t \int_{0}^{2\eps} |u_t(\theta)|^2 =\int_0^{\eps} s^2\, \d s =\frac{\eps^3}{3}. \end{align} We conclude that \begin{align} \KL (p_1||q_1)\geq \frac12 \left(\mathrm{MSE}_p(u, v)\right)^{1/3}. \end{align} In particular, it is not possible to bound the KL-divergence by the MSE even when the vector fields are Lipschitz continuous. \end{proof} Let us put this into context. It was already shown in \cite{albergo2023stochastic} that we can, in general, not bound the forward KL-divergence by the mean squared error and our Lemmas~\ref{le:holes} and \ref{le:lipschitz} are concrete examples. On the other hand, when considering SDEs the KL-divergence can be bounded by the mean squared error of the drift terms as shown in \cite{song2021maximum}. Indeed, in \cite{albergo2023stochastic} the favorable smoothing effect was carefully investigated.

Here we show that we can alternatively obtain an upper bound on the KL-divergence when assuming that $u_t$, $v_t$, and $p_0$ satisfy additional regularity assumptions. This allows us to recover the mass covering property from bounds on the means squared error for sufficiently smooth vector fields. The scaling is nevertheless still weaker than for SDEs.

We now state our assumptions. We denote the gradient with respect to $\theta$ by $\nabla = \nabla_\mu$ and second derivatives by $\nabla^2=\nabla^2_{\mu\nu}$. When applying the chain rule, we leave the indices implicit. We denote by $|\cdot|$ the Frobenius norm $|A|=\left(\sum_{ij} A_{ij}^2\right)^{1/2}$ of a matrix. The Frobenius norm is submultiplicative, i.e., $|AB|\leq |A|\cdot |B|$ and directly generalizes to higher order tensors. \begin{assumption}\label{as:1} We assume that \begin{align} |\nabla u_t |\leq L, \; |\nabla v_t|\leq L, \; |\nabla ^2 u_t|\leq L',\; |\nabla ^2 v_t|\leq L'. \end{align} \end{assumption} We require one further assumption on $p_0$. \begin{assumption}\label{as:2} There is a constant $C_1$ such that \begin{align}\label{eq:cond_bound_score} | \nabla \ln p_0(\theta)|\leq C_1(1 + |\theta|). \end{align} We also assume that \begin{align}\label{eq:cond_square_integrable} \Ex_{p_0} |\theta|^2 <C_2< \infty. \end{align} \end{assumption} Note that \eqref{eq:cond_bound_score} holds, e.g., if $p_0$ follows a Gaussian distribution but also for smooth distribution with slower decay at $\infty$. If we assume that $| \nabla \ln p_0(\theta)|$ is bounded the proof below simplifies slightly. This is, e.g., the case if $p_0(\theta)\sim e^{-|\theta|}$ as $|\theta|\to \infty$.

We need some additional notation. It is convenient to introduce $\phi^s_t = \phi_t \circ (\phi_s)^{-1}$, i.e., the flow from time $s$ to $t$ (in particular $\phi^0_t=\phi_t$) and similarly for $\psi$. We can now restate and prove Theorem~\ref{theorem:mass-coverage}. \begin{theorem} Let $p_0=q_0$ and assume $u_t$ and $v_t$ are two vector fields whose flows satisfy $p_1=(\phi_1)_\ast p_0$ and $q_1=(\psi_1)_\ast q_0$. Assume that $p_0$ satisfies Assumption~\ref{as:2} and $u_t$ and $v_t$ satisfy Assumption~\ref{as:1}. Then there is a constant $C>0$ depending on $L$, $L'$, $C_1$, $C_2$, and $d$ such that (for $\MSE_p(u,v)<1)$) \begin{align} \KL(p_1||q_1)\leq C \MSE_p(u,v)^{\frac12}. \end{align}

\end{theorem} \begin{remark} We do not claim that our results are optimal, it might be possible to find similar bounds for the forward KL-divergence with weaker assumptions. However, we emphasize that Lemma~\ref{le:lipschitz} shows that the result of the theorem is not true without the assumption on the second derivative of $v_t$ and $u_t$. \end{remark} \begin{proof}

We want to control $\KL(p_1||q_1)$. It can be shown that (see equation above (25) in \cite{song2021maximum} or Lemma~2.19 in \cite{albergo2023stochastic} ) \begin{align} \partial_t \KL(p_t||q_t) = -\int p_t(\d \theta) (u_t(\theta)-v_t(\theta)) \cdot (\nabla \ln p_t(\theta)- \nabla \ln q_t(\theta)). \end{align} Using Cauchy-Schwarz we can bound this by \begin{align}\label{eq:partial_KL} \partial_t \KL(p_t||q_t) \leq \left(\int p_t(\d \theta) |u_t(\theta)-v_t(\theta)|^2\right)^{\frac12} \left( \int p_t(\d \theta) |\nabla \ln p_t(\theta)- \nabla \ln q_t(\theta)|^2 \right)^{\frac12}. \end{align} We use the relation (see \eqref{eq:cnf-density}) \begin{align} \ln (p_t(\phi_t(\theta_0)) = \ln(p_0(\theta_0)) - \int_0^t (\Div u_s)(\phi_s(\theta_0)) \d s, \end{align} which can be equivalently rewritten (setting $\theta=\phi_t \theta_0$) as \begin{align} \ln(p_t(\theta)) = \ln(p_0(\phi^t_0\theta)) - \int_0^t (\Div u_s)(\phi_s^t\theta) \d s. \end{align} We use the following relation for $\nabla \phi_s^t$ \begin{align} \nabla \phi_s^t(\theta) = \exp\left(\int_t^s\d \tau\, (\nabla u_\tau)(\phi^t_\tau(\theta))\right). \end{align} This relation is standard and can be directly deduced from the following ODE for $\nabla \phi_s^t$ \begin{align} \partial_s \nabla \phi_s^t(\theta) = \nabla \partial_s\phi_s^t(\theta) = \nabla (u_s(\phi_s^t(\theta))) = \left((\nabla u_s)(\phi_s^t(\theta))\right) \cdot \nabla \phi_s^t(\theta). \end{align} We can conclude that for $0\leq s,t\leq 1$ the bound \begin{align}\label{eq:bound_D_flow} | \nabla \phi_s^t(\theta)|\leq e^L \end{align} holds. We find \begin{align} \begin{split}\label{eq:bound_score} | \nabla \ln(p_t(\theta))| &= \left| \nabla \ln(p_0)(\phi^t_0\theta)\cdot \nabla \phi^t_0(\theta) - \int_0^t (\nabla \Div u_s)(\phi_s^t\theta)\cdot \nabla \phi^t_s (\theta)\d s\right| \\ &\leq |\nabla \ln(p_0)(\phi^t_0\theta) | e^L +L'e^L, \end{split} \end{align} and a similar bound holds for $q_t$. In words, we have shown that the score of $p_t$ at $\theta$ can be bounded by the score of $p_0$ of theta transported along the vector field $u_t$ minus a correction which quantifies the change of score along the path. We now bound using the definition $p_t=(\phi_t)_\ast p_0$ and the assumption \eqref{eq:cond_bound_score} \begin{align} \begin{split}\label{eq:bound_dq0_final} \int p_t(\d \theta) |\nabla\ln p_0(\phi^t_0(\theta))|^2 &= \int p_0(\d \theta_0) |\nabla\ln p_0(\phi^t_0\phi_t(\theta_0))|^2 = \Ex_{p_0} |\nabla\ln p_0(\theta_0)|^2 \\ &\leq \Ex_{p_0} (C_1 (1 + |\theta_0|)^2) \leq 2C_1^2 (1 + \Ex_{p_0} |\theta_0|^2)\leq 2C_1^2(1+C_2^2). \end{split} \end{align} Similarly we obtain using $q_0=p_0$ \begin{align}\label{eq:bound_qt_score_p_t} \int p_t(\d \theta) |\nabla\ln q_0(\psi^t_0\theta)|^2 = \int p_0(\d \theta_0) | \nabla\ln q_0(\psi^t_0\phi_t\theta_0)|^2 . \end{align} In words, to control the score of $q$ integrated with respect to $p_t$ we need to control the distortion we obtain when moving forward with $u$ and backwards with $v$. We investigate $\psi^t_0\phi_t(\theta_0)$. We now show \begin{align}\label{eq:forward_backward} \partial_h \psi^{t+h}_t \phi_{t+h}^t(\theta)|_{h=0} = u_t(\theta) - v_t(\theta). \end{align} First, by definition of $\phi$, we find \begin{align}\label{eq:forward} \partial_h \phi_{t+h}^t(\theta)|_{h=0} = \partial_h \phi_{t+h} \phi_t^{-1}(\theta)|_{h=0} = u_t(\phi_t \phi_t^{-1}(\theta))=u_t(\theta). \end{align} To evaluate the second contribution we observe \begin{align}\label{eq:backward} \begin{split} 0&= \partial_h \theta|_{h=0}=\partial_h \psi^{t+h}_{t+h} (\theta)|_{h=0} =\partial_h \psi_{t+h} \psi_{t+h}^{-1} (\theta) |_{h=0} \\ &= (\partial_h \psi_{t+h}) \psi_{t}^{-1} (\theta) |_{h=0} + \psi_{t} (\partial_h \psi_{t+h}^{-1}) (\theta) |_{h=0} =v_t(\psi_t\psi_{t}^{-1} (\theta)) + \partial_h \psi_{t} \psi_{t+h}^{-1} (\theta) |_{h=0} \\ &=v_t(\theta) + \partial_h \psi_{t}^{t+h} (\theta) |_{h=0} \end{split} \end{align} Now \eqref{eq:forward_backward} follows from \eqref{eq:forward} and \eqref{eq:backward} together with $\phi_t^t=\psi_t^t=\mathrm{Id}$. Using \eqref{eq:forward_backward} we find \begin{align} \partial_t (\psi^t_0 \phi_t) (\theta_0) = \partial_h (\psi^t_0 \psi^{t+h}_t \phi_{t+h}^t \phi_t)(\theta_0)|_{h=0} = (\nabla \psi^t_0)(\phi_t (\theta_0))\cdot \left((u_t-v_t)(\phi_t(\theta_0))\right). \end{align} Using \eqref{eq:bound_D_flow} we conclude that \begin{align} \begin{split} | \psi^t_0 \phi_t (\theta_0)-\theta_0| &\leq \left| \int_0^t \partial_s \psi^s_0 \phi_s (\theta_0)\, \d s \right| \leq \int_0^t |(\nabla \psi^s_0)(\phi_s (\theta_0))| \cdot |u_s-v_s|(\phi_s(\theta_0))\, \d s \\ & \leq e^L \int_0^t |u_s-v_s|(\phi_s(\theta_0))\, \d s. \end{split} \end{align} We use this and the assumption \eqref{eq:cond_bound_score} to continue to estimate \eqref{eq:bound_qt_score_p_t} as follows \begin{align} \begin{split} \int p_t(\d \theta) |\nabla \ln q_0(\psi^t_0\theta)|^2 &= \int p_0(\d \theta_0) |\nabla\ln q_0(\psi^t_0\phi_t(\theta_0))|^2 \\ &\leq C_1^2 \int p_0(\d \theta_0) (1 +|\psi^t_0\phi_t(\theta_0)|)^2 \\ &\leq C_1^2 \int p_0(\d \theta_0) (1 +|\psi^t_0\phi_t(\theta_0)-\theta_0| + |\theta_0|)^2 \\ &\leq 3C_1^2 +3C_1^2 \int p_0(\d \theta_0) \left(|\psi^t_0\phi_t(\theta_0)-\theta_0|^2 + |\theta_0|^2\right) \\ &\leq 3C_1^2(1 + \Ex_{p_0}|\theta_0|^2) +3C_1^2e^{2L} \int p_0(\d \theta_0) \left(\int_0^t \d s\, |u_s-v_s|(\phi_s(\theta_0))\right)^2. \end{split} \end{align} Here we used $(a+b+c)^2\leq 3(a^2+b^2+c^2)$ in the second to last step. We bound the remaining integral using Cauchy-Schwarz as follows \begin{align} \begin{split} \int p_0(\d \theta_0) \left(\int_0^t |u_s-v_s|(\phi_s(\theta_0))\right)^2 &\leq \int p_0(\d \theta_0) \left(\int_0^t \d s\, |u_s-v_s|^2(\phi_s(\theta_0))\right) \left(\int_0^t \d s\, 1^2 \right) \\ & \leq t \int_0^t \d s \int p_0(\d \theta_0) |u_s-v_s|^2(\phi_s(\theta_0)) \\ &= t \int_0^t \d s \int p_s(\d \theta_s) |u_s-v_s|^2(\theta_s) \\ &\leq \int_0^1 \d s \int p_s(\d \theta_s) |u_s-v_s|^2(\theta_s)=\MSE_p(u,v) . \end{split} \end{align} The last displays together imply \begin{align}\label{eq:bound_dp0_final} \int p_t(\d \theta) |\nabla\ln q_0(\psi^t_0\theta)|^2 \leq 3C_1^2\left(1 + \Ex_{p_0}|\theta_0|^2+e^{2L} \MSE_p(u,v)\right). \end{align} Now we have all the necessary ingredients to bound the derivative of the KL-divergence. We control the second integral in \eqref{eq:partial_KL} using \eqref{eq:bound_score} (and again $(\sum_{i=1}^4 a_i)^2\leq 4\sum a_i^2$) as follows, \begin{align} \begin{split} \int p_t(\d \theta)& |\nabla \ln p_t(\theta)- \nabla \ln q_t(\theta)|^2 \\&\leq 2\cdot 2^2 \cdot L'^2 e^{2L}+ 4e^{2L} \int p_t(\d \theta) \left(|\nabla\ln q_0(\psi^t_0)\theta)|^2 + |\nabla\ln p_0(\phi^t_0)\theta)|^2\right). \end{split} \end{align} Using \eqref{eq:bound_dq0_final} and \eqref{eq:bound_dp0_final} we finally obtain \begin{align} \begin{split} \int p_t(\d \theta) |\nabla \ln p_t(\theta)- \nabla \ln q_t(\theta)|^2 & \leq 8 \cdot L'^2 e^{2L} + C_1^2 e^{2L}\left( 20 (1+C_2^2) +12 \MSE_p(u,v)\right) \\ &\leq C(1+\MSE_p(u,v)) \end{split} \end{align} for some constant $C>0$. Finally, we obtain \begin{align} \begin{split} \KL(p_1||q_1) &=\int_0^1 \d t\, \partial_t \KL(p_t||q_t) \\ &\leq (C(1+\MSE_p(u,v)))^{\frac12} \int_0^1\d t\, \left(\int p_t(\d \theta) |u_t(\theta)-v_t(\theta)|^2\right)^{\frac12} \\ &\leq (C(1+\MSE_p(u,v)))^{\frac12} \left( \int_0^1\d t\, \int p_t(\d \theta) |u_t(\theta)-v_t(\theta)|^2\right)^{\frac12} \\ &\leq (C(1+\MSE_p(u,v)))^{\frac12} \MSE_p(u,v)^{\frac12}. \end{split} \end{align} \end{proof} 

# SBI Benchmark
\label{sec:appendix-sbibm}

In this section, we collect missing details and additional results for the analysis of the SBI benchmark in Section~\ref{sec:sbibm}. 

## Network architecture and hyperparameters

For each task and simulation budget in the benchmark, we perform a mild hyperparameter optimization. We sweep over the batch size and learning rate (which is particularly important as the simulation budgets differ by orders of magnitudes), the network size and the $\alpha$ parameter for the time prior defined in Section~\ref{subsec:t-weighting} (see Tab.~\ref{tab:sbibm-hyperparameters} for the specific values). We reserve 5\% of the simulation budget for validation and choose the model with the best validation loss across all configurations.

## Additional results

We here provide various additional results for the SBI benchmark. First, we compare the performance of FMPE and NPE when using the Maximum Mean Discrepancy metric (MMD). The results can be found in Fig.~\ref{fig:benchmark-mmd}. FMPE shows superior performance to NPE for most tasks and simulation budgets. Compared to the C2ST scores in Fig.~\ref{fig:benchmark-c2st} the improvement shown by FMPE in MMD is more substantial.

Fig.~\ref{fig:benchmark-SMPE} compares the FMPE results with the optimal transport path from the main text with a comparable score matching model using the Variance Preserving diffusion path \cite{song2020score}. The score matching results were obtained using the same batch size, network size and learning rate as the FMPE network, while optimizing for $\beta_{\text{min}}\in \{0.1, 1, 4\}$ and $\beta_{\text{max}}\in \{ 4, 7, 10\}$. FMPE with the optimal transport path clearly outperforms the score-based model on almost all configurations. 

In Fig.~\ref{fig:benchmark-theta-embedding} we compare FMPE using the architecture proposed in Section~\ref{subsec:fmpe-arch} with $(t,\theta)$-conditioning via gated linear units to FMPE with a naive architecture operating directly on the concatenated $(t,\theta,x)$ vector. For the two displayed tasks the context dimension $\dim(x) = 100$ is much larger than the parameter dimension $\dim(\theta) \in \{5, 10 \}$, and there is a clear performance gain in using the GLU conditioning. Our interpretation is that the low dimensionality of $(t, \theta)$ means that it is not well-learned by the network when simply concatenated with $x$.

Fig.~\ref{fig:sbibm-logprobs-all} displays the densities of the reference samples under the FMPE model as a histogram for all tasks (extended version of Fig.~\ref{fig:sbibm-logprobs}). The support of the learned model $q(\theta |x)$ covers the reference samples $\theta \sim p(\theta | x)$, providing additional empirical evidence for the mass-covering behavior theoretically explored in Thm.~\ref{theorem:mass-coverage}. However, samples from the true posterior distribution may have a small density under the learned model, especially if the deviation between model and reference is high; see Lotka-Volterra (bottom right panel). Fig.~\ref{fig:p-p} displays P--P plots for two selected tasks.

Finally, we study the impact of our time prior re-weighting for one example task in Fig.~\ref{fig:ablation-time-prior}. We clearly see that our proposed re-weighting leads to increased performance by up-weighting samples for $t$ closer to $1$ during training. 

**Table:** Sweep values for the hyperparamters for the SBI benchmark. We split the configurations according to simulation budgets, e.g.\ for 1000 simulations, we only swept over smaller values for network size and batch size. The network architecture has a diamond shape, with increasing layer width from smallest to largest and then decreasing to the output dimension. Each block consists of two fully-connected residual layers. \label{tab:sbibm-hyperparameters}

| hyperparameter | sweep values |
| --- | --- |
| hidden dimensions | $2^n$ for $n \in \{4, \ldots, 10\}$ |
| number of blocks | $ 10, \ldots, 18$ |
| batch size | $2^n$ for $n \in \{2, \ldots, 9\}$ |
| learning rate | 1.e-3, 5.e-4, 2.e-4, 1.e-4 |
| $\alpha$ (for time prior) | -0.25, -0.5, 0, 1, 4 |

**Figure:** Comparison of FMPE and NPE performance across 10 SBI benchmarking tasks~\cite{lueckmann2021benchmarking}. We here quantify the deviation in terms of the Maximum Mean Discrepancy (MMD) as an alternative metric to the C2ST score used in Fig.~\ref{fig:benchmark-c2st}. MMD can be sensitive to its hyperparameters~\cite{lueckmann2021benchmarking}, so we use the C2ST score as a primary performance metric. \label{fig:benchmark-mmd}

**Figure:** Comparison of FMPE with the optimal transport path (as used throughout the main paper) with comparable models trained with a Variance Preserving diffusion path \cite{song2020score} by regressing on the score (SMPE). Note that the SMPE baseline shown here is not directly comparable to NPSE~\cite{sharrock2022sequential,geffner2022score}, as this method uses Langevin steps, which reduces the dependence of the results on the vector field for small $t$ (at the cost of a tractable density). \label{fig:benchmark-SMPE}

**Figure:** Comparison of the architecture proposed in Section~\ref{subsec:fmpe-arch} with gated linear units for the $(t,\theta)$-conditioning (red) and a naive architecture based on a simple concatenation of $(t, \theta, x)$ (black). FMPE with the proposed architecture performs substantially better. \label{fig:benchmark-theta-embedding}

**Figure:** Histogram of FMPE densities $\log q(\theta|x)$ for samples $\theta\sim q(\theta|x)$ and reference samples $\theta\sim p(\theta|x)$ for simulation budgets $N=10^3$ (left), $N=10^4$ (center) and $N=10^5$ (right). The reference samples $\theta\sim p(\theta|x)$ are all within the support of the learned model $q(\theta|x)$, indicating mass covering FMPE results. Nonetheless, reference samples may have a small density under $q(\theta|x)$, if the validation loss is high, see Lotka-Volterra (LV). \label{fig:sbibm-logprobs-all}

**Figure:** P-P plot for the marginals of the FMPE-posterior for the Two Moons (upper) and SLCP (lower) tasks for training budgets of $10^3$ (left), $10^4$ (center), and $10^5$ (right) samples. \label{fig:p-p}

**Figure:** Comparison of the time prior re-weighting proposed in Section~\ref{subsec:t-weighting} with a uniform prior over $t$ on the Two Moons task (Section~\ref{sec:sbibm}). The network trained with the re-weighted prior clearly outperforms the reference on all simulation budgets. \label{fig:ablation-time-prior}

\FloatBarrier 

# Gravitational-wave inference
\label{sec:appendix-gw}

We here provide the missing details and additional results for the gravitational wave inference problem analyzed in Section~\ref{sec:gw}. 

## Network architecture and hyperparameters
\label{sec:appendix-architecture}

Compared to NPE with normalizing flows, FMPE allows for generally simpler architectures, since the output of the network is simply a vector field. This also holds for NPSE (model also defined by a vector) and NRE (defined by a scalar). Our FMPE architecture builds on the embedding network developed in~\cite{Dax:2021tsq}, however we extend the network capacity by adding more residual blocks (Tab.~\ref{tab:fmpe-gw-hyperparameters}, top panel). For the $(t,\theta)$-conditioning we use gated linear units applied to each residual block, as described in Section~\ref{subsec:fmpe-arch}. We also use a small residual network to embed $(t,\theta)$ before applying the gated linear units.

In this Appendix we also perform an ablation study, using the *same* embedding network as the NPE network (Tab.~\ref{tab:fmpe-gw-hyperparameters}, bottom panel). For this configuration, we additionally study the effect of conditioning on $(t, \theta)$ starting from different layers of the main residual network.

**Table:** Hyperparameters for the FMPE models used in the main text (top) and in the ablation study (bottom, see Fig.~\ref{fig:jsds-ablation}). The network is composed of a sequence of residual blocks, each consisting of two fully-connected hidden layers, with a linear layer between each pair of blocks. The ablation network is the same as the embedding network that feeds into the NPE normalizing flow. \label{tab:fmpe-gw-hyperparameters}

| } hyperparameter | values |
| --- | --- |
| residual blocks | $2048$, $4096 \times 3$, $2048 \times 3$, $1024 \times 6$, $512 \times 8$, $256 \times 10$, |
| | $128 \times 5$, $64 \times 3$, $32 \times 3$, $16 \times 3$ |
| residual blocks $(t, \theta)$ embedding | $16, 32, 64, 128, 256$ |
| batch size | 4096 |
| learning rate | 5.e-4 |
| $\alpha$ (for time prior) | 1 |
| residual blocks | $2048 \times 2$, $1024 \times 4$, $512 \times 4$, $256 \times 4$, $128 \times 4$, $64 \times 3$, |
| | $32 \times 3$, $16 \times 3$ |
| residual blocks $(t, \theta)$ embedding | $16, 32, 64, 128, 256$ |
| batch size | 4096 |
| learning rate | 5.e-4 |
| $\alpha$ (for time prior) | 1 |

## Data settings

We use the data settings described in~\cite{Dax:2021tsq}, with a few minor modifications. In particular, we use the waveform model IMRPhenomPv2~\cite{Hannam:2013oca,Khan:2015jqa,Bohe:2016} and the prior displayed in Tab.~\ref{tab:GW-parameters-with-priors}. Generation of the training dataset with 5,000,000 samples takes around 1 hour on 64 CPUs. Compared to~\cite{Dax:2021tsq}, we reduce the frequency range from $[20,1024]~\text{Hz}$ to $[20,512]~\text{Hz}$ to reduce the computational load for data preprocessing. We also omit the conditioning on the detector noise power spectral density (PSD) introduced in~\cite{Dax:2021tsq} as we evaluate on a single GW event. Preliminary tests show that the performance with PSD conditioning is similar to the results reported in this paper. All changes to the data settings have been applied to FMPE and the NPE baselines alike to enable a fair comparison. 

**Table:** Priors for the astrophysical binary black hole parameters. Priors are uniform over the specified range unless indicated otherwise. Our models infer the mass parameters in the basis $(M_c,q)$ and marginalize over the phase parameter $\phi_c$. \label{tab:GW-parameters-with-priors}

| Description | Parameter | Prior |
| --- | --- | --- |
| component masses | $m_1$, $m_2$ | $[10,120]~\mathrm{M}_\odot$, $m_1\geq m_2$ |
| chirp mass | $M_c=(m_1 m_2)^{\frac{3}{5}} / (m_1 + m_2)^{\frac{1}{5}}$ | $[20,120]~\mathrm{M}_\odot$ (constraint) |
| mass ratio | $q=m_2 / m_1$ | [0.125, 1.0] (constraint) |
| spin magnitudes | $a_1$, $a_2$ | $[0,0.99]$ |
| spin angles | $\theta_1$, $\theta_2$, $\phi_{12}$, $\phi_{JL}$ | standard as in~\cite{Farr:2014qka} |
| time of coalescence | $t_c$ | $[-0.03,0.03]$~s |
| luminosity distance | $d_L$ | $[100,1000]$~Mpc |
| reference phase | $\phi_c$ | $[0,2\pi]$ |
| inclination | $\theta_{JN}$ | $[0,\pi]$ uniform in sine |
| polarization | $\psi$ | $[0,\pi]$ |
| sky position | $\alpha, \beta$ | uniform over sky |

## Additional results
\label{subsec:appendix-gw-results}

Tab.~\ref{tab:inference} displays the inference times for FMPE and NPE. NPE requires only a single network pass to produce samples and (log-)probabilities, whereas many forwards passes are needed for FMPE to solve the ODE with a specific level of accuracy. A significant portion of the additional time required for calculating (log-)probabilities in conjunction with the samples is spent on computing the divergence of the vector field, see Eq.~\eqref{eq:cnf-density}.

Fig.~\ref{fig:jsds-ablation} presents a comparison of the FMPE performance using networks of the same hidden dimensions as the NPE embedding network (Tab.~\ref{tab:fmpe-gw-hyperparameters} bottom panel). This comparison includes an ablation study on the timing of the $(t, \theta)$ GLU-conditioning. In the top-row network, the $(t, \theta)$ conditioning is applied only after the 256-dimensional blocks. In contrast, the middle-row network receives $(t, \theta)$ immediately after the initial residual block. With FMPE we can achieve performance comparable to NPE, while having only $\approx 1/3$ of the network size (most of the NPE network parameters are in the flow). This suggests that parameterizing the target distribution in terms of a vector field requires less learning capacity, compared to directly learning its density. Delaying the $(t, \theta)$ conditioning until the final layers impairs performance. However, the number of FLOPs at inference is considerably reduced, as the context embedding can be cached and a network pass only involves the few layers with the $(t, \theta)$ conditioning. Consequently, there's a trade-off between accuracy and inference speed, which we will explore in a greater scope in future work. 

**Table:** Inference times per batch for FMPE and NPE on a single Nvidia A100 GPU, using the training batch size of 4096. We solve the ODE for FMPE using the \texttt{dopri5} discretization \cite{DORMAND198019} with absolute and relative tolerances of 1e-7. For FMPE, generation of the (log-)probabilities additionally requires the computation of the divergence, see equation~\eqref{eq:cnf-density}. This needs additional memory and therefore limits the maximum batch size that can be used at inference. \label{tab:inference}

| lrrr@{}} | Network Passes | Inference Time (per batch) |
| --- | --- | --- |
| FMPE (sample only) | 248 | 26s |
| [0.5ex] FMPE (sample and log probs) | 350 | 352s |
| [0.5ex] NPE (sample and log probs) | 1 | 1.5s |
| [0.5ex] | | |

**Figure:** Jensen-Shannon divergence between inferred posteriors and the reference posteriors for GW150914 \cite{Abbott:2016blz}. We compare two FMPE models with the same architecture as the NPE embedding network, see Tab.~\ref{tab:fmpe-gw-hyperparameters} bottom panel. For the model in the first row, the GLU conditioning of $(\theta, t)$ is only applied before the final 128-dim blocks. The model in the middle row is given the context after the very first 2048 block. \label{fig:jsds-ablation}
