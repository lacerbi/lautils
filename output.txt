\addtocontents{toc}{\protect\setcounter{tocdepth}{0}}

# Amortized Probabilistic Conditioning for Optimization, Simulation and Inference

\runningauthor{Chang$^{*}$, Loka$^{*}$, Huang$^{*}$, Remes, Kaski, Acerbi}

\twocolumn[ \aistatstitle{Amortized Probabilistic Conditioning \\ for Optimization, Simulation and Inference}

\aistatsauthor{Paul E. Chang$^{*1}$ \And Nasrulloh Loka$^{*1}$ \And Daolang Huang$^{*2}$ \And Ulpu Remes$^3$ \And Samuel Kaski$^{2,4}$ \And Luigi Acerbi$^1$}

\aistatsaddress{$^1$Department of Computer Science, University of Helsinki, Helsinki, Finland \\ $^2$Department of Computer Science, Aalto University, Espoo, Finland \\ $^3$Department of Mathematics and Statistics, University of Helsinki, Helsinki, Finland \\ $^4$Department of Computer Science, University of Manchester, Manchester, United Kingdom } ]

# Abstract

Amortized meta-learning methods based on pre-training have propelled fields like natural language processing and vision. Transformer-based neural processes and their variants are leading models for probabilistic meta-learning with a tractable objective. Often trained on synthetic data, these models implicitly capture essential latent information in the data-generation process. However, existing methods do not allow users to flexibly *inject* (condition on) and *extract* (predict) this probabilistic latent information at runtime, which is key to many tasks. We introduce the Amortized Conditioning Engine (ACE), a new transformer-based meta-learning model that explicitly represents latent variables of interest. ACE affords conditioning on both observed data and interpretable latent variables, the inclusion of priors at runtime, and outputs predictive distributions for discrete and continuous data and latents. We show ACE's modeling flexibility and performance in diverse tasks such as image completion and classification, Bayesian optimization, and simulation-based inference.\looseness=-1

# Introduction

**Figure:** **Probabilistic conditioning and prediction.** Many tasks reduce to *probabilistic conditioning* on data and key latent variables (left) and then *predicting* data and latents (right). (\textcolor{blue}{a}) Image completion and classification (data: pixels; latents: classes). *Top*: Class prediction. *Bottom*: Conditional generation. (\textcolor{blue}{b}) Bayesian optimization (data: function values; latents: optimum location $x_\text{opt}$ and value $y_\text{opt}$). We predict both the function values and $x_\text{opt}$, $y_\text{opt}$ given function observations and a prior over $x_\text{opt}$, $y_\text{opt}$ (here flat). (\textcolor{blue}{c}) Simulator-based inference (data: observations; latents: model parameter $\theta$). Given data and a prior over $\theta$, we can compute both the posterior over $\theta$ and predictive distribution over unseen data. Our method fully amortizes probabilistic conditioning and prediction. \label{fig:intro}

Amortization, or pre-training, is a crucial technique for improving computational efficiency and generalization across many machine learning tasks, from regression \citep{garnelo2018neural} to optimization \citep{amos2022tutorial} and simulation-based inference \citep{cranmer2020frontier}. By training a deep neural network on a large dataset of related problems and solutions, amortization can achieve both fast inference time, solving problems with a single forward pass, and meta-learning, better adapting to new problems by capturing high-level statistical relations \citep{brown2020language}. Probabilistic meta-learning models based on the transformer architecture \citep{vaswani2017attention} are the state-of-the-art for amortizing complex predictive data distributions \citep{nguyen2022transformer, muller2022transformers}.

This paper capitalizes on the fact that many machine learning problems reduce to *predicting* data and task-relevant latent variables after *conditioning* on other data and latents \citep{ghahramani2015probabilistic}; see \cref{fig:intro}. Moreover, in many scenarios the user has exact or probabilistic information (priors) about task-relevant variables that they would like to leverage, but incorporating such prior knowledge is challenging, requiring dedicated, expensive solutions.

For instance, in Bayesian optimization \citep{garnett2023bayesian}, the goal is to find the location $\xopt$ and value $\yopt$ of the global minimum of a function (\cref{fig:intro}b). Following information-theoretical principles, we should query points that would reduce uncertainty (entropy) about the optimum's location or value. However, predictive distributions over $\xopt$ and $\yopt$ are intractable, leading to complex approximation techniques \citep{hennig2012entropy, hernandez2014predictive, wang2017max}. Moreover, incorporating prior knowledge in the optimization process, such as a theoretically known $\yopt$ value or expert knowledge about likely $\xopt$ locations, requires ad-hoc solutions \citep{nguyen2020knowing, souza2021bayesian}.

Similar challenges extend to many machine learning tasks, including regression and classification (\cref{fig:intro}a), and simulation-based inference (\cref{fig:intro}c), all involving predicting, sampling, and probabilistic conditioning on either exact values or distributions (priors) at runtime.

In this work, we address the desiderata above by introducing the *Amortized Conditioning Engine* (ACE), a general amortization framework which extends transformer-based meta-learning architectures \citep{nguyen2022transformer, muller2022transformers} with explicit and flexible probabilistic modeling of task-relevant latent variables. Our main goal with ACE is to develop a method capable of addressing a variety of tasks that would otherwise require bespoke solutions and approximations. Through the lens of amortized probabilistic conditioning and prediction, we provide a unifying methodological bridge across multiple fields.

#### Contributions.

Our contributions include:

- We propose ACE, a transformer-based architecture that simultaneously affords, at inference time, conditioning and autoregressive probabilistic prediction for arbitrary combinations of data and latent variables, both continuous and discrete.
- We introduce a new technique for allowing the user to provide probabilistic information (priors) over each latent variable at inference time.
- We substantiate the generality of our framework through a series of tasks from different fields, including image completion and classification, Bayesian optimization, and simulation-based inference, on both synthetic and real data.

#### Related work.

Our work builds on advances in neural processes, meta-learning, and simulation-based inference (SBI). ACE extends neural processes \citep{garnelo2018neural, garnelo2018conditional, kim2019attentive, nguyen2022transformer} and prior-fitted networks (PFNs; \citealp{muller2022transformers}) by explicitly modeling and conditioning on latent variables, allowing flexible priors at runtime, and yielding predictive distributions for both data and interpretable latents. Related SBI works \citep{cranmer2020frontier, radev2023jana, gloeckler2024all} use deep networks to recover posterior distributions or emulate simulators, with some models allowing runtime conditioning. However, ACE uniquely accommodates both flexible priors and discrete outputs, and we show its applicability to multiple tasks beyond SBI. An extensive discussion of related work is in \cref{app:ex_related_work}.

# Preliminaries
\label{sec:preliminaries}

In this section, we review previous work on transformer-based probabilistic meta-learning models within the framework of prediction maps \citep{foong2020meta, markou2022practical} and Conditional Neural Processes (CNPs; \citealp{garnelo2018conditional}). We denote with $\x \in \XX \subseteq \mathbb{R}^{D}$ input vectors (covariates) and $y \in \YY \subseteq \mathbb{R}$ scalar output vectors (values).

#### Prediction maps.

A *prediction map* $\pi$ is a function that maps (1) a *context set* of input/output pairs $\data_N = \{(\x_1, y_1), \ldots, (\x_N, y_N)\}$ and (2) a collection of *target inputs* $\xt_{1:M} \equiv (\xt_1, \ldots, \xt_M)$ to a distribution over the corresponding *target outputs* $\yt_{1:M} \equiv (\yt_{1}, \ldots, \yt_M)$:

$$
\label{eq:predictionmap}
\pi\left(\yt_{1:M} \vert \xt_{1:M}; \data_N \right) = p\left(\yt_{1:M}| \r(\xt_{1:M}, \data_N) \right),
$$

where $\r$ is a *representation vector* of the context and target sets that parameterizes the predictive distribution. Such map should be invariant with respect to permutations of the context set and, separately, of the targets \citep{foong2020meta}. The Bayesian posterior is a prediction map, with the Gaussian Process (GP; \citealp{rasmussen2006gaussian}) posterior a special case.

#### Diagonal prediction maps.

We call a prediction map *diagonal* if it represents and predicts each target independently:

$$
\label{eq:diagonal_map}
\pi(\yt_{1:M} \vert \xt_{1:M}; \data_N ) = \prod_{m=1}^M p\left(\yt_m| \r(\xt_m, \r_{\data}(\data_N))\right)
$$

where $\r_{\data}$ denotes a representation of the context alone. Diagonal outputs ensure that a prediction map is permutation and marginalization consistent with respect to the targets for a fixed context, necessary conditions for a valid stochastic process \citep{markou2022practical}.

CNPs are diagonal prediction maps parameterized by deep neural networks \citep{garnelo2018conditional}. CNPs encode the context set to a *fixed-dimension* vector $\r_{\data}$ via a DeepSet \citep{zaheer2017deep} to ensure permutation invariance of the context, and each target predictive distribution is a Gaussian whose mean and variance are decoded by a multi-layer perceptron (MLP). Given the likelihood in \cref{eq:diagonal_map}, CNPs are easily trainable via maximum-likelihood optimization of parameters of encoder and decoder networks by sampling batches of context and target sets.

#### Transformers.

Transformers \citep{vaswani2017attention} are deep neural networks based on the attention mechanism, which computes a weighted combination of hidden states of dimension $D_\text{emb}$ through three learnable linear projections: query ($Q$), key ($K$), and value ($V$). The attention operation, $\text{softmax}(QK^T / \sqrt{D_\text{emb}})V$, captures complex relationships between inputs. *Self-attention* computes $Q$, $K$, and $V$ from the same input set, whereas *cross-attention* uses two sets, one for computing the queries and another for keys and values. A standard transformer architecture consists of multiple stacked self-attention and MLP layers with residual connections and layer normalization \citep{vaswani2017attention}. Without specifically injecting positional information, transformers process inputs in a permutation-equivariant manner.

#### Transformer diagonal prediction maps.

We define here a general *transformer prediction map* model family, focusing on its *diagonal* variant (TPM-D), which includes the TNP-D model from \citet{nguyen2022transformer} and *prior-fitted networks* (PFNs; \citealp{muller2022transformers}). TPM-Ds are not strictly CNPs because the context set is encoded by a *variable-size* representation, but they otherwise share many similarities. In a TPM-D, context data $(\x_n, y_n)_{n=1}^N$ and target inputs $(\xt_m)_{m=1}^M$ are first individually mapped to vector embeddings of size $D_\text{emb}$ via an embedder $f_\text{emb}$, often a linear map or an MLP. The embedded context points are processed together via a series of $B-1$ transformer layers implementing *self-attention* within the context set. We denote by $\E^{(b)} = (\e_1^{(b)}, \ldots, \e_N^{(b)})$ the matrix of output embeddings of the $b$-th transformer layer, with $b=0$ the embedding layer. The encoded context representation is the stacked output of all layers, i.e. $\r_{\data} = (\E^{(0)}, \ldots, \E^{(B-1)})$, whose size is *linear* in the context size $N$. The decoder is represented by a series of $B$ transformer layers that apply *cross-attention* from the embedded target points to the context set layer-wise, with the $b$-th target transformer layer attending the output $\E^{(b-1)}$ of the previous context transformer layer. The decoder transformer layers operate *in parallel* on each target point. The $M$ outputs of the $B$-th decoder block are fed in parallel to an output head yielding the predictive distribution, \cref{eq:diagonal_map}. This shows that indeed TPM-Ds are diagonal prediction maps. The predictive distribution is a single Gaussian in TNP-D \citep{nguyen2022transformer} and a `Riemannian distribution' (a mixture of uniform distributions with fixed bin edges and half-Gaussian tails on the sides) in PFNs \citep{muller2022transformers}. While in TPM-Ds encoding is mathematically decoupled from decoding, in practice encoding and decoding are commonly implemented in parallel within a single transformer layer via masking \citep{nguyen2022transformer, muller2022transformers}.

**Figure:** **Prior amortization.** Two example posterior distributions for the mean $\mu$ and standard deviation $\sigma$ of a 1D Gaussian. (\subref{fig:prior_distribution}) Prior distribution over $\vtheta = (\mu, \sigma)$ set at runtime. (\subref{fig:likelihood}) Likelihood for the observed data. (\subref{fig:true_posterior}) Ground-truth Bayesian posterior. (\subref{fig:ace_predicted_posterior}) ACE's predicted posterior approximates well the true posterior. \label{fig:prior_distribution} \label{fig:likelihood} \label{fig:true_posterior} \label{fig:ace_predicted_posterior} \label{fig:gaussian_examples_main}

# Amortized Conditioning Engine
\label{sec:ace}

We describe now our proposed Amortized Conditioning Engine (ACE) architecture, which affords arbitrary probabilistic conditioning and predictions. We assume the problem has $ L$ task-relevant latent variables of interest $ \vtheta = (\theta_1, \ldots, \theta_L) $. ACE amortizes arbitrary conditioning over latents (in context) and data to predict arbitrary combinations of latents (in target) and data. ACE also amortizes conditioning on probabilistic information about unobserved latents, represented by an approximate prior distribution $p(\theta_l)$ for $l \in \{1, \ldots, L\}$; see \cref{fig:gaussian_examples_main} for an example (details in \cref{app:priors}).

## ACE encodes latents and priors
\label{sec:latentspriors}

We demonstrate here that ACE is a new member of the TPM-D family, by extending the prediction map formalism to explicitly accommodate latent variables. In ACE, we aim to seamlessly manipulate variables that could be either data points $(\x, y)$ or latent variables $\theta_l$, for a finite set of continuous or discrete-valued latents $1 \le l\le L$.

We redefine inputs as $\vxi \in \XX \cup \{\ell_1, \ldots, \ell_L \}$ where $\XX \subseteq \mathbb{R}^D$ denotes the data input space (covariates) and $\ell_l$ is a marker for the $l$-th latent. We also redefine the values as $z \in \mathcal{Z} \subseteq \mathbb{R}$ where $\mathcal{Z}$ can be continuous or a finite set of integers for discrete-valued output. Thus, $(\vxi, z)$ could denote either a (input, output) data pair or a (index, value) latent pair with either continuous or discrete values. With these new flexible definitions, ACE is indeed a transformer diagonal prediction map (TPM-D). In particular, we can predict any combination of target variables (data or latents) conditioning on any other combination of context data and latents, $\dataplus_N = \left\{(\vxi_1, z_1), \ldots, (\vxi_N, z_N)\right\}$:

$$
\label{eq:ace_map}
\pi(\zt_{1:M} \vert \vxit_{1:M}; \dataplus_N ) = \prod_{m=1}^M p\left(\zt_m| \r(\vxit_m, \r_{\data}(\dataplus_N))\right).
$$

#### Prior encoding.
\label{sec:prior}

ACE also allows the user to express probabilistic information over latent variables as prior probability distributions at runtime. Our method affords prior specification separately for each latent, corresponding to a factorized prior $p(\vtheta) = \prod_{l=1}^L p(\theta_l)$. To flexibly approximate a broad class of distributions, we convert each one-dimensional probability density function $p(\theta_l)$ to a normalized histogram of probabilities $\mathbf{p}_l \in [0, 1]^{N_\text{grid}}$ over a predefined grid $\mathcal{G}$ of $N_\text{bins}$ bins uniformly covering the range of values. We can represent this probabilistic conditioning information within the prediction map formalism by extending the context output representation to $z \in \left\{ \mathcal{Z} \cup [0,1]^{N_\text{bins}} \right\}$, meaning that a context point either takes a specific value or a prior defined on $\mathcal{G}$ (see \cref{app:priors}).

## ACE architecture

We detail below how ACE extends the general TPM-D architecture presented in \cref{sec:preliminaries} to implement latent and prior encoding, enabling flexible probabilistic conditioning and prediction. We introduce a novel embedding layer for latents and priors, adopt an efficient transformer layer implementation, and provide an output represented by a flexible Gaussian mixture or categorical distribution.

#### Embedding layer.

In ACE, the embedders map context and target data points and latents to the same underlying embedding space of dimensionality $D_\text{emb}$. The ACE embedders can handle discrete or continuous inputs without the need of tokenization. For the context set, we embed an observed data point $(\x_n, y_n)$ as $f_\x(\x_n) + f_\text{val}(y_n) + \e_\text{data}$, while we embed an observed latent variable $\theta_l$ as $f_\text{val}(\theta_l) + \e_l$, where $\e_\text{data}$ and $\e_l$ for $1\le l \le L$ are learnt vector embeddings, and $f_\x$ and $f_\text{val}$ are learnt nonlinear embedders (MLPs) for the covariates and values, respectively. For discrete-valued variables (data or latents), $f_\text{val}$ is replaced by a vector embedding matrix $\E_\text{val}$ with a separate row for each discrete value. Latent variables with a prior $\mathbf{p}_l$ are mapped to the context set as $f_\text{prob}(\mathbf{p}_l) + \e_l$, where $f_\text{prob}$ is a learnt MLP. In the target set, the *value* of a variable is unknown and needs to be predicted, so we replace the value embedders above with a learnt `unknown' embedding $\e_?$, i.e. $f_\x(\x_n) + \e_? + \e_\text{data}$ for data points and $\e_? + \e_l$ for latents.

#### Transformer layers.

After the embedding layer, the network consists of $B$ stacked feed-forward transformer layers \citep{vaswani2017attention}. Each transformer layer consists of a multi-head attention block followed by a MLP. Both the attention and MLP blocks are followed by a normalization layer, and include a skip connection from the previous step. The multi-head attention block combines encoder and decoder in the same step, with self-attention on the context points (encoding) and cross-attention from the target points to the context set (decoding). Our implementation differs from others which compute a single masked context + target attention matrix with $O((N + M)^2)$ cost \citep{nguyen2022transformer,muller2022transformers}. Instead, by separating the context self-attention and target cross-attention matrices we incur a $O(N^2 + NM)$ cost for attention \citep{feng2023latent}.

#### Output heads.

A mixture-of-Gaussians prediction output head is applied in parallel to all target points after the output of the last transformer layer. For a continuous-valued variable, the output head consists of $K$ MLPs that separately output the parameters of $K$ 1D Gaussians, that is `raw' (weight, mean, standard deviation) for each mixture component \citep{uria2016neural}. A learnt global raw bias term ($3 \times K$ parameters) is summed to each raw output, helping the network learn deviations from the global (marginal) distribution of values. Then weights, means and standard deviations for each mixture component are obtained after applying appropriate transformations (softmax, identity, and softplus, respectively). For discrete-valued variables, the output head is a MLP that outputs a softmax categorical distribution over the discrete values.

## Training and prediction
\label{sec:training}

ACE is trained via maximum-likelihood on synthetic data consisting of batches of context and target sets, using the Adam optimizer (details in \cref{app:sampling}).

#### Training.

We generate each problem instance hierarchically by first sampling the latent variables $\vtheta$, and then data points $(\X, \y)$ according to the generative model of the task. For example, $\vtheta$ could be length scale and output scale of a 1D Gaussian process with a given kernel, and $(\X, \y)$ input locations and function values. Data and latents are randomly split between context and target. For training with probabilistic information $\mathbf{p}$, we first sample the priors for each latent variable from a hierarchical model $\mathcal{P}$ which includes mixtures of Gaussians and Uniform distributions (see \cref{app:priors}) and then sample the value of the latent from the chosen prior. During training, we minimize the expected negative log-likelihood of the target set conditioned on the context, $\mathcal{L}\left(**w**\right)$:

$$
\label{eq:loss}
\begin{split}

\mathbb{E}_\mathcal{\mathbf{p} \sim P} &\left[ \mathbb{E}_{\dataplus_N, \vxi_{1:M},\z_{1:M} \sim \mathbf{p}}\left[-\sum_{m=1}^M \log q\left(z^\star_m|\r_**w**(\vxi^\star_m, \dataplus_N)\right) \right] \right], \end{split}
$$

\noindent where $q$ is our model's prediction (a mixture of Gaussians or categorical), and $**w**$ are the model parameters. Minimizing \cref{eq:loss} is equivalent to minimizing the Kullback-Leibler (KL) divergence between the data sampled from the generative process and the model. Since the generative process is consistent with the provided contextual prior information, training will aim to converge (KL-wise) as close as possible, for the model capacity, to the correct Bayesian posteriors and predictive distributions for the specified generative model and priors \citep{muller2022transformers, elsemueller2024sensitivity}.

#### Prediction.

ACE is trained via *independent* predictions of target data and latents, \cref{eq:loss}. Given the closed-form likelihood (mixture of Gaussians or categorical), we can easily evaluate or sample from the predictive distribution at any desired target point (data or latent) in parallel, conditioned on the context. Moreover, we can predict *non-independent* joint distributions autoregressively \citep{nguyen2022transformer,bruinsma2023autoregressive}; see \cref{app:autoregressive} for details.

**Figure:** \fontsize{7}{8}\selectfont{Image} \fontsize{7}{8}\selectfont{$\data_N$} \fontsize{7}{8}\selectfont{TNP-D} \fontsize{7}{8}\selectfont{ACE} \fontsize{7}{8}\selectfont{ACE-$\vtheta$} Negative log-probability density vs. Context **Image completion.** Image (\subref{fig:image_full}) serves as the reference for the problem, where $10\%$ of the pixels are observed (\subref{fig:context_celeb}). Figures (\subref{fig:tnpd_celeb}) through (\subref{fig:ACE_theta_celeb}) display model predictions conditioned on the observed pixels (\subref{fig:context_celeb}). In addition, (\subref{fig:ACE_theta_celeb}) incorporates latent variable $\vtheta$ information for the ACE model. Figure (\subref{fig:celeba-nlpd_main}) illustrates the different models' performance across varying levels of context. \label{fig:image_full} \label{fig:context_celeb} \label{fig:tnpd_celeb} \label{fig:ACE_image_celeb} \label{fig:ACE_theta_celeb} \label{fig:celeba-nlpd_main} \label{fig:whole_image_celeb_main}

# Experiments
\label{sec:experiments}

The following section showcases ACE's capabilities as a general framework applicable to diverse machine learning and modeling tasks.\footnote{The code implementation of ACE is available at \href{https://github.com/acerbilab/amortized-conditioning-engine/}{github.com/acerbilab/amortized-conditioning-engine/}.}

Firstly, \cref{exp:image} demonstrates how ACE complements transformer-based meta-learning in image completion and classification. In \cref{exp:bo}, we show how ACE can be applied to Bayesian optimization (BO) by treating the location and value of the global optimum as latent variables. We then move to simulation-based inference (SBI) in \cref{exp:sbi}, where ACE unifies the SBI problem into a single framework, treating parameters as latent variables and affording both forward and inverse modelling. Notably, SBI and BO users may have information about the simulator or target function. ACE affords incorporation of informative priors about latent variables at runtime, as detailed in \cref{sec:prior}, a variant we call ACEP in these experiments. Finally,

in \cref{app:gp} we provide extra experimental results on Gaussian Processes (GPs) where ACE can accurately predict the kernel, \ie model selection (\cref{fig:GP}), while at the same time learn the hyperparameters, in addition to the common data prediction task.

## Image completion and classification
\label{exp:image}

We treat image completion as a regression task \citep{garnelo2018neural}, where the goal is given some limited $\data_N$ of image coordinates and corresponding pixel values to predict the complete image. For the `MNIST` \citep{deng2012mnist} task, we downsize the images to $16 \times 16$ and likewise for `CelebA` to $32 \times 32$ \citep{liu2015faceattributes}. We turn the class label into a single discrete latent for `MNIST` while for `CelebA`, we feed the full $40$ corresponding binary features (\eg, `BrownHair, Man, Smiling`). The latents are sampled using the procedure outlined in \cref{app:sampling} and more experimental details of the image completion task can be found in \cref{app:image}. Notably, ACE affords conditional predictions of pixels based on latent variables ($\vtheta$) -- such as class labels in `MNIST` and appearance features in `CelebA` -- as well as the prediction of these latent variables themselves.

**Results.** In \cref{fig:whole_image_celeb_main} we present a snapshot of the results for `CelebA` and in \cref{app:image} we present the same for `MNIST`. The more complex output distribution allows ACE to outperform other Transformer NPs convincingly, and the integration of latent information shows a clear improvement. In \cref{app:image}, we present our full results, including predicting $\vtheta$.

## Bayesian optimization (BO)
\label{exp:bo}

**Figure:** (\subref{fig:BO_walkthrough_1}) ACE predicts function values \legendpygivenx \ as well as latents \legendpxopt \ and \legendpyopt. (\subref{fig:BO_walkthrough_2}) Further conditioning on \legendyoptcond \ (here the true minimum value) leads to updated predictions. \label{fig:BO_walkthrough_1} \label{fig:BO_walkthrough_2} \label{fig:BO_walkthrough}

BO aims to find the global minimum $ \yopt = f(\xopt) $ of a black-box function. This is typically achieved iteratively by building a *surrogate model* that approximates the target and optimizing an *acquisition function* $\alpha(\mathbf{x})$ to determine the next query point. ACE provides additional modeling options for the BO loop by affording direct conditioning on, and prediction of, key latents $\xopt$ and $\yopt$, yielding closed-form predictive distributions and samples for $p(\xopt | \data_N)$, $p(\yopt | \data_N)$, and $ p(\xopt | \data_N, \yopt)$; see \cref{fig:BO_walkthrough}.

For the BO task, we trained ACE on synthetic functions generated from GP samples with RBF and Matérn-($\nicefrac{1}{2}$, $\nicefrac{3}{2}$, $\nicefrac{5}{2}$) kernels and a random global optimum $(\xopt,\yopt)$ within the function domain; see \cref{app:bo} for details. We leverage ACE's explicit modeling of latents in multiple ways.

Firstly, ACE affords a straightforward implementation of a variant of Thompson Sampling (TS) \citep{dutordoir2023neural, liu2024large}. First, we sample a candidate optimum value, $\yopt^\star$, conditioning on it being below a threshold $\tau$, from the truncated predictive distribution $\yopt^\star \sim p(\yopt | \data_N, \yopt<\tau)$; see \cref{fig:BO_walkthrough_1}. Given $\yopt^\star$, we then sample the query point $\x^\star \sim p(\xopt | \data_N, \yopt^\star)$; \cref{fig:BO_walkthrough_2}.\footnote{Why not sampling directly from $p(\xopt | \data_N)$? The issue is that $p(\xopt | \data_N)$ may reasonably include substantial probability mass at the current optimum, which would curb exploration. The constraint $\yopt < \tau$, with $\tau$ (just) below the current optimum value, ensures continual exploration.} This process is repeated iteratively within the BO loop (see \cref{fig:bo_evolution}). For higher input dimensions ($D > 1$), we sample $\x^\star$ autoregressively \citep{bruinsma2023autoregressive}; see \cref{sec:ace-bo-alg}. Crucially, ACE's capability of directly modeling $\xopt$ and $\yopt$ bypasses the need for surrogate optimization or grid searches typical of standard TS implementations (\eg, GP-TS or TNP-D based TS).

**Figure:** **Bayesian optimization.** Regret comparison across BO benchmarks. ACE is competitive across the benchmarks with the gold standard GP method, with MES outperforming TS in higher dimension. \label{fig:bo_comparisons}

Second, ACE also easily supports advanced acquisition functions used in BO, such as Max-Value Entropy Search (MES; \citealp{wang2017max}). For a candidate point $\xt$, MES evaluates the expected gain in mutual information between $\yopt$ and $\xt$:

$$
\begin{aligned} \label{eq:mes}
\alpha_\text{MES}(\xt) &= \textcolor{red}{H[p(\yt| \xt, \data_N)]}\\ &- \mathbb{E}_{\textcolor{latent2}{p(\yopt| \data_N)}}\left[\textcolor{red}{H[p(\yt| \xt, \data_N, \yopt)]}\right]. \end{aligned}
$$

With all predictive distributions available in closed form, ACE can readily calculate the expectation and entropies in \cref{eq:mes} via \textcolor{latent2}{Monte Carlo sampling} and fast 1D \textcolor{red}{numerical integration}, unlike other methods that require more laborious approximations. For maximizing $\alpha(\x^\star)$, we obtain a good set of candidate points via Thompson Sampling (see \cref{sec:ace-bo-alg} for details).

In our BO benchmarks, we compare ACE with gold-standard Gaussian processes (GPs) and the state-of-the-art TNP-D model (\cref{fig:bo_comparisons}). Additionally, we test a setup where prior information is provided about the location of the optimum \citep{souza2021bayesian, hvarfner2022pi, muller2023pfns4bo}; \cref{fig:bo_prior_main_comparisons}. Unlike other methods that employ heuristics or complex approximations, ACE's architecture affords seamless incorporation of a prior $p(\xopt)$. Here, we consider two types of priors: strong and weak, represented by Gaussians with a standard deviation equal to, respectively, 10\% and 25\% of the optimization box width in each dimension, and mean drawn from a Gaussian centered on the true optimum and same standard deviation (see \cref{sec:bo_prior}).

**Results.** In \cref{fig:bo_comparisons}, we compare the performance of ACE Thompson sampling (ACE-TS) and MES (ACE-MES) with GP-based MES (GP-MES; \citealp{wang2017max}), GP-based Thompson Sampling (GP-TS; \citealp{balandat2020botorch}), and Autoregressive TNP-D based Thompson Sampling (AR-TNPD-TS; \citealp{bruinsma2023autoregressive,nguyen2022transformer}) on several benchmark functions (see \cref{sec:bo-benchmarks} for details). ACE-MES frequently outperforms ACE-TS and often matches the gold-standard GP-MES. In the prior setting, we compare ACE without (ACE-TS) and with prior (ACEP-TS) against $\pi$BO-TS, a state-of-the-art heuristic for prior injection in BO \citep{hvarfner2022pi}, as well as the GP-TS baseline. ACEP-TS shows significant improvement over its no-prior variant ACE-TS while showing competitive performance compared to $\pi$BO-TS in both weak and strong prior case (\cref{fig:bo_prior_main_comparisons}).

**Figure:** **Bayesian optimization with prior over $\xopt$.** Regret comparison on 2D and 3D optimization benchmarks. Left: Weak Gaussian prior (25\%), Right: Strong prior (10\%). ACEP-TS performs competitively compared to $\pi$BO-TS. \label{fig:bo_prior_main_comparisons}

## Simulation-based inference (SBI)
\label{exp:sbi}

**Table:** **Comparison metrics for SBI models** on parameters ($\vtheta$) and data ($y$) prediction; mean and (\textcolor{gray}{standard deviation}) from 5 runs. *Left*: Statistically significantly (see \cref{app:sbi_cfg}) best results are **bolded**. ACE shows performance comparable to the other methods on latents prediction. In the data prediction task, ACE performs similarly to Simformer with much lower sampling cost at runtime (see text). *Right*: ACE can leverage probabilistic information provided at runtime by informative priors (ACEP), yielding improved performance. \label{tab:sbi}

```latex
\begin{table*}[!t] \centering \scalebox{0.88}{ \begin{tabular}{cc|cccc|cc} \toprule & & NPE & NRE & Simformer & ACE & ACEP$_\text{weak prior}$ & ACEP$_\text{strong prior}$ \\ \cmidrule(lr){3-8} \multirow{3}{*}{OUP} & $\text{log-probs}_{\theta}$ ($\uparrow$) & **1.04**\textcolor{gray}{(0.03)} & **1.08**\textcolor{gray}{(0.13)} & **1.02**\textcolor{gray}{(0.03)} & **1.01**\textcolor{gray}{(0.01)} & 1.28\textcolor{gray}{(0.03)} & 1.57\textcolor{gray}{(0.03)} \\ & $\text{RMSE}_{\theta}$ ($\downarrow$) & **0.58**\textcolor{gray}{(0.01)} & 0.60\textcolor{gray}{(0.01)} & **0.59**\textcolor{gray}{(0.04)} & **0.59**\textcolor{gray}{(0.00)} & 0.43\textcolor{gray}{(0.01)} & 0.26\textcolor{gray}{(0.01)} \\ & $\text{MMD}_{y}$ ($\downarrow$) & - & - & **0.43**\textcolor{gray}{(0.02)} & 0.50\textcolor{gray}{(0.00)} & 0.39\textcolor{gray}{(0.00)} & 0.35\textcolor{gray}{(0.00)} \\ \midrule \midrule \multirow{3}{*}{SIR} & $\text{log-probs}_{\theta}$ ($\uparrow$) & 6.80\textcolor{gray}{(0.07)} & 6.51\textcolor{gray}{(0.29)} & **6.87**\textcolor{gray}{(0.06)} & 6.77\textcolor{gray}{(0.02)} & 6.85\textcolor{gray}{(0.05)} & 6.95\textcolor{gray}{(0.06)} \\ & $\text{RMSE}_{\theta}$ ($\downarrow$) & **0.03**\textcolor{gray}{(0.00)} & 0.04\textcolor{gray}{(0.00)} & 0.07\textcolor{gray}{(0.02)} & **0.03**\textcolor{gray}{(0.00)} & 0.03\textcolor{gray}{(0.00)} & 0.02\textcolor{gray}{(0.00)} \\ & $\text{MMD}_{y}$ ($\downarrow$) & - & - & **0.02**\textcolor{gray}{(0.00)} & **0.03**\textcolor{gray}{(0.00)} & 0.02\textcolor{gray}{(0.00)} & 0.00\textcolor{gray}{(0.00)} \\ \midrule \midrule \multirow{3}{*}{Turin} & $\text{log-probs}_{\theta}$ ($\uparrow$) & 2.07\textcolor{gray}{(0.10)} & 2.42\textcolor{gray}{(0.11)} & **3.02**\textcolor{gray}{(0.03)} & **3.01**\textcolor{gray}{(0.01)} & 6.06\textcolor{gray}{(0.12)} & 7.58\textcolor{gray}{(0.73)} \\ & $\text{RMSE}_{\theta}$ ($\downarrow$) & 0.39\textcolor{gray}{(0.00)} & 0.43\textcolor{gray}{(0.00)} & **0.25**\textcolor{gray}{(0.00)} & **0.25**\textcolor{gray}{(0.00)} & 0.10\textcolor{gray}{(0.01)} & 0.08\textcolor{gray}{(0.01)} \\ & $\text{MMD}_{y}$ ($\downarrow$) & - & - & **0.34**\textcolor{gray}{(0.00)} & **0.34**\textcolor{gray}{(0.00)} & 0.33\textcolor{gray}{(0.00)} & 0.32\textcolor{gray}{(0.00)} \\ \bottomrule \end{tabular}}

\end{table*}
```

We now apply ACE to simulation-based inference (SBI; \citealp{cranmer2020frontier}). With ACE, we can predict the posterior distribution of (latent) model parameters, simulate data based on parameters, predict missing data given partial observations, and set priors at runtime. We consider two benchmark time-series models, each with two latents: the Ornstein-Uhlenbeck Process (OUP; \citealt{uhlenbeck1930theory}) and the Susceptible-Infectious-Recovered model (SIR; \citealt{kermack1927contribution}); and a third more complex engineering model from the field of radio propagation (Turin; \citealp{turin1972statistical}), which has four parameters and produces 101-dimensional data representing a radio signal. See \cref{app:sbi} for all model descriptions.

We compare ACE with Neural Posterior Estimation (NPE; \citealt{greenberg2019automatic}), Neural Ratio Estimation (NRE; \citealt{miller2022contrastive}), and Simformer \citep{gloeckler2024all}, from established to state-of-the-art methods in amortized SBI. We evaluate ACE in three different scenarios. For the first one, we use only the observed data as context. For the other two scenarios, we inform ACE with priors over the parameters (ACEP), to assess their impact on posterior prediction. These priors are Gaussians with standard deviation equal to 25\% (weak) or 10\% (strong) of the parameter range, and mean drawn from a Gaussian centered on the true parameter value and the same standard deviation.

We evaluate the performance of posterior estimation using the log probabilities of the ground-truth parameters and the root mean squared error (RMSE) between the true parameters and posterior samples. Since both ACE and Simformer can predict missing data from partial observations -- an ability that previous SBI methods lack -- we also test them on a data prediction task. For each observed dataset, we randomly designate half of the data points as missing and use the remaining half as context for predictions. We then measure performance via the maximum mean discrepancy (MMD) between the true data and the predicted distributions.

**Results.** Results are reported in \cref{tab:sbi}; see \cref{app:sbi} for details. For ACE, we see that joint training to predict data and latents does not compromise its posterior estimation performance compared to NPE and NRE, even achieving better performance on the Turin model. ACE and Simformer obtain similar results. However, as Simformer uses diffusion, data sampling is substantially slower. For example, we measured the time required to generate 1,000 posterior samples for 100 sets of observations on the OUP model using a CPU (\textcolor{magenta}{GPU}) across 5 runs: the average time for Simformer is $\sim$ 130 minutes (\textcolor{magenta}{14 minutes}), whereas ACE takes 0.05 seconds (\textcolor{magenta}{0.02 s}). When we provide ACE with informative priors (ACEP; \cref{tab:sbi} right), performance improves in proportion to the provided information. Importantly, simulation-based calibration checks \citep{talts2018validating} show that both ACE and ACEP output good approximate posteriors (\cref{app:sbc}).

Finally, we applied ACE to a real-world outbreak dataset \citep{avilov20231978} using an extended, four-parameter version of the SIR model. We show that ACE can handle real data, providing reasonable results under a likely model mismatch (see \cref{app:sbi_real}).

# Discussion
\label{sec:discussion}

\label{sec:limitations}

In this paper, we introduced the Amortized Conditioning Engine (ACE), a unified transformer-based architecture that affords arbitrary probabilistic conditioning and prediction over data and task-relevant variables for a wide variety of tasks. In each tested domain ACE performed on par with state-of-the-art, bespoke solutions, and with greater flexibility. As a key feature, ACE allows users to specify probabilistic information at runtime (priors) without the need for retraining as required instead by the majority of previous amortized inference methods \citep{cranmer2020frontier}.

#### Limitations and future work.

As all amortized and machine learning methods, ACE's predictions become unreliable if applied to data unseen or sparsely seen during training due to mismatch between the generative model used in training and real data (*misspecification* or covariate shift). This is an active area of research in terms of developing both more robust training objectives \citep{huang2024learning} and diagnostics \citep{schmitt2023detecting}.

Due to self-attention, our method incurs quadratic cost in the context size $N$ (but linear in the target size $M$). Future work could investigate sub-quadratic attention variants for set representations \citep{feng2023latent}. Our model could also be improved by incorporating known equivariances \citep{huang2023practical} and varying covariate dimensionality at runtime \citep{liu2020task, dutordoir2023neural,muller2023pfns4bo}.

By learning to predict marginal conditionals, ACE can eventually learn the full joint distribution which we can query autoregressively \citep{bruinsma2023autoregressive}. However, scalability of learning the full joint pdf of a large number of data and latents remains challenging. Similar limitations apply to our prior-setting technique, currently applied to a small number of latents under a factorized, smooth prior. Further work should extend these probabilistic modeling capabilities.

#### Conclusions.

ACE shows strong promise as a new unified and versatile method for amortized probabilistic conditioning and prediction, able to perform various probabilistic machine learning tasks.

# Acknowledgments

PC, DH, UR, SK, and LA were supported by the Research Council of Finland (Flagship programme: Finnish Center for Artificial Intelligence FCAI). NL was funded by Business Finland (project 3576/31/2023). LA was also supported by Research Council of Finland grants 358980 and 356498. SK was also supported by the UKRI Turing AI World-Leading Researcher Fellowship, [EP/W002973/1]. The authors wish to thank the Finnish Computing Competence Infrastructure (FCCI), Aalto Science-IT project, and CSC–IT Center for Science, Finland, for the computational and data storage resources provided.

\begin{thebibliography}{64} \providecommand{\natexlab}[1]{#1} \providecommand{\url}[1]{\texttt{#1}} \expandafter\ifx\csname urlstyle\endcsname\relax \providecommand{\doi}[1]{doi: #1}\else \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Garnelo et~al.(2018{\natexlab{a}})Garnelo, Schwarz, Rosenbaum, Viola, Rezende, Eslami, and Teh]{garnelo2018neural} Marta Garnelo, Jonathan Schwarz, Dan Rosenbaum, Fabio Viola, Danilo~J Rezende, SM~Ali Eslami, and Yee~Whye Teh. \newblock Neural processes. \newblock In *ICML Workshop on Theoretical Foundations and Applications of Deep Generative Models*, 2018{\natexlab{a}}.

\bibitem[Amos(2022)]{amos2022tutorial} Brandon Amos. \newblock Tutorial on amortized optimization for learning to optimize over continuous domains. \newblock *arXiv e-prints*, pages arXiv--2202, 2022.

\bibitem[Cranmer et~al.(2020)Cranmer, Brehmer, and Louppe]{cranmer2020frontier} Kyle Cranmer, Johann Brehmer, and Gilles Louppe. \newblock The frontier of simulation-based inference. \newblock *Proceedings of the National Academy of Sciences*, 117\penalty0 (48):\penalty0 30055--30062, 2020.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language} Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al. \newblock Language models are few-shot learners. \newblock *Advances in Neural Information Processing Systems*, 33:\penalty0 1877--1901, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention} Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin. \newblock Attention is all you need. \newblock *Advances in Neural Information Processing Systems*, 30, 2017.

\bibitem[Nguyen and Grover(2022)]{nguyen2022transformer} Tung Nguyen and Aditya Grover. \newblock {T}ransformer {N}eural {P}rocesses: {U}ncertainty-aware meta learning via sequence modeling. \newblock In *Proceedings of the International Conference on Machine Learning (ICML)*, pages 123--134. PMLR, 2022.

\bibitem[M{\"u}ller et~al.(2022)M{\"u}ller, Hollmann, Arango, Grabocka, and Hutter]{muller2022transformers} Samuel M{\"u}ller, Noah Hollmann, Sebastian~Pineda Arango, Josif Grabocka, and Frank Hutter. \newblock Transformers can do {B}ayesian inference. \newblock In *International Conference on Learning Representations*, 2022.

\bibitem[Ghahramani(2015)]{ghahramani2015probabilistic} Zoubin Ghahramani. \newblock Probabilistic machine learning and artificial intelligence. \newblock *Nature*, 521\penalty0 (7553):\penalty0 452--459, 2015.

\bibitem[Garnett(2023)]{garnett2023bayesian} Roman Garnett. \newblock *Bayesian optimization*. \newblock Cambridge University Press, 2023.

\bibitem[Hennig and Schuler(2012)]{hennig2012entropy} Philipp Hennig and Christian~J Schuler. \newblock Entropy search for information-efficient global optimization. \newblock *Journal of Machine Learning Research*, 13\penalty0 (6), 2012.

\bibitem[Hern{\'a}ndez-Lobato et~al.(2014)Hern{\'a}ndez-Lobato, Hoffman, and Ghahramani]{hernandez2014predictive} Jos{\'e}~Miguel Hern{\'a}ndez-Lobato, Matthew~W Hoffman, and Zoubin Ghahramani. \newblock Predictive entropy search for efficient global optimization of black-box functions. \newblock *Advances in Neural Information Processing Systems*, 27, 2014.

\bibitem[Wang and Jegelka(2017)]{wang2017max} Zi~Wang and Stefanie Jegelka. \newblock Max-value entropy search for efficient {B}ayesian optimization. \newblock In *International Conference on Machine Learning*, pages 3627--3635. PMLR, 2017.

\bibitem[Nguyen and Osborne(2020)]{nguyen2020knowing} Vu~Nguyen and Michael~A Osborne. \newblock Knowing the what but not the where in {B}ayesian optimization. \newblock In *International Conference on Machine Learning*, pages 7317--7326. PMLR, 2020.

\bibitem[Souza et~al.(2021)Souza, Nardi, Oliveira, Olukotun, Lindauer, and Hutter]{souza2021bayesian} Artur Souza, Luigi Nardi, Leonardo~B Oliveira, Kunle Olukotun, Marius Lindauer, and Frank Hutter. \newblock {B}ayesian optimization with a prior for the optimum. \newblock In *Machine Learning and Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13--17, 2021, Proceedings, Part III 21*, pages 265--296. Springer, 2021.

\bibitem[Garnelo et~al.(2018{\natexlab{b}})Garnelo, Rosenbaum, Maddison, Ramalho, Saxton, Shanahan, Teh, Rezende, and Eslami]{garnelo2018conditional} Marta Garnelo, Dan Rosenbaum, Chris~J Maddison, Tiago Ramalho, David Saxton, Murray Shanahan, Yee~Whye Teh, Danilo~J Rezende, and SM~Ali Eslami. \newblock Conditional neural processes. \newblock In *International Conference on Machine Learning*, pages 1704--1713, 2018{\natexlab{b}}.

\bibitem[Kim et~al.(2019)Kim, Mnih, Schwarz, Garnelo, Eslami, Rosenbaum, Vinyals, and Teh]{kim2019attentive} Hyunjik Kim, Andriy Mnih, Jonathan Schwarz, Marta Garnelo, Ali Eslami, Dan Rosenbaum, Oriol Vinyals, and Yee~Whye Teh. \newblock Attentive neural processes. \newblock In *International Conference on Learning Representations*, 2019.

\bibitem[Radev et~al.(2023)Radev, Schmitt, Pratz, Picchini, K{\"o}the, and B{\"u}rkner]{radev2023jana} Stefan~T Radev, Marvin Schmitt, Valentin Pratz, Umberto Picchini, Ullrich K{\"o}the, and Paul-Christian B{\"u}rkner. \newblock {JANA}: {J}ointly amortized neural approximation of complex {B}ayesian models. \newblock In *Uncertainty in Artificial Intelligence*, pages 1695--1706. PMLR, 2023.

\bibitem[Gloeckler et~al.(2024)Gloeckler, Deistler, Weilbach, Wood, and Macke]{gloeckler2024all} Manuel Gloeckler, Michael Deistler, Christian Weilbach, Frank Wood, and Jakob~H Macke. \newblock All-in-one simulation-based inference. \newblock In *International Conference on Machine Learning*. PMLR, 2024.

\bibitem[Foong et~al.(2020)Foong, Bruinsma, Gordon, Dubois, Requeima, and Turner]{foong2020meta} Andrew~YK Foong, Wessel~P Bruinsma, Jonathan Gordon, Yann Dubois, James Requeima, and Richard~E Turner. \newblock Meta-learning stationary stochastic process prediction with convolutional neural processes. \newblock In *Advances in Neural Information Processing Systems*, volume~33, pages 8284--8295, 2020.

\bibitem[Markou et~al.(2022)Markou, Requeima, Bruinsma, Vaughan, and Turner]{markou2022practical} Stratis Markou, James Requeima, Wessel~P Bruinsma, Anna Vaughan, and Richard~E Turner. \newblock Practical conditional neural processes via tractable dependent predictions. \newblock In *International Conference on Learning Representations*, 2022.

\bibitem[Rasmussen and Williams(2006)]{rasmussen2006gaussian} Carl~Edward Rasmussen and Christopher~KI Williams. \newblock *Gaussian Processes for Machine Learning*. \newblock MIT Press, 2006.

\bibitem[Zaheer et~al.(2017)Zaheer, Kottur, Ravanbakhsh, P{\'o}czos, Salakhutdinov, and Smola]{zaheer2017deep} Manzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnab{\'a}s P{\'o}czos, Ruslan Salakhutdinov, and Alexander~J Smola. \newblock Deep sets. \newblock In *Advances in Neural Information Processing Systems*, volume~30, pages 3391--3401, 2017.

\bibitem[Feng et~al.(2023)Feng, Hajimirsadeghi, Bengio, and Ahmed]{feng2023latent} Leo Feng, Hossein Hajimirsadeghi, Yoshua Bengio, and Mohamed~Osama Ahmed. \newblock Latent bottlenecked attentive neural processes. \newblock In *The Eleventh International Conference on Learning Representations, {ICLR* 2023}. PMLR (Proceedings of Machine Learning Research), 2023.

\bibitem[Uria et~al.(2016)Uria, C{\^o}t{\'e}, Gregor, Murray, and Larochelle]{uria2016neural} Benigno Uria, Marc-Alexandre C{\^o}t{\'e}, Karol Gregor, Iain Murray, and Hugo Larochelle. \newblock Neural autoregressive distribution estimation. \newblock *Journal of Machine Learning Research*, 17\penalty0 (205):\penalty0 1--37, 2016.

\bibitem[Elsem{\"u}ller et~al.(2024)Elsem{\"u}ller, Olischl{\"a}ger, Schmitt, B{\"u}rkner, Koethe, and Radev]{elsemueller2024sensitivity} Lasse Elsem{\"u}ller, Hans Olischl{\"a}ger, Marvin Schmitt, Paul-Christian B{\"u}rkner, Ullrich Koethe, and Stefan~T. Radev. \newblock Sensitivity-aware amortized bayesian inference. \newblock *Transactions on Machine Learning Research*, 2024.

\bibitem[Bruinsma et~al.(2023)Bruinsma, Markou, Requeima, Foong, Andersson, Vaughan, Buonomo, Hosking, and Turner]{bruinsma2023autoregressive} Wessel~P Bruinsma, Stratis Markou, James Requeima, Andrew~YK Foong, Tom~R Andersson, Anna Vaughan, Anthony Buonomo, J~Scott Hosking, and Richard~E Turner. \newblock Autoregressive conditional neural processes. \newblock In *International Conference on Learning Representations*, 2023.

\bibitem[Deng(2012)]{deng2012mnist} Li~Deng. \newblock The mnist database of handwritten digit images for machine learning research. \newblock *IEEE Signal Processing Magazine*, 29\penalty0 (6):\penalty0 141--142, 2012.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{liu2015faceattributes} Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. \newblock Deep learning face attributes in the wild. \newblock In *Proceedings of International Conference on Computer Vision (ICCV)*, December 2015.

\bibitem[Dutordoir et~al.(2023)Dutordoir, Saul, Ghahramani, and Simpson]{dutordoir2023neural} Vincent Dutordoir, Alan Saul, Zoubin Ghahramani, and Fergus Simpson. \newblock Neural diffusion processes. \newblock In *International Conference on Machine Learning*, pages 8990--9012. PMLR, 2023.

\bibitem[Liu et~al.(2024)Liu, Astorga, Seedat, and van~der Schaar]{liu2024large} Tennison Liu, Nicol{\'a}s Astorga, Nabeel Seedat, and Mihaela van~der Schaar. \newblock Large language models to enhance {B}ayesian optimization. \newblock *International Conference on Learning Representations*, 2024.

\bibitem[Hvarfner et~al.(2022)Hvarfner, Stoll, Souza, Nardi, Lindauer, and Hutter]{hvarfner2022pi} Carl Hvarfner, Danny Stoll, Artur Souza, Luigi Nardi, Marius Lindauer, and Frank Hutter. \newblock $\pi${BO}: Augmenting acquisition functions with user beliefs for bayesian optimization. \newblock In *International Conference on Learning Representations*, 2022.

\bibitem[M{\"u}ller et~al.(2023)M{\"u}ller, Feurer, Hollmann, and Hutter]{muller2023pfns4bo} Samuel M{\"u}ller, Matthias Feurer, Noah Hollmann, and Frank Hutter. \newblock Pfns4bo: In-context learning for bayesian optimization. \newblock In *International Conference on Machine Learning*, pages 25444--25470. PMLR, 2023.

\bibitem[Balandat et~al.(2020)Balandat, Karrer, Jiang, Daulton, Letham, Wilson, and Bakshy]{balandat2020botorch} Maximilian Balandat, Brian Karrer, Daniel Jiang, Samuel Daulton, Ben Letham, Andrew~G Wilson, and Eytan Bakshy. \newblock {BoTorch}: A framework for efficient {M}onte-{C}arlo {B}ayesian optimization. \newblock *Advances in Neural Information Processing Systems*, 33:\penalty0 21524--21538, 2020.

\bibitem[Uhlenbeck and Ornstein(1930)]{uhlenbeck1930theory} George~E Uhlenbeck and Leonard~S Ornstein. \newblock On the theory of the {B}rownian motion. \newblock *Physical Review*, 36\penalty0 (5):\penalty0 823, 1930.

\bibitem[Kermack and McKendrick(1927)]{kermack1927contribution} William~O Kermack and Anderson~G McKendrick. \newblock A contribution to the mathematical theory of epidemics. \newblock *Proceedings of the Royal Society of London. Series A, Containing papers of a mathematical and physical character*, 115\penalty0 (772):\penalty0 700--721, 1927.

\bibitem[Turin et~al.(1972)Turin, Clapp, Johnston, Fine, and Lavry]{turin1972statistical} George~L Turin, Fred~D Clapp, Tom~L Johnston, Stephen~B Fine, and Dan Lavry. \newblock A statistical model of urban multipath propagation. \newblock *IEEE Transactions on Vehicular Technology*, 21\penalty0 (1):\penalty0 1--9, 1972.

\bibitem[Greenberg et~al.(2019)Greenberg, Nonnenmacher, and Macke]{greenberg2019automatic} David Greenberg, Marcel Nonnenmacher, and Jakob Macke. \newblock Automatic posterior transformation for likelihood-free inference. \newblock In *International Conference on Machine Learning*, pages 2404--2414. PMLR, 2019.

\bibitem[Miller et~al.(2022)Miller, Weniger, and Forr{\'e}]{miller2022contrastive} Benjamin~K Miller, Christoph Weniger, and Patrick Forr{\'e}. \newblock Contrastive neural ratio estimation. \newblock *Advances in Neural Information Processing Systems*, 35:\penalty0 3262--3278, 2022.

\bibitem[Talts et~al.(2018)Talts, Betancourt, Simpson, Vehtari, and Gelman]{talts2018validating} Sean Talts, Michael Betancourt, Daniel Simpson, Aki Vehtari, and Andrew Gelman. \newblock Validating bayesian inference algorithms with simulation-based calibration. \newblock *arXiv preprint arXiv:1804.06788*, 2018.

\bibitem[Avilov et~al.(2023)Avilov, Li, Stone, and He]{avilov20231978} Konstantin Avilov, Qiong Li, Lewi Stone, and Daihai He. \newblock The 1978 english boarding school influenza outbreak: Where the classic seir model fails. \newblock *SSRN 4586177*, 2023.

\bibitem[Huang et~al.(2023{\natexlab{a}})Huang, Bharti, Souza, Acerbi, and Kaski]{huang2024learning} Daolang Huang, Ayush Bharti, Amauri Souza, Luigi Acerbi, and Samuel Kaski. \newblock Learning robust statistics for simulation-based inference under model misspecification. \newblock *Advances in Neural Information Processing Systems*, 36, 2023{\natexlab{a}}.

\bibitem[Schmitt et~al.(2023)Schmitt, B{\"u}rkner, K{\"o}the, and Radev]{schmitt2023detecting} Marvin Schmitt, Paul-Christian B{\"u}rkner, Ullrich K{\"o}the, and Stefan~T Radev. \newblock Detecting model misspecification in amortized {B}ayesian inference with neural networks. \newblock In *DAGM German Conference on Pattern Recognition*, pages 541--557. Springer, 2023.

\bibitem[Huang et~al.(2023{\natexlab{b}})Huang, Haussmann, Remes, John, Clart{\'e}, Luck, Kaski, and Acerbi]{huang2023practical} Daolang Huang, Manuel Haussmann, Ulpu Remes, ST~John, Gr{\'e}goire Clart{\'e}, Kevin Luck, Samuel Kaski, and Luigi Acerbi. \newblock {P}ractical equivariances via {R}elational {C}onditional {N}eural {P}rocesses. \newblock *Advances in Neural Information Processing Systems*, 36:\penalty0 29201--29238, 2023{\natexlab{b}}.

\bibitem[Liu et~al.(2020)Liu, Sun, Ramadge, and Adams]{liu2020task} Sulin Liu, Xingyuan Sun, Peter~J Ramadge, and Ryan~P Adams. \newblock Task-agnostic amortized inference of {G}aussian process hyperparameters. \newblock In *Advances in Neural Information Processing Systems*, volume~33, pages 21440--21452, 2020.

\bibitem[Gordon et~al.(2020)Gordon, Bruinsma, Foong, Requeima, Dubois, and Turner]{gordon2020convolutional} Jonathan Gordon, Wessel~P Bruinsma, Andrew~YK Foong, James Requeima, Yann Dubois, and Richard~E Turner. \newblock Convolutional conditional neural processes. \newblock In *International Conference on Learning Representations*, 2020.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model} Chelsea Finn, Pieter Abbeel, and Sergey Levine. \newblock Model-agnostic meta-learning for fast adaptation of deep networks. \newblock In *International Conference on Machine Learning*, pages 1126--1135. PMLR, 2017.

\bibitem[Finn et~al.(2018)Finn, Xu, and Levine]{finn2018probabilistic} Chelsea Finn, Kelvin Xu, and Sergey Levine. \newblock Probabilistic model-agnostic meta-learning. \newblock *Advances in Neural Information Processing Systems*, 31, 2018.

\bibitem[Simpson et~al.(2021)Simpson, Davies, Lalchand, Vullo, Durrande, and Rasmussen]{simpson2021kernel} Fergus Simpson, Ian Davies, Vidhi Lalchand, Alessandro Vullo, Nicolas Durrande, and Carl~Edward Rasmussen. \newblock Kernel identification through transformers. \newblock In *Advances in Neural Information Processing Systems*, volume~34, pages 10483--10495, 2021.

\bibitem[Kobalczyk and van~der Schaar(2024)]{kobalczyk2024informed} Katarzyna Kobalczyk and Mihaela van~der Schaar. \newblock Informed meta-learning. \newblock *arXiv preprint arXiv:2402.16105*, 2024.

\bibitem[Lueckmann et~al.(2017)Lueckmann, Goncalves, Bassetto, {\"O}cal, Nonnenmacher, and Macke]{lueckmann2017flexible} Jan-Matthis Lueckmann, Pedro~J Goncalves, Giacomo Bassetto, Kaan {\"O}cal, Marcel Nonnenmacher, and Jakob~H Macke. \newblock Flexible statistical inference for mechanistic models of neural dynamics. \newblock *Advances in Neural Information Processing Systems*, 30, 2017.

\bibitem[Papamakarios et~al.(2019)Papamakarios, Sterratt, and Murray]{papamakarios2019sequential} George Papamakarios, David Sterratt, and Iain Murray. \newblock Sequential neural likelihood: {F}ast likelihood-free inference with autoregressive flows. \newblock In *The 22nd International Conference on Artificial Intelligence and Statistics*, pages 837--848. PMLR, 2019.

\bibitem[Song and Ermon(2019)]{song2019generative} Yang Song and Stefano Ermon. \newblock Generative modeling by estimating gradients of the data distribution. \newblock *Advances in Neural Information Processing Systems*, 32, 2019.

\bibitem[Murphy et~al.(2019)Murphy, Srinivasan, Rao, and Ribeiro]{murphy2019janossy} Ryan~L Murphy, Balasubramaniam Srinivasan, Vinayak Rao, and Bruno Ribeiro. \newblock {J}anossy pooling: Learning deep permutation-invariant functions for variable-size inputs. \newblock In *International Conference on Learning Representations*, 2019.

\bibitem[Hensman et~al.(2015)Hensman, Matthews, and Ghahramani]{hensman2015scalable} James Hensman, Alexander Matthews, and Zoubin Ghahramani. \newblock Scalable variational {G}aussian process classification. \newblock In *Artificial Intelligence and Statistics*, pages 351--360. PMLR, 2015.

\bibitem[Kingma and Ba(2015)]{kingma14} Diederik~P. Kingma and Jimmy Ba. \newblock Adam: {A} method for stochastic optimization. \newblock In *3rd International Conference on Learning Representations, {ICLR* 2015}. PMLR (Proceedings of Machine Learning Research), 2015.

\bibitem[Robert and Casella(2004)]{robert2004monte} Christian~P. Robert and George Casella. \newblock *Monte Carlo Statistical Methods*. \newblock Springer Texts in Statistics. Springer, 2nd edition, 2004.

\bibitem[Lueckmann et~al.(2021)Lueckmann, Boelts, Greenberg, Goncalves, and Macke]{lueckmann2021benchmarking} Jan-Matthis Lueckmann, Jan Boelts, David Greenberg, Pedro Goncalves, and Jakob Macke. \newblock Benchmarking simulation-based inference. \newblock In *Proc. AISTATS*, pages 343--351. PMLR, 2021.

\bibitem[Pedersen(2019)]{pedersen2019stochastic} Troels Pedersen. \newblock Stochastic multipath model for the in-room radio channel based on room electromagnetics. \newblock *IEEE Transactions on Antennas and Propagation*, 67\penalty0 (4):\penalty0 2591--2603, 2019.

\bibitem[Bharti et~al.(2019)Bharti, Adeogun, and Pedersen]{bharti2019estimator} Ayush Bharti, Ramoni Adeogun, and Troels Pedersen. \newblock Estimator for stochastic channel model without multipath extraction using temporal moments. \newblock In *2019 IEEE 20th International Workshop on Signal Processing Advances in Wireless Communications (SPAWC)*, pages 1--5. IEEE, 2019.

\bibitem[Tejero-Cantero et~al.(2020)Tejero-Cantero, Boelts, Deistler, Lueckmann, Durkan, Gonçalves, Greenberg, and Macke]{tejero-cantero2020sbi} Alvaro Tejero-Cantero, Jan Boelts, Michael Deistler, Jan-Matthis Lueckmann, Conor Durkan, Pedro~J. Gonçalves, David~S. Greenberg, and Jakob~H. Macke. \newblock sbi: A toolkit for simulation-based inference. \newblock *Journal of Open Source Software*, 5\penalty0 (52):\penalty0 2505, 2020.

\bibitem[Papamakarios et~al.(2017)Papamakarios, Pavlakou, and Murray]{papamakarios2017masked} George Papamakarios, Theo Pavlakou, and Iain Murray. \newblock Masked autoregressive flow for density estimation. \newblock *Advances in Neural Information Processing Systems*, 30, 2017.

\bibitem[S{\"a}ilynoja et~al.(2022)S{\"a}ilynoja, B{\"u}rkner, and Vehtari]{sailynoja2022graphical} Teemu S{\"a}ilynoja, Paul-Christian B{\"u}rkner, and Aki Vehtari. \newblock Graphical test for discrete uniformity and its applications in goodness-of-fit evaluation and multiple sample comparison. \newblock *Statistics and Computing*, 32\penalty0 (2):\penalty0 32, 2022.

\bibitem[Bingham et~al.(2018)Bingham, Chen, Jankowiak, Obermeyer, Pradhan, Karaletsos, Singh, Szerlip, Horsfall, and Goodman]{bingham2018pyro} Eli Bingham, Jonathan~P. Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan, Theofanis Karaletsos, Rohit Singh, Paul Szerlip, Paul Horsfall, and Noah~D. Goodman. \newblock {Pyro: Deep Universal Probabilistic Programming}. \newblock *Journal of Machine Learning Research*, 2018.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch} Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al. \newblock Pytorch: An imperative style, high-performance deep learning library. \newblock *Advances in Neural Information Processing Systems*, 32, 2019.

\end{thebibliography}

\appendix

\addtocontents{toc}{\protect\setcounter{tocdepth}{2}}

\onecolumn

\setcounter{figure}{0} \setcounter{table}{0} \setcounter{equation}{0}

\renewcommand{\thefigure}{S\arabic{figure}} \renewcommand{\thetable}{S\arabic{table}} \renewcommand{\theequation}{S\arabic{equation}}

\aistatstitle{Supplementary Material} \label{app:intro}

*{-14cm}

\tableofcontents

# Related Work
\label{app:ex_related_work}

Our work combines insights from different fields such as neural processes, meta-learning, and simulation-based inference, providing a new unified and versatile framework for amortized inference.

#### Neural processes.

ACE relates to the broader work on neural processes \citep{garnelo2018neural, garnelo2018conditional, kim2019attentive, gordon2020convolutional, markou2022practical, nguyen2022transformer, huang2023practical}. Unlike previous methods that focused on predictive data distributions conditioned on observed data, ACE also explicitly models and conditions on latent variables of interest. While Latent Neural Processes (LNPs) capture correlations via non-interpretable latent vectors that yield an intractable learning objective \citep{garnelo2018neural}, ACE uses autoregressive sampling to model correlations \citep{bruinsma2023autoregressive}, and `latents' in ACE denote explicitly modelled task variables.

#### Meta-learning.

Our approach falls within the fields of amortized inference, meta-learning, and pre-trained models, which overlap significantly with neural process literature \citep{finn2017model, finn2018probabilistic}. Prior-Fitted Networks (PFNs) demonstrated the use of transformers for Bayesian inference \citep{muller2022transformers} and optimization \citep{muller2023pfns4bo}, focusing on predictive posterior distributions over data using fixed bins (`Riemannian' distributions). In particular, \citet{muller2023pfns4bo} allow users to indicate a `high-probability' interval for the optimum location at runtime within a set of specified ranges. ACE differs from these methods by allowing the specification of more flexible priors at runtime, using learnable mixtures of Gaussians for continuous values, and, most importantly, for yielding explicit predictive distributions over both data and task-relevant, interpretable latents. While other works target specific optimization tasks \citep{liu2020task, simpson2021kernel, amos2022tutorial}, we provide a generative model for random functions with known optima, directly amortizing predictive distributions useful for optimization (see \cref{app:bo}). By providing methods to condition on interpretable latents as well as injecting probabilistic knowledge (priors), our work aligns with the principles of *informed meta-learning* \citep{kobalczyk2024informed}.

#### Simulation-based inference.

ACE is related to the extensive literature on amortized or neural simulation-based inference (SBI) \citep{cranmer2020frontier}, where deep networks are used to (a) recover the posterior distribution over model parameters \citep{lueckmann2017flexible}, and (b) emulate the simulator or likelihood \citep{papamakarios2019sequential}. While these two steps are often implemented separately, recent work has started to combine them \citep{radev2023jana}. In particular, the recently proposed Simformer architecture allows users to freely condition on observed variables and model parameters, in a manner similar to ACE \citep{gloeckler2024all}. Simformer uses a combination of a transformer architecture and a denoising diffusion model \citep{song2019generative}, which makes it suitable for continuous predictions but does not immediately apply to discrete outputs. Notably, Simformer allows the user to specify *intervals* for variables at runtime, but not (yet) general priors. To our knowledge, the only work in amortized SBI that affords some meaningful flexibility in prior specification at runtime is \citet{elsemueller2024sensitivity}. Even so, the choice there is between a limited number of fixed priors or a global prior scale parameter, with the main focus being sensitivity analysis \citep{elsemueller2024sensitivity}. Importantly, all these works focus entirely on SBI settings, while our paper showcases the applicability of ACE to a wider variety of machine learning tasks.

# Methods

## Sampling of latents
\label{app:sampling}

ACE can condition and predict data, latent variables, and combinations of both. Here, we outline the sampling process used to construct the training batch.

- First, we generate our dataset by following the steps outlined for the respective cases (GP, Image Completion, BO, SBI). For example, in the GP emulation case, we draw $n_{\text{data}}$ points from a function sampled from a GP along with its respective latent variables.
- Next, we sample the number of context points, $n_{\text{context}}$, uniformly between the minimum and maximum context points, $\text{min\_ctx}$ and $\text{max\_ctx}$. We then split our data based on this $n_{\text{context}}$ value; the remaining data points that are not in the context set are allocated to the target set.
- We then determine whether the context includes any latent variables at all with a 50\% probability. If latent variables are to be included in the context set, we sample the number of latents residing in the context set, uniformly from 1 to $n_{\text{latents}}$. All latent variables not in the context set are assigned to the target set.

The above steps are applied for each element (dataset) in the training batch. In the implementation, we also ensure that, within each batch, the number of context points remains consistent across all elements, as does the number of target points, to facilitate batch training for our model. However, the number of latents in the context set may vary for each element, introducing variability that improves the model's training process.

## Details and experiments with prior injection
\label{app:priors}

#### Prior generative process.
\label{app:prior}

To expose ACE to a wide array of distinct priors during training, we generate priors following a hierarchical approach that generates smooth priors over a bounded range. The process is as follows, separately for each latent variable $\theta_l$, for $1 \le l \le L$:

- We first sample the type of priors for the latent variable. Specifically, with 80\% probability, we sample from a mixture of Gaussians to generate a smooth prior, otherwise, we create a flat prior with uniform distribution.
- If we sample from a mixture of Gaussians: \begin{itemize}
- We first sample the number of Gaussian components $K$ from a geometric distribution with $q=0.5$:

$$
K \sim \text{Geometric}(0.5).
$$
- If $K > 1$, we randomly choose among three configurations with equal probability:

1. Same means and different standard deviations.
2. Different means and same standard deviations.
3. All different means and standard deviations.
- Given the predefined global priors for mean and standard deviation (uniform over a range), we sample the means $\mu$ and standard deviations $\sigma$ for each component.
- The weights of the mixture components are sampled from a Dirichlet distribution:

$$
\mathbf{w} \sim \text{Dirichlet}(\alpha_0 = 1).
$$
- Finally, we convert the mixture of Gaussians into a normalized histogram over a grid $\mathcal{G}$ with $N_\text{bins}$ uniformly-spaced bins. For each bin $b$, we compute the probability mass $\mathbf{p}^{(b)}_l$ by calculating the difference between the cumulative distribution function values at the bin edges. This is done for each Gaussian component and then summed up, weighted by the mixture weights.
- We normalize the bin probabilities to ensure a valid probability distribution:

$$
\mathbf{p}_l = \frac{\mathbf{p}_l}{\sum_{b=1}^{N_\text{bins}} \mathbf{p}^{(b)}_l}.
$$

- If we sample from a uniform distribution:

- We assign equal probability to each bin over the grid:

$$
\mathbf{p}_l = \frac{1}{N_\text{bins}} \mathbf{1}_{N_\text{bins}},
$$

where $\mathbf{1}_{N_\text{bins}}$ is a vector of ones of length $N_\text{bins}$.

\end{itemize}

For all experiments, we select $N_\text{bins} = 100$ as the number of bins for the prior grid. See \cref{fig:prior_injection_samples} for some examples of sampled priors.

**Figure:** Examples of randomly sampled priors over the range $[0, 2]$. Samples includes mixtures of Gaussians and Uniform distributions. \label{fig:prior_injection_samples}

#### Investigation of prior injection with a Gaussian toy model.
\label{app:gaussian}

To investigate the effect of the injected prior, we test our method with a simple 1D Gaussian model with two latent variables: mean $\mu$ and standard deviation $\sigma$. The data is the samples drawn from this distribution, $\data_N = \{y_n\}_{n=1}^N \sim \mathcal{N}\left(\mu, \sigma^2\right)$. We can numerically compute the exact Bayesian posterior on the predefined grid given the data and any prior, and subsequently compare the ground-truth posterior with the ACE's predicted posterior after injecting the same prior.

We first sample random priors using the generative process described above. Then we sample $\mu$ and $\sigma$ from the priors and generate the corresponding data $\data_N$. We pass the data along with the prior to ACE to get the predictive distributions $p(\mu|\data_N)$ and $p(\sigma | \data_N)$ as well as the autoregressive predictions $p(\mu|\sigma, \data_N)$ and $p(\sigma|\mu, \data_N)$.

With these equations, we can autoregressively compute our model's prediction for $p(\mu, \sigma|\data_N)$ on the grid.

The true posterior is calculated numerically via Bayes rule on the grid. See \cref{fig:gaussian_examples} for several examples.

To quantitatively assess the quality of our model's predicted posteriors, we compare the posterior mean and standard deviation (i.e., the first two moments\footnote{We prefer standard deviation to variance as it has the same units as the quantity of interest, as opposed to squared units which are less interpretable.}) for $\mu$ and $\sigma$ of predicted vs. true posteriors, visualized in \cref{fig:gaussian_scatter}. The scatter points are aligned along the diagonal line, indicating that the moments of the predicted posterior closely match the moments of true posterior. These results show that ACE is effectively incorporating the information provided by the prior and adjusts the final posterior accordingly. In \cref{app:sbc} we perform a more extensive analysis of posterior calibration in ACE with a complex simulator model.

**Figure:** Examples of the true and predicted posterior distributions in the toy 1D Gaussian case. (a) Prior distribution over $\vtheta = (\mu, \sigma)$ set at runtime. (b) Likelihood for the observed data (not shown). (c) Ground-truth Bayesian posterior. (d) ACE's predicted posterior, based on the user-set prior and observed data, approximates well the true posterior. \label{fig:gaussian_examples}

**Figure:** The scatter plots compare the predicted and true posterior mean and standard deviation values for both $\mu$ and $\sigma$ across 100 examples. We can see that the points lie closely along the diagonal red dashed line, indicating that the moments (mean and standard deviation) of the predicted posteriors closely match the true posteriors. \label{fig:gaussian_scatter}

## Autoregressive predictions
\label{app:autoregressive}

While ACE predicts conditional marginals independently, we can still obtain *joint* predictions over both data and latents autoregressively \citep{nguyen2022transformer,bruinsma2023autoregressive}. Suppose we want to predict the joint target distribution $p(\z^\star_{1:M}| \vxi^\star_{1:M}, \dataplus_N)$, where we use compact indexing notation. We can write:

$$
p(\z^\star_{1:M}| \vxi^\star_{1:M}, \dataplus_N) = \prod_{m=1}^M p(z^\star_m | \z^\star_{1:m-1}, \vxi^\star_{1:m}, \dataplus_N) = \mathbb{E}_{\vpi} \left[ \prod_{m=1}^M p(z^\star_{\pi_m} | \z^\star_{\pi_1:\pi_{m-1}}, \vxi^\star_{\pi_1:\pi_m}, \dataplus_N) \right],
$$

where $\vpi$ is a permutation of $(1, \ldots, M)$, i.e. an element of the symmetric group $\mathcal{S}_M$. The first passage follows from the standard rules of probability and the second passage follows from permutation invariance of the joint distribution with respect to the ordering of the variables $\vxi_{1:M}$. The last expression can be used to enforce permutation invariance and validity of our joint predictions even if sequential predictions of the model are not natively invariant \citep{murphy2019janossy}. In practice, for moderate to large $M$ ($M \gtrsim 4$) we approximate the expectation over permutations via Monte Carlo sampling.

# Experimental details
\label{app:experiments}

In this section, we show additional experiments to validate our method and provide additional details about sampling, training, and model architecture.

## Gaussian process (GP) experiments
\label{app:gp}

**Figure:** (a) Conditioning on the latent variable $\vtheta$ (kernel hyperparameters and type) improves predictive performance, approaching the GP upper bound for the log predictive density. (b) ACE can identify the kernel $\kappa$. (c) ACE can learn kernel hyperparameters. \label{fig:condition} \label{fig:accuracy} \label{fig:latent} \label{fig:GP}

We now demonstrate the use of ACE for performing amortized inference tasks in the Gaussian processes (GP) model class. GPs are a Bayesian non-parametric method used as priors over functions (see \citealp{rasmussen2006gaussian}). To perform inference with a GP, one must first define a kernel function $\kappa_\vtheta$ parameterized by hyperparameters $\vtheta$ such as lengthscales and output scale. As a flexible model of distributions over functions used for regression and classification, GPs are a go-to generative model for meta-learning and feature heavily in the (conditional) neural process literature (CNP; \citealp{garnelo2018conditional}). ACE can handle many of the challenges faced when applying GPs. Firstly, it can accurately amortize the GP predictive distribution as is usually shown in the CNP literature. In addition, ACE can perform other crucial tasks in the GP workflow, such as amortized learning of $\vtheta$, usually found through optimization of the marginal likelihood (Gaussian likelihood) or via approximate inference for non-Gaussian likelihoods (e.g., \citealt{hensman2015scalable}). Furthermore, we can also do kernel selection by treating the kernel as a latent discrete variable, and incorporate prior knowledge about $\vtheta$.

**Results.** The main results from our GP regression experiments are displayed in \cref{fig:GP}. We trained ACE on samples from four kernels, the RBF and Matérn-($\nicefrac{1}{2}$, $\nicefrac{3}{2}$, $\nicefrac{5}{2}$), using the architecture described in \cref{sec:ace}; see below for details. In \cref{fig:condition}, we show ACE's ability to condition on provided information: data only, or data and $\vtheta$ (kernel hyperparameters and type). As expected, there is an improvement when conditioning on more information, specifically when the context set $\data_N$ is small. As an upper bound, we show the ground-truth GP predictive performance. The method can accurately predict the kernel, \ie model selection (\cref{fig:accuracy}), while at the same time learn the hyperparameters (\cref{fig:latent}), both improving as a function of the context set size.

#### Sampling from a GP.

Both the GP experiments and the Bayesian optimization experiments reported in \cref{exp:bo} and further detailed in \cref{app:bo} use a similar sampling process to generate data.

- We first sample the latents. These are kernel hyperparameters, the output scale $\sigma_f$ and lengthscale $\mathbf{\ell}$. Each input dimension of $\x$ is assigned its own lengthscale $\mathbf{\ell} = (\ell^{(1)}, \ell^{(2)},...)$ and a corresponding kernel. For all GP examples the RBF and three Matérn-($\nicefrac{1}{2}$, $\nicefrac{3}{2}$, $\nicefrac{5}{2}$) kernels were used with equal weights. The kernel output scale $\sigma_f\sim U(0.1, 1)$ and each $k$-th lengthscale is $\mathbf{\ell}^{(k)} \sim \mathcal{N}(1/3, 0.75)$ truncated between $[0.05, 2]$.
- Once all latent information is defined, we draw from a GP prior from a range $[-1, 1]$ for each input dimension. The realizations from the prior form our context data $\mathcal{D}_{N}$ where the size of the context set $N$ is drawn from a discrete uniform distribution ${3,4,....50}$. The target data $(\mathbf{X}^*,\mathbf{y}^*)$ of size $200 - N$ is then drawn from the predictive posterior of the GP conditioned on $\mathcal{D}_{N}$.

#### Architecture.

The ACE model used in the GP experiments had embedding dimension $256$ and $6$ transformer layers. The attention blocks had $6$ heads and the MLP block had hidden dimension $128$. The output head had $K=20$ MLP components with hidden dimension $256$. The model was trained for $5\times 10^4$ steps with batch size $32$, using learning rate $1 \times 10^{-4}$ with cosine annealing.

**Figure:** Image $\data_N$ TNPD ACE ACE-$\vtheta$ NLPD v Context($\%$ of image) **Image regression (MNIST).** Image (\subref{fig:image1}) serves as the reference for the problem, while (\subref{fig:context}) is the context where $10\%$ of the pixels are observed. Figures (\subref{fig:tnpd}) - (\subref{fig:ACE_theta}) are the respective model predictions, while (\subref{fig:celeba-nlpd}) shows performance over varying context (mean and 95\% confidence interval). In (\subref{fig:ACE_theta}) the model is also conditioned on the class label, showing a clear improvement in performance. \label{fig:image1} \label{fig:context} \label{fig:tnpd} \label{fig:ACE_image} \label{fig:ACE_theta} \label{fig:celeba-nlpd} \label{fig:whole_image}

## Image completion and classification

In this section, we detail the image experiments in \cref{exp:image} as well as report additional experiments. Image completion experiments have long been a benchmark in the neural process literature treating them as regression problems \citep{garnelo2018neural, kim2019attentive}. We use two standard datasets, `MNIST` \citep{deng2012mnist} and `CelebA` \citep{liu2015faceattributes}. The two datasets were downsized to $16 \times 16$ and $32 \times 32$, respectively, and normalised based on the complete dataset average and standard deviation. The data input $\mathbf{x}$ for this experiment is the $2$\text{-}D image pixel-coordinates and the data value $y$ for `MNIST` which has one output dimension, while `CelebA` uses all three RGB channels and thus is a multi-output $\mathbf{y}$.

The experiments on images demonstrate the versatility of the ACE method and its advantages over conventional CNPs. We outperform the current state-of-the-art TNP-D on the standard image completion task \cref{fig:whole_image_celeb_main}. Given a random sample from the image space as context $\data_N$, the model predicts the remaining $M$ image pixel values at $\xt$. The total number of points $N + M$ for `MNIST` is thus $256$ points and $1024$ for `CelebA` where the split is randomly sampled (see below for details). The model is then trained in the way detailed in \cref{sec:training}. Thus, the final trained model can perform image completion, also sometimes known as in-painting.

In addition to image completion, our method can condition on and predict latent variables $\vtheta$. For `MNIST`, we use the class labels as latents, so `0, 1, 2, ...`, which were encoded into a single discrete variable. Meanwhile, for `CelebA` we use as latents the $40$ binary features that accompany the dataset, \eg `BrownHair, Man, Smiling`, trained with the sampling procedure discussed below. We recall that in ACE the latents $\vtheta$ can be both conditioned on and predicted. We can do conditional generation based on the class or features or, given a sample of an image, predict its class or features, as initially promised in \cref{fig:intro}a.

**Figure:** Context `Bald` = `True` `Bald` = `False` Context `Bald` = `True` `Bald` = `False` Example of ACE conditioning on the value of the `Bald` feature when the image is masked for the first 22 rows. (\subref{fig:mask_context}) and (\subref{fig:mask_context_}) show the context points used for prediction, where (\subref{fig:bald_pred}) and (\subref{fig:bald_pred_}) show predictions where the `Bald` feature is conditioned on `True`. Meanwhile, \subref{fig:no_bald_pred} and \subref{fig:no_bald_pred_} are conditioned on `False`. \label{fig:mask_context} \label{fig:bald_pred} \label{fig:no_bald_pred} \label{fig:mask_context_} \label{fig:bald_pred_} \label{fig:no_bald_pred_} \label{fig:masked_celeba_row}

#### Results.

The main image completion results for the `CelebA` dataset are shown in \cref{fig:whole_image_celeb_main}, with the same experiment performed on `MNIST` and displayed in \cref{fig:whole_image}. In both figures, we display some example images and predictions and negative log-probability density for different context sizes (shaded area is 95\% confidence interval). Our method demonstrates a clear improvement over the TNP-D method across all context sizes on both datasets (\cref{fig:whole_image_celeb_main} and \cref{fig:whole_image}). Moreover, incorporating latent information for conditional generation further enhances the performance of our base method. A variation of the image completion experiment is shown in \cref{fig:masked_celeba_row}, where the context is no longer randomly sampled from within the image but instead selected according to a top 22-row image mask. In this case, the latent information `Bald` is either conditioned on `True` or `False`. The results show that the model adjusts its generated output based on the provided latent information, highlighting the potential of conditional generation. Finally, in \cref{fig:image_classification}, we show examples of ACE's ability to perform image classification showing a subset of the $40$ features in `CelebA` dataset. Despite only having $10 \%$ of the image available, ACE can predict most features successfully.

\label{app:image}

**Figure:** Context Full image Classification probability for some features An example showing the classification ability of ACE. (\subref{fig:plot1_dup}) is the context available of the full image displayed in the panel (\subref{fig:plot2_dup}). The probabilities for a subset of features are in (\subref{fig:plot3_dup}). \label{fig:plot1} \label{fig:plot2} \label{fig:plot3} \label{fig:plot1_dup} \label{fig:plot2_dup} \label{fig:plot3_dup} \label{fig:image_classification}

#### Sampling for Image experiments.
\label{app:image_sampling}

For sampling, we use the full available dataset for both `MNIST` and `CelebA`. The sampling procedure for all but \cref{fig:masked_celeba_row} was the same as detailed in \cref{app:sampling}. For `MNIST` it was one latent class label and for `CelebA` all 40 features were used. In \cref{fig:masked_celeba_row}, the sampling procedure was adjusted to represent features that would influence the top 22 rows of the images. Therefore, we selected a subset of seven features, which were `Bald`, `Bangs`, `Black\_Hair`, `Blond\_Hair`, `Brown\_Hair`, `Gray\_Hair` and `Eyeglasses`. The same sampling procedure was performed again but now from a smaller set of features.

#### Architecture and training.

For the image experiments, we used the same embedder layer as in the other experiments. Through grid search, we found that a transformer architecture with 8 heads, 6 layers, and a hidden dimension of 128 performed best. For the MLP layer, we used a dimension of 256. Finally, we reduced the number of components for the output head to $K=3$. We trained the model for $80,000$ iterations using a cosine scheduler with `Adam` \citep{kingma14}, with a learning rate $0.0005$ and a batch size of $64$.

## Bayesian optimization
\label{app:bo}

This section presents ACE's Bayesian Optimization (BO) experiments (\cref{exp:bo} in the main paper) in more detail, including the training data generation, algorithms, benchmark functions, and baselines used in this paper.

### Bayesian Optimization dataset, architecture and training details
\label{sec:bo-dataset}

#### Dataset.

Following the approach used in the GP experiments (\cref{app:gp}), the BO datasets are generated by sampling from a GP and then modified to incorporate the known optimum location and value of the function into the generative process. The procedure for generating the BO datasets is outlined below:

- First, we sample a single kernel from choices of RBF and three Matérn-($\nicefrac{1}{2}$, $\nicefrac{3}{2}$, $\nicefrac{5}{2}$) kernels with weights of $[0.35, 0.1, 0.2, 0.35]$ respectively. Then, we sample whether the kernel is isotropic or not with $p=0.5$. The output scale $\sigma_f$ and lengthscales $l^{(k)}$ are sampled similarly to the GP experiments.
- We sample the GP mean function from the max-value distribution for a normal distribution with scale equal to the GP output scale and with $N$ observations, where $N$ is determined based on the length scale of the kernel. This ensures that the optimum value (approximately) respects the distribution of optima for the GP. With $p=0.1$ probability, we add $\Delta y \sim \exp(1)$ to the mean function to represent an unlikely deeper optimum.
- We sample the optimum location $\xopt$ uniformly randomly inside $[-1,1]^D$ and temporarily set the optimum value $\yopt$ as $0$ (we will later shift it).
- We sample a total of $100 \cdot D$ (context + target) locations where the number of context points is sampled similarly to the GP dataset generation. The maximum number of context points is 50 for the 1D case and 100 for both 2D and 3D cases. Then, the values of this context set are jointly sampled from a GP posterior conditioned on one observation at $(\xopt,\yopt)$. Instead, the target points are sampled *independently* from a GP posterior conditioned on $\data_N$ (the previously sampled context points) and $(\xopt,\yopt)$. Independent sampling of the targets speeds up GP data generation and is valid since *during training* we only predict marginal distributions at the target points.
- To ensure that the global optimum is at $(\xopt,\yopt)$ we add a convex envelope (a quadratic component). Specifically, we transform the $y$ values of the datasets as $y^\prime_i =|y_i| + \frac{1}{5}||\xopt - \x_i||^2$ where $\x_i$ and $y_i$ are the input and output values of all sampled points.
- Lastly, we add an offset to the $y^\prime$ values of sampled points uniformly drawn from $[-5, 5]$, meaning that $\yopt \in [-5, 5]$.

One and two-dimensional examples of the sampled functions are illustrated in \cref{fig:bo_dataset_1d} and \cref{fig:bo_dataset_2d}, respectively.

**Figure:** One-dimensional Bayesian optimization dataset samples, with their optimum (red dot). \label{fig:bo_dataset_1d}

**Figure:** Two-dimensional Bayesian optimization dataset samples, with their optimum (red dot). \label{fig:bo_dataset_2d}

#### Architecture and training details.

In the BO experiments, the ACE model was configured with an embedding dimension of $D_\text{emb} = 256$ and consisted of six transformer layers. Each attention block included $16$ heads, while the MLP block had a hidden dimension of $128$. The output head comprised $K=20$ MLP components, each with a hidden dimension of $128$. The model was trained for $5\times 10^5$ steps with a batch size of $64$, using learning rate $5 \times 10^{-4}$ with cosine annealing. We apply loss weighing to give more importance to the latent variables during training. This adjustment accounts for the fact that the number of latent variables, $ n_{\text{latent}}$, is generally much smaller than the number of data points, $(n_{\text{total}}-n_{\text{latent}})$. The weight assigned to the latent loss is calculated as $w_{\text{latent}} = ({n_{\text{total}} - \nicefrac{1}{2} \left( \text{max\_ctx} + \text{min\_ctx} \right)}/{n_{\text{latent}}})^{T}$ where $T$ is a tunable parameter, $\text{max\_ctx}$ and $\text{max\_ctx}$ are the maximum and minimum number of context points during the dataset generation. We conducted a grid search over $T = 1, \nicefrac{2}{3}, \nicefrac{1}{3}, 0$ to identify the best-performing model. For our experiments, the best $T$ values are 1 in 1D and $\nicefrac{2}{3}$ in 2D and 3D. We found that including latent loss weighing stabilized training. Note that ACE has different models trained with different datasets for each input dimensionality.

### ACE-BO Algorithm
\label{sec:ace-bo-alg}

\begin{algorithm} \caption{ACE-Thompson Sample (D>1)}\label{alg:ACE-TS-ND} \begin{algorithmic}[1] \Require observed data points $\data_N = \{\x_{1:N}, y_{1:N}\}$, improvement parameter $\alpha$, input dimensionality $D \in \mathbb{N}^{+}$, whether to condition on $\yopt$\ or not flag $c \in \{\text{True}, \text{False}\}$. \State **Initialization** $y_\text{min} \gets \min{y_{1:N}}$, $y_\text{max} \gets \max{y_{1:N}}$. \If {$c$ is $\text{True}$} \State set threshold value: $\tau \gets y_\text{min} - \alpha \max(1,y_\text{max}-y_\text{min}) $. \State sample $\yopt$ from mixture truncated at $\tau$: $\yopt \sim p(\yopt| \data_N, \yopt<\tau)$. \EndIf \State randomly permute dimension indices: $(1, \dots, D) \rightarrow (\pi_1, \dots, \pi_D)$. \Comment{$\vpi$ is permutation of $(1, \dots, D)$} \For{$i \gets \pi_1, \dots, \pi_D$} \If {$c$ is $\text{True}$} \State sample $x^{i}_\text{opt}$ conditioned on $\yopt$, $\data_N$, and already sampled $\xopt$ dimensions if any: \State $x^{i}_\text{opt} \sim p(x^{i}_\text{opt}| \data_N, \yopt, x^{0:i-1}_\text{opt})$. \Else \State sample $x^{i}_\text{opt}$ conditioned on $\data_N$ and already sampled $\xopt$ dimensions if any: \State $x^{i}_\text{opt} \sim p(x^{i}_\text{opt}| \data_N, x^{0:i-1}_\text{opt})$. \EndIf \EndFor \State get full value of $\xopt$ using the true indices: $\xopt \gets (x^{1}_\text{opt}, \dots ,x^{D}_\text{opt})$.

\State \Return $\xopt$ \end{algorithmic} \end{algorithm}

\begin{algorithm} \caption{ACE-MES}\label{alg:ACE-MES} \begin{algorithmic}[1] \Require observed data points $\data_N = \{x_{1:N}, y_{1:N}\}$, number of candidate points $N_\text{cand}$, Thompson sampling ratio for candidate point $TS_\text{ratio}$. \State **Initialization** $N_{TS1} \gets N_\text{cand} \times TS_\text{ratio}$, $N_{TS2} \gets N_\text{cand} \times (1-TS_\text{ratio})$. \State propose $N_\text{cand}$ candidate points $X^*_{1:N_\text{cand}}$ according to $TS_\text{ratio}$: \State sample $X^*_{1:N_{TS1}}$ using ACE-TS with conditioning on $\yopt$ ($c = \text{True}$). \State sample $X^*_{N_{TS1}+1:N_{TS1}+N_{TS2}}$ using ACE-TS without conditioning on $\yopt$ ($c = \text{True}$). \For {$i \gets 1$ **to** $N_\text{cand}$}: \State sample $\yopt$ for conditioning: $\yopt \sim p(\yopt| \data_N)$. \State $\alpha_{(i)}(\mathbf{x}^*_{(i)}) = H[p(y^*_{(i)}| \mathbf{x}^*_{(i)}, \data_N)] - \mathbb{E}(H[p(y^*_{(i)}| \mathbf{x}^*_{(i)}, \data_N, \yopt)])$ \Comment{see \cref{sec:ace-bo-alg} for more detail} \EndFor \State $\xopt = \text{arg}\,\max \boldsymbol{\alpha}$. \State \Return $\xopt$ \end{algorithmic} \end{algorithm}

#### Bayesian optimization with Thompson sampling (ACE-TS).

For Thompson sampling, we sample the query point at each step from $p(\xopt| \data_N, \yopt < \tau)$ where $\tau$ is a threshold lower than the minimum point seen so far. This encourages exploration to sample a new point that is lower than the current optimum. We set $\tau=y_\text{min} - \alpha \max(1, y_\text{max}-y_\text{min})$, where $y_\text{max}$ and $ y_\text{min}$ are the maximum and minimum values currently observed so far, and $\alpha$ a parameter controlling the minimum improvement. We set $\alpha = 0.01$ throughout all experiments. First, we sample $\yopt$ from a truncated mixture of Gaussian obtained from ACE's predictive distribution $p(\yopt| \data_N)$, truncated for $\yopt < \tau$. After that, we sample $\xopt$ conditioned on that sampled $\yopt$ (i.e., sample from $p(\xopt|\data_N, \yopt < \tau)$). For higher dimension $(D>1)$ we sample $\xopt$ in an autoregressive manner, one dimension at a time. The order of the dimensions is randomly permuted to mitigate order bias among the dimensions. The detailed pseudocode for ACE-TS (D>1) is presented in Algorithm \cref{alg:ACE-TS-ND}. An example evolution of ACE-TS is reported in \cref{fig:bo_evolution}.

#### Bayesian optimization with Minimum-value Entropy Search (ACE-MES).

For Minimum-value Entropy Search (MES; \citealp{wang2017max}), the procedure is as follows:

1. First, we propose $N_\text{candidate}$ points. We generate these candidate points by sampling 80\% of them using the conditional Thompson sampling approach described earlier, i.e., $p(\xopt| \data_N, \yopt < \tau)$, and the remaining 20\% directly from $p(\xopt|\data_N)$. In our experiments we use $N_\text{candidate} = 20$.
2. For each candidate point $\xs$, we evaluate the acquisition function, which in our case is the gain in mutual information between the maximum $\yopt$ and the candidate point $\xs$ (\cref{eq:mes}).
3. To compute the first term of the right-hand side of \cref{eq:mes}, for a candidate point $\xs$, we calculate the predictive distribution $p(y^\star| \mathbf{x}^\star, \data_N)$ represented in our model by a mixture of Gaussians. We compute its entropy via numerical integration over a grid.
4. For the second term of the right-hand side of \cref{eq:mes}, we perform Monte Carlo sampling to evaluate the expected entropy. For each candidate point $\xs$, we draw $N_\text{mc}$ samples of $\yopt$ from the predictive distribution $p(\yopt | \data_N)$. We set $N_\text{mc} = 20$ to ensure the procedure remains efficient while maintaining accuracy.
5. For each sampled $\yopt$, we determine the predictive distribution $p(y^\star| \mathbf{x}^\star, \data_N, \yopt)$. Then, for each mixture, we compute the entropy as in step 2. We then average over samples to compute the expectation.
6. To compute the estimated MES value of candidate point $\xs$ we subtract the computed first term to the second term of the equation \cref{eq:mes}.
7. We repeat this procedure for all candidate points $\xs$ and select the point with the highest information gain. This point is expected to yield the lowest uncertainty about the value of the minimum, thus guiding our next query in the Bayesian optimization process.

The pseudocode for ACE-MES is presented in \cref{alg:ACE-MES}.

**Figure:** **Bayesian optimization example.** We show here an example evolution of ACE-TS on a 1D function. The orange pdf on the left of each panel is $p(\yopt|\data_N)$, the red pdf at the bottom of each panel is $p(x_\text{opt}|\yopt,\data_N)$, for a sampled $\yopt$ (orange dashed-dot line). The acquired point at each iteration is shown as a red asterisk. Note how ACE is able to learn complex conditional predictive distributions for $\xopt$ and $\yopt$. \label{fig:bo_evolution}

### Benchmark functions and baselines
\label{sec:bo-benchmarks}

We use a diverse set of benchmark functions with input dimensions ranging from 1D to 3D to thoroughly evaluate ACE's performance on the BO task. These include (1) the non-convex Ackley function in both 1D and 2D, (2) the 1D Gramacy-Lee function, known for its multiple local minima, (3) the 1D Negative Easom function, characterized by a sharp, narrow global minimum and deceptive flat regions, (4) the non-convex 2D Branin Scaled function with multiple global minima, (5) the Michalewicz function, which features a complex landscape with multiple local minima, (6) the 3D Levy function, with numerous local minima due to its sinusoidal component, and (7) the 3D Hartmann function, a widely used standard benchmark. These functions present a range of challenges, allowing us to effectively test the robustness and accuracy of ACE across different scenarios.

For our baselines, we employ three methods: autoregressive Thompson Sampling with TNP-D (AR-TNPD-TS) \citep{nguyen2022transformer}, Gaussian Process-based Bayesian Optimization with the MES acquisition function (GP-MES) \citep{wang2017max}, and Gaussian Process-based Thompson Sampling (GP-TS) with 5000 candidate points. In addition, we use $\pi$BO-TS for the prior injection case as the baseline \cite{hvarfner2022pi} (using the same number of candidate points used in GP-TS). We optimize the acquisition function in GP-MES using the `shotgun' procedure detailed later in this section (with 1000 candidate points for minimum value approximation via Gumbel sampling). Both GP-MES and GP-TS implementations are written using the BOTorch library \citep{balandat2020botorch}. For AR-TNPD-TS, we use the same network architecture configurations as ACE, but with a non-linear embedder and a single Gaussian head \citep{nguyen2022transformer}. Additionally, AR-TNPD-TS uses autoregressive sampling, as described in \citep{bruinsma2023autoregressive}.

We conducted our experiments with 100 BO iterations across all benchmark functions. The number of initial points was set to 3 for 1D experiments and 10 for 2D and 3D experiments. These initial points were drawn uniformly randomly within the input domain. We evaluated the runtime performance of our methods and baseline algorithms on a local machine equipped with a 13th Gen Intel(R) Core(TM) i5-1335U processor and 15GB of RAM. On average, the runtime for 100 Bayesian Optimization (BO) iterations was as follows: ACE-TS and ACEP-TS completed in approximately 5 seconds; ACE-MES required about 1.3 minutes; GP-TS and $\pi$BO-TS took roughly 2 minutes; GP-MES took about 1.4 minutes; and AR-TNPD-TS was the slowest, requiring approximately 10 minutes, largely due to the computational cost of its autoregressive steps.

#### Shotgun optimization.

To perform fast optimization in parallel, we first sample 10000 points from a quasirandom grid using the Sobol sequence. Then we pick the point with the highest acquisition function value, referred to as $\bold{x}_0$. Subsequently, we sample 1000 points around $\x_0$ using a multivariate normal distribution with diagonal covariance $\sigma^2 \mathbf{I}$, where the initial $\sigma$ is set to the median distance among the points. We re-evaluate the acquisition function over this neighborhood, including $\x_0$, and select the best point. After that, we reduce $\sigma$ by a factor of five and repeat the process, iterating from the current best point. This `shotgun' approach allows us to zoom into a high-valued region of the acquisition function while exploiting large parallel evaluations.

### Bayesian optimization with prior over $\xopt$
\label{sec:bo_prior}

ACE is capable of injecting a prior over latents when predicting values and latents. In the context of BO, this prior could incorporate information about the location of the optimum, $\xopt$. Several works, such as \citep{souza2021bayesian, hvarfner2022pi, muller2023pfns4bo}, have explored the use of priors in BO to improve predictive performance. In our experiments, we evaluate two types of priors: strong and weak, to assess the robustness of the model under varying levels of prior knowledge. As a baseline, we utilize a $\pi$BO-like procedure \citep{hvarfner2022pi}, as described below, to perform Thompson sampling across all experiments.

#### Training.

For training, we generate a prior distribution similar to \cref{app:priors}, but with slight adjustments: when sampling the mixture distribution, we include a 50\% chance of adding a uniform component. If present, the uniform distribution weight $w_\text{unif}$ is sampled uniformly from 0.0 to 0.2 (otherwise $w_\text{unif} = 0$). The uniform component is then added as follows: \begin{align} \mathbf{p} = (w_\text{unif} \cdot \mathbf{p}_\text{unif}) + (1 - w_\text{unif}) \cdot \mathbf{p}_\text{mixture}, \end{align} where $\mathbf{p}_\text{unif}$ represents the uniform component, and $\mathbf{p}_\text{mixture}$ is the sampled mixture. The inclusion of a uniform component during training means that the prior can be a mixture of an informative and a non-informative (flat) component, which will be useful later.

Using this binned distribution, we then sample $\xopt$ and $\yopt$, and use these two latent samples to construct our function, as outlined in \cref{sec:bo-dataset}.

#### Testing.

During the BO testing phase, we consider two scenarios:

1. Strong prior: We first sample a mean for the $\xopt$ prior by drawing from a Gaussian distribution centered on the true $\xopt$ with a standard deviation set to $10\%$ of the domain (in our case $[-1,1]$), resulting in a standard deviation of $0.2$. We use this sampled prior mean and standard deviation to construct the binned prior.
2. Weak prior: The same steps are applied to generate the prior, but with a standard deviation of $25\%$, which translates to $0.5$ for our domain.

In both scenarios, we add a uniform prior component with $w_\text{uniform}=0.1$. The uniform component helps with model and prior misspecification, by allowing the model to explore outside the region indicated by the prior.

We compare ACE with Prior Thompson Sampling (ACEP-TS) to the no-prior ACE-TS and a baseline GP-TS. We also consider a state-of-the-art heuristic for prior injection in BO, $\pi$BO \citep{hvarfner2022pi}, with the TS acquisition procedure described below ($\pi$BO-TS). The procedure is repeated 10 times for each case, with different initial points sampled at random.

#### $\pi$BO-TS.

The main technique in $\pi$BO for injecting a prior in the BO procedure consists of rescaling the chosen acquisition function $\alpha(\x)$ by the user-provided prior over the optimum location $\pi(\x)$ (Eq. 6 in \citealp{hvarfner2022pi}),

$$
\label{eq:pibo}
\alpha_\text{$\pi$BO}(\x; \alpha) \propto \alpha(\x) \pi(\x)^{\gamma_n},
$$

where $n$ is the BO iteration and $\gamma_n$ governs the relative influence of the prior with respect to the acquisition function, which is heuristically made to decay over iterations to reflect the increased role of the observed data. As in \citet{hvarfner2022pi}, we set $\gamma_n = \frac{\beta}{n}$ where $\beta$ is a hyperparameter reflecting the user confidence on the prior.

To implement Thompson sampling (TS) with $\pi$BO, we first note that the TS acquisition function $\alpha_\text{TS}(\x)$ corresponds to the current posterior probability over the optimum location, and the TS procedure consists of drawing one sample from this acquisition function (as opposed to optimizing it). Thus, the $\pi$BO variant of TS ($\pi$BO-TS) corresponds to sampling from \cref{eq:pibo}, where the current posterior over the optimum takes the role of $\alpha(\x)$. We sample from \cref{eq:pibo} using a self-normalized importance sampling-resampling approach \citep{robert2004monte}. Namely, we sample $N_\text{TS} = 100$ points from $\alpha_\text{TS}$ using batch GP-TS, then resample one point from this batch using importance sampling weight $w \propto \frac{\alpha_\text{TS}(\x) \pi(\x)^{\beta/n}}{\alpha_\text{TS}(\x)} = \pi(\x)^{\beta/n}$, where all weights are then normalized to sum to 1. Following \citep{hvarfner2022pi}, we set $\beta = 10$, i.e., equal to their setting when running BO experiments with 100 iterations, as in our case.

#### Results.

Additional results on the weak prior scenario are presented in \cref{fig:bo_comparisons_weak} and with strong prior in \cref{fig:bo_comparisons_strong}. The results indicate that ACEP-TS consistently outperforms ACE-TS, particularly when using a strong prior. In this case, the model benefits from the prior information, leading to a notable improvement in performance. Specifically, the strong prior allows the model to converge more rapidly toward the optimum.

**Figure:** **Bayesian optimization with weak prior.** Prior injection can improve the performance of ACE, making it perform competitively compared to $\pi$BO-TS. \label{fig:bo_comparisons_weak}

**Figure:** **Bayesian optimization with strong prior.** When strong priors are used, the gap between ACE-TS and ACEP-TS is more evident compared to weak priors. \label{fig:bo_comparisons_strong}

## Simulation-based inference
\label{app:sbi}

### Simulators
\label{app:sbi-simulators}

The experiments reported in \cref{exp:sbi} used three time-series models to simulate the training and test data. This section describes the simulators in more details.

**Ornstein-Uhlenbeck Process (OUP)** is widely used in financial mathematics and evolutionary biology due to its ability to model mean-reverting stochastic processes \citep{uhlenbeck1930theory}. The model is defined as: \begin{equation*} y_{t+1} = y_t + \Delta y_t, \quad \Delta y_t = \theta_1 \left[\exp(\theta_2) - y_t \right] \Delta t + 0.5w, \quad \text{ for } t = 1, \ldots, T, \end{equation*}

where $ T = 25 $, $\Delta t = 0.2 $, $ x_0 = 10 $, and $w \sim \mathcal{N}(0, \Delta t) $. We use a uniform prior $ U([0, 2] \times [-2, 2]) $ for the latent variables $ \vtheta = (\theta_1, \theta_2)$ to generate the simulated data.

**Susceptible-Infectious-Recovered (SIR)** is a simple compartmental model used to describe infectious disease outbreaks \citep{kermack1927contribution}. The model divides a population into susceptible (S), infectious (I), and recovered (R) individuals. Assuming population size $N$ and using $S_t$, $I_t$, and $R_t$ to denote the number of individuals in each compartment at time $t$, $t = 1, \ldots, T$, the disease outbreak dynamics can be expressed as \begin{equation*} \Delta S_t = -\beta\frac{I_t S_t}{N}, \quad \Delta I_t = \beta\frac{I_t S_t}{N} - \gamma I_t, \quad \Delta R_t = \gamma I_t, \end{equation*} where the parameters $\beta$ and $\gamma$ denote the contact rate and the mean recovery rate. An observation model with parameters $\phi$ is used to convert the SIR model predictions to observations~$(t, y_t)$. The experiments carried out in this work consider two observation models and simulator setups.

The setups considered in this work are as follows. First, we consider a SIR model with fixed initial condition and $10$ observations $y_t\sim\mathrm{Bin}(1000, I_t/N)$ collected from $T=160$ time points at even interval, as proposed in \citep{lueckmann2021benchmarking}. Here the population size $N=10^6$ and the initial condition is fixed as $S_0=N-1$, $I_0=1$, $R_0=0$. We use uniform priors $\beta\sim U(0.01, 1.5)$ and $\gamma\sim U(0.02, 0.25)$. We used this model version in the main experiments presented in \cref{exp:sbi} and \cref{app:sbi_cfg}.

In addition we consider a setup where $N$ and $I_0$ are unknown and we collect $25$ observations $y_t\sim\mathrm{Poi}(\phi I_t/N)$ from $T=250$ time points at even interval. We use $\beta\sim U(0.5, 3.5)$, $\gamma\sim U(0.0001, 1.5)$, $\phi\sim U(50, 5000)$, and $I_0/N\sim U(0.0001, 0.01)$ with $S_0/N = 1 - I_0/N$ and $R_0/N = 0$ to generate simulated samples. We used this model version in an additional experiment to test ACE on real world data, presented in \cref{app:sbi_real}.

**Turin** model is a time-series model used to simulate radio propagation phenomena, making it useful for testing and designing wireless communication systems \citep{turin1972statistical, pedersen2019stochastic, bharti2019estimator}. The model generates high-dimensional complex-valued time-series data and is characterized by four key parameters that control different aspects of the radio signal: $G_0$ controls the reverberation gain, $T$ determines the reverberation time, $\nu$ specifies the arrival rate of the point process, and $\sigma^2_W$ represents the noise variance.

The model starts with a frequency bandwidth $B=0.5$ GHz and simulates the transfer function $H_k$ over $N_s = 101$ equidistant frequency points. The measured transfer function at the $k$-th point, $Y_k$, is given by: \begin{equation*} Y_k = H_k + W_k, \quad k = 0, 1, \ldots, N_s - 1, \end{equation*} where $W_k$ denotes additive zero-mean complex circular symmetric Gaussian noise with variance $\sigma^2_W$. The transfer function $H_k$ is defined as: \begin{equation*} H_k = \sum_{l=1}^{N_{\text{points}}} \alpha_l \exp(-j 2 \pi \Delta f k \tau_l), \end{equation*} where $\tau_l$ are the time delays sampled from a one-dimensional homogeneous Poisson point process with rate $ \nu$, and $\alpha_l$ are complex gains. The gains $\alpha_l$ are modeled as i.i.d. zero-mean complex Gaussian random variables conditioned on the delays, with a conditional variance: \begin{equation*} \mathbb{E}[\vert \alpha_l \vert^2 | \tau_l] = \frac{G_0 \exp(-\tau_l / T)}{\nu}. \end{equation*}

The time-domain signal $\tilde{y}(t)$ can be obtained by taking the inverse Fourier transform: \begin{equation*} \tilde{y}(t) = \frac{1}{N_s} \sum_{k=0}^{N_s - 1} Y_k \exp(j 2 \pi k \Delta f t), \end{equation*} with $\Delta f = B / (N_s - 1) $ being the frequency separation. Our final real-valued output is calculated by taking the absolute square of the complex-valued data and applying a logarithmic transformation $y(t) = 10 \log_{10}(|\tilde{y}(t)|^2)$.

The four parameters of the model are sampled from the following uniform priors: $G_0 \sim \mathcal{U}(10^{-9}, 10^{-8})$, $ T \sim \mathcal{U}(10^{-9}, 10^{-8})$, $ \nu \sim \mathcal{U}(10^{7}, 5 \times 10^{9}) $, $ \sigma^2_W \sim \mathcal{U}(10^{-10}, 10^{-9})$.

### Main experiments
\label{app:sbi_cfg}

ACE was trained on examples that included simulated time series data and model parameters divided between target and context. In these experiments, the time series data were divided into context and target data by sampling $N_d$ data points into the context set and including the rest in the target set. The context size $N_d\sim U(10, 25)$ in the OUP experiments, $N_d\sim U(5, 10)$ in the SIR experiments, and $N_d\sim U(50, 101)$ in the Turin experiments. In addition, the model parameters were randomly assigned to either the context or target set. NPE and NRE cannot handle partial observations and was trained with the full time series data in both cases.

The ACE model used in these experiments had embedding dimension $64$ and $6$ transformer layers. The attention blocks had $4$ heads and the MLP block had hidden dimension $128$. The output head had $K=20$ MLP components with hidden dimension $128$. The model was trained for $5\times 10^4$ steps with batch size $32$, using learning rate $5 \times 10^{-4}$ with cosine annealing.

We used the \texttt{sbi} package \citep{tejero-cantero2020sbi} (\url{https://sbi-dev.github.io/sbi/}, Version: 0.22.0, License: Apache 2.0) to implement NPE and NRE. Specifically, we chose the NPE-C \citep{greenberg2019automatic} and NRE-C \citep{miller2022contrastive} with Masked Autoregressive Flow (MAF) \citep{papamakarios2017masked} as the inference network. We used the default configuration with $50$ hidden units and $5$ transforms for MAF, and training with a fixed learning rate $5 \times 10^{-4}$. For Simformer \citep{gloeckler2024all}, we used their official package (\url{https://github.com/mackelab/simformer}, Version: 2, License: MIT). We used the same configuration as in our setup for the transformer, while we used their default configuration for the diffusion part. For a fair comparison, we pre-generated $10^4$ parameter-simulation pairs for all methods. We also normalized the parameters of the Turin model when feeding into the networks. For evaluation, we randomly generated 100 observations and assessed each method across 5 runs. For MMD, we use an exponentiated quadratic kernel with a lengthscale of 1.

#### Statistical comparisons.

We evaluate models based on their average results across multiple runs and perform pairwise comparisons to identify models with comparable performance. The results from pairwise comparisons are used in \cref{tab:sbi} to highlight in bold the models that are considered best in each experiment. The following procedure is used to determine the best models:

- First, we identify the model (A) with the highest empirical mean and highlight it in bold.
- For each alternative model (B), we perform $10^5$ bootstrap iterations to resample the mean performance for both model A and model B.
- We then calculate the proportion of bootstrap iterations where model B outperforms model A.
- If this proportion is larger than the significance level ($\alpha=0.05$), model B is considered statistically indistinguishable from model A.
- All models that are not statistically significantly different from the best model are highlighted in bold.

### Simulation-based calibration
\label{app:sbc}

To evaluate the calibration of the approximate posteriors obtained by ACE, we apply simulation-based calibration (SBC; \citealt{talts2018validating}) on the Turin model to evaluate whether the approximate posteriors produced by ACE are calibrated.

We recall that SBC checks if a Bayesian inference process is well-calibrated by repeatedly simulating data from parameters drawn from the prior and inferring posteriors under those priors and simulated datasets. If the inference is calibrated, the average posterior should match the prior. Equivalently, when ranking the true parameters within each posterior, the ranks should follow a uniform distribution \citep{talts2018validating}.

We use the following procedure for SBC: for a given prior, we first sample 1000 samples from the prior distribution and generate corresponding simulated data. Then we use ACE to approximate the posteriors and subsequently compare the true parameter values with samples drawn from the inferred posterior distribution.

To visualize the calibration, we plot the density of the posterior samples against the prior samples. If the model is well-calibrated, the posterior distribution should recover the true posterior, which results in a close match between the density of the posterior samples and the prior. We also present the fractional rank statistic against the ECDF difference \citep{sailynoja2022graphical}. Ideally, the ECDF difference between the rank statistics and the theoretical uniform distribution should remain close to zero, indicating well-calibrated posteriors.

**Figure:** Simulation-based calibration of ACE on the Turin model. The top row shows the density of the posterior samples from ACE compared with the prior samples. The bottom row shows the fractional rank statistic against the ECDF difference with 95\% confidence bands. ACE is well-calibrated. \label{fig:sbc}

**Figure:** Simulation-based calibration of ACE and ACEP on the Turin model with an example custom prior. ACEP demonstrates improved calibration by closely following the prior distribution and showing lower deviations in the ECDF difference, highlighting its ability to condition on user-specified priors effectively. \label{fig:sbc_pi}

\cref{fig:sbc} shows that our ACE is well-calibrated with pre-defined uniform priors across all four latents. Since ACEP allows conditioning on different priors at runtime, we also test the calibration of ACEP using randomly generated priors (following \cref{app:prior}). For comparison, we show what happens if we forego prior-injection, using vanilla ACE instead of ACEP. The visualization on one set of priors is shown in \cref{fig:sbc_pi}. As expected, vanilla ACE (without prior-injection) does not include the correct prior information and shows suboptimal calibration performance, whereas ACEP correctly leverages the provided prior information and shows closer alignment with the prior and lower ECDF deviations. We also calculate the average absolute deviation over 100 randomly sampled priors. In the prior-injection setting, ACEP demonstrates better calibration, with an average deviation of $0.03 \pm 0.01$ compared to $0.10 \pm 0.05$ for ACE without the correct prior.

### Extended SIR model on real-world data
\label{app:sbi_real}

We present here results obtained by considering an extended four-parameter version of the SIR model then applied to real-world data. We further include details on the training data and model configurations used in the real-data experiment as well as additional evaluation results from experiments carried out with simulated data. As our real-world data, we used a dataset that describes an influenza outbreak in a boarding school. The dataset is available in the R package \texttt{outbreaks} (\url{https://cran.r-project.org/package=outbreaks}, Version: 1.9.0, License: MIT).

#### Methods.

The four-parameter SIR model we used is detailed in \cref{app:sbi-simulators} (last paragraph). The ACE models were trained with samples constructed based on simulated data as follows. The observations were divided into context and target points by sampling $N_d\sim U(2, 20)$ data points into the context set and $2$ data points into the target set. The examples included $50$\% interpolation tasks where the context and target points were sampled at random (without overlap) and $50$\% forecast tasks where the points were sampled in order. The model parameters were divided between the context and target set by sampling the number to include $N_l\sim U(0, 4)$ and sampling the $N_l$ parameters from the parameter set at random. The parameters were normalized to range $[-1, 1]$ and the observations were square-root compressed and scaled to the approximate range $[-1, 1]$.

The ACE models had the same architecture as the models used in the main experiment, but the models were trained for $10^5$ steps with batch size $32$. In this experiment, we generated the data online during the training, which means that the models were trained with $3.2 \times 10^6$ samples. The NPE models used in this experiment had the same configuration as the model used in the main experiment, for fair comparison, the models were now trained with $3.2 \times 10^6$ samples. Each sample corresponded to a unique simulation and the full time series was used as the observation data.

To validate model predictions, we note that ground-truth parameter values are not available for real data. Instead, we examined whether running the simulator with parameters sampled from the posterior can replicate the observed data. For reference, we also included MCMC results. The MCMC posterior was sampled with Pyro \citep{bingham2018pyro} (\url{https://pyro.ai/}, Version: 1.9.0, License: Apache 2.0) using the random walk kernel and sampling $4$ chains with $5\times 10^4$ warm-up steps and $5 \times 10^4$ samples.

#### Results.

**Figure:** **SIR model on a real dataset.** Posterior predictive distributions based on the ACE, NPE, and MCMC posteriors. Note that this dataset is mildly misspecified, in that even MCMC does not fully match the data. \label{fig:sir-example}

The posterior predictive distributions and log-probabilities for observed data calculated based on ACE, NPE, and MCMC results are shown in \cref{fig:sir-example}. For this visualization, ACE and NPE models were trained once, and simulations were carried out with $5000$ parameters sampled from each posterior distribution. The log-probabilities observed in this experiment are $-64.4$ with ACE, $-64.6$ with NPE. Repeating ACE and NPE training and posterior estimation $10$ times, the average log-probabilities across the $10$ runs were $-65.1$ (standard deviation $0.4$) with ACE and $-65.5$ (standard deviation $0.7$) with NPE, showing a similar performance. The ACE predictions used in this experiment are sampled autoregressively (see \cref{app:autoregressive}). These results show that ACE can handle inference with real data.

#### Validation on simulated data.

For completeness, we performed a more extensive validation of ACE and other methods with the extended SIR model using simulated data. Specifically, we assessed the ACE and NPE models on simulated data and evaluated the same ACE models in a data completion task with the TNP-D baseline. All the training details remain the same as in the real-world experiment for ACE and NPE. The TNP-D models had the same overall architecture as ACE but used a different embedder and output head. The MLP block in the TNP-D embedder had hidden dimension $64$ and the MLP block in the single-component output head hidden dimension $128$. The TNP-D models were trained for $10^5$ steps with batch size $32$. The evaluation set used in these experiments included $1000$ simulations sampled from the training distribution and the evaluation metrics included log-probabilities and coverage probabilities calculated based on 95\% quantile intervals that were estimated based on $5000$ samples.

We start with the posterior estimation task where we used ACE and NPE to predict simulator parameters based on the simulated observations with $25$ observation points. The results are reported in \cref{tab:sbi_sir_ext_ace_npe}. We observe that the ACE log-probabilities are on average better than NPE log-probabilities and that both methods have marginal coverage probabilities close to the nominal level $0.95$.

**Table:** Comparison between ACE and NPE in posterior estimation task in the extended SIR model. The ACE predictions were generated autoregressively so both methods target the joint posterior. The estimated posteriors are compared based on log-probabilities and 95\% marginal coverage probabilities. The evaluation set includes 1000 examples and we report the mean and \textcolor{gray}{(standard deviation)} from $10$ runs. ACE log-probabilities are on average better than NPE log-probabilities and the coverage probabilities are close to the nominal level $0.95$. \label{tab:sbi_sir_ext_ace_npe}

| | log-probs ($\uparrow$) | cover $\beta$ | cover $\gamma$ | cover $\phi$ | cover $I_0$ | cover ave |
| --- | --- | --- | --- | --- | --- | --- |
| NPE | 6.63 (0.16) | 0.92 (0.01) | 0.94 (0.01) | 0.94 (0.01) | 0.92 (0.01) | 0.93 (0.01) |
| ACE (AR) | 7.38 (0.04) | 0.96 (0.00) | 0.97 (0.00) | 0.97 (0.00) | 0.96 (0.00) | 0.97 (0.00) |

The simulated observations used in the previous experiment were complete with $25$ observation points. Next, we evaluate ACE posteriors estimated based on incomplete data with $5$--$20$ observation points. NPE is not included in this experiment since it cannot handle incomplete observations. Instead, we use this experiment to compare independent and autoregressive ACE predictions. The results are reported in \cref{tab:sbi_sir_ext_ace}. The log-probabilities indicate that both independent and autoregressive predictions improve when more observation points are available while the coverage probabilities are close to the nominal level in all conditions. That autoregressive predictions result in better log-probabilities than independent predictions indicates that ACE is able to use dependencies between simulator parameters.

**Table:** ACE posterior estimation based on incomplete data with $M$ observation points using either independent or autoregressive predictions. The estimated posteriors are evaluated using (a) log-probabilities and (b) average 95\% marginal coverage probabilities. We report the mean and \textcolor{gray}{(standard deviation)} from $10$ runs. The log-probabilities improve when the context size $M$ increases and when autoregressive predictions are used. \label{tab:sbi_sir_ext_ace}

```latex
\begin{tabular}{cc} (a) & \begin{tabular}{lccccc} \toprule & $M=25$ & $M=20$ & $M=15$ & $M=10$ & $M=5$ \\ \midrule ACE & 4.94 \textcolor{gray}{(0.04)} & 4.55 \textcolor{gray}{(0.03)} & 3.87 \textcolor{gray}{(0.02)} & 2.82 \textcolor{gray}{(0.03)} & 0.88 \textcolor{gray}{(0.03)}\\ ACE (AR) & 7.38 \textcolor{gray}{(0.04)} & 6.93 \textcolor{gray}{(0.04)} & 6.21 \textcolor{gray}{(0.04)} & 5.11 \textcolor{gray}{(0.04)} & 2.91 \textcolor{gray}{(0.05)}\\ \midrule\midrule \end{tabular}
\begin{tabular}{lccccc} ACE & 0.97 \textcolor{gray}{(0.00)} & 0.96 \textcolor{gray}{(0.00)} & 0.95 \textcolor{gray}{(0.00)} & 0.95 \textcolor{gray}{(0.00)} & 0.96 \textcolor{gray}{(0.00)}\\ ACE (AR) & 0.97 \textcolor{gray}{(0.00)} & 0.97 \textcolor{gray}{(0.00)} & 0.96 \textcolor{gray}{(0.00)} & 0.96 \textcolor{gray}{(0.00)} & 0.97 \textcolor{gray}{(0.00)}\\ \bottomrule \end{tabular}
```

The same ACE models that have been evaluated in the posterior estimation (latent prediction) task can also make predictions about the unobserved values in incomplete data. To evaluate ACE in the data completion task, we selected $5$ target observations from each evaluation sample and used $5$--$20$ remaining observations as context. We used ACE to make target predictions either based on the context data alone or based on both context data and the simulator parameters $\theta$. For comparison, we also evaluated data completion with TNP-D. The results are reported in \cref{tab:sbi_sir_ext_data_pred}. We observe that ACE log-probabilities are on average better than TNP-D log-probabilities and improve when simulator parameters are available as context. In these experiments, both ACE and TNP-D were used to make independent predictions.

**Table:** Comparison between ACE and TNP-D in data completion task in the extended SIR model. The estimated predictive distributions are compared based on (a) log-probabilities and (a) 95\% coverage probabilities. We report the mean and \textcolor{gray}{(standard deviation)} from $10$ runs. ACE log-probabilities are on average better than TNP-D log-probabilities and improve both when the context size $M$ increases or when predictions are conditioned on the simulator parameters $\theta$. \label{tab:sbi_sir_ext_data_pred}

```latex
\begin{tabular}{cc} (a) & \begin{tabular}{lcccc} \toprule & $M=20$ & $M=15$ & $M=10$ & $M=5$ \\ \midrule TNP-D & 10.1 \textcolor{gray}{(0.11)} & 9.99 \textcolor{gray}{(0.09)} & 9.44 \textcolor{gray}{(0.10)} & 8.02 \textcolor{gray}{(0.07)} \\ ACE & 14.2 \textcolor{gray}{(0.31)} & 13.8 \textcolor{gray}{(0.31)} & 13.2 \textcolor{gray}{(0.31)} & 11.4 \textcolor{gray}{(0.28)} \\ ACE + $\theta$ & 14.7 \textcolor{gray}{(0.31)} & 14.6 \textcolor{gray}{(0.31)} & 14.6 \textcolor{gray}{(0.30)} & 14.3 \textcolor{gray}{(0.30)} \\ \midrule\midrule \end{tabular}
\begin{tabular}{lccccc} TNP-D & 0.96 \textcolor{gray}{(0.00)} & 0.96 \textcolor{gray}{(0.00)} & 0.95 \textcolor{gray}{(0.00)} & 0.95 \textcolor{gray}{(0.00)} \\ ACE & 0.97 \textcolor{gray}{(0.00)} & 0.96 \textcolor{gray}{(0.00)} & 0.96 \textcolor{gray}{(0.00)} & 0.95 \textcolor{gray}{(0.00)} \\ ACE + $\theta$ & 0.96 \textcolor{gray}{(0.00)} & 0.96 \textcolor{gray}{(0.00)} & 0.96 \textcolor{gray}{(0.00)} & 0.96 \textcolor{gray}{(0.00)} \\ \bottomrule \end{tabular}
```

## Computational resources and software
\label{app:computation}

For the experiments and baselines, we used a GPU cluster containing AMD MI250X GPUs. All experiments can be run using a single GPU with a VRAM of 50GB. Most of the experiments took under 6 hours, with the exception of a few BO experiments that took around 10 hours. The core code base was built using Pytorch \citep{paszke2019pytorch} (\url{https://pytorch.org/} Version: 2.2.0, License: modified BSD license) and based on the Pytorch implementation for TNP \citep{nguyen2022transformer} (\url{https://github.com/tung-nd/TNP-pytorch}, License: MIT). Botorch \citep{balandat2020botorch} (\url{https://github.com/pytorch/botorch} Version: 0.10.0, License: MIT) was used for the implementation of GP-MES, GP-TS, and $\pi$BO-TS.
